{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint TV for multi-contrast MR\n",
    "This demonstration shows how to do a synergistic reconstruction of two MR images with different contrast. Both MR images show the same underlying anatomy but of course with different contrast. In order to make use of this similarity a joint total variation (TV) operator is used as a regularisation in an iterative image reconstruction approach. \n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christoph Kolbitsch, Evangelos Papoutsellis, Edoardo Pasca  \n",
    "First version: 16th of June 2021  \n",
    "Updated: 26nd of June 2021  \n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2021 Rutherford Appleton Laboratory STFC.    \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.  \n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Positron Emission Tomography and Magnetic Resonance imaging\n",
    "(http://www.ccppetmr.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is installed that we need\n",
    "!pip install brainweb nibabel --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import SIRF functionality\n",
    "import notebook_setup\n",
    "import sirf.Gadgetron as mr\n",
    "from sirf_exercises import exercises_data_path\n",
    "\n",
    "\n",
    "# Import CIL functionality\n",
    "from cil.framework import  AcquisitionGeometry, BlockDataContainer, BlockGeometry, ImageGeometry\n",
    "from cil.optimisation.functions import Function, OperatorCompositionFunction, SmoothMixedL21Norm, L1Norm, L2NormSquared, BlockFunction, MixedL21Norm, IndicatorBox, TotalVariation, LeastSquares, ZeroFunction\n",
    "from cil.optimisation.operators import GradientOperator, BlockOperator, ZeroOperator, CompositionOperator, LinearOperator, FiniteDifferenceOperator\n",
    "from cil.optimisation.algorithms import PDHG, FISTA, GD\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_TV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    \n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)\n",
    "\n",
    "# This functions creates a regular (pattern='regular') or random (pattern='random') undersampled k-space data\n",
    "# with an undersampling factor us_factor and num_ctr_lines fully sampled k-space lines in the k-space centre.\n",
    "# For more information on this function please see the notebook f_create_undersampled_kspace\n",
    "def create_undersampled_kspace(acq_orig, us_factor, num_ctr_lines, pattern='regular'):\n",
    "    \"\"\"Create a regular (pattern='regular') or random (pattern='random') undersampled k-space data\"\"\"\n",
    "    \n",
    "    # Get ky indices\n",
    "    ky_index = acq_orig.get_ISMRMRD_info('kspace_encode_step_1')\n",
    "    \n",
    "    # K-space centre in the middle of ky_index\n",
    "    ky0_index = len(ky_index)//2\n",
    "    \n",
    "    # Fully sampled k-space centre\n",
    "    ky_index_subset =  numpy.arange(ky0_index-num_ctr_lines//2, ky0_index+num_ctr_lines//2)\n",
    "    \n",
    "    if pattern == 'regular':\n",
    "        ky_index_outside = numpy.arange(start=0, stop=len(ky_index), step=us_factor)\n",
    "        \n",
    "    elif pattern == 'random':\n",
    "        ky_index_outside = numpy.asarray(random.sample(list(ky_index), len(ky_index)//us_factor)) \n",
    "        \n",
    "    else:\n",
    "        raise ValueError('pattern should be \"random\" or \"linear\"')\n",
    "        \n",
    "    # Combine fully sampled centre and outer undersampled region    \n",
    "    ky_index_subset = numpy.concatenate((ky_index_subset, ky_index_outside), axis=0)\n",
    "    \n",
    "    # Ensure k-space points are note repeated\n",
    "    ky_index_subset = numpy.unique(ky_index_subset)\n",
    "    \n",
    "    # Create new k-space data\n",
    "    acq_new = preprocessed_data.new_acquisition_data(empty=True)\n",
    "\n",
    "    # Select raw data\n",
    "    for jnd in range(len(ky_index_subset)):\n",
    "        cacq = preprocessed_data.acquisition(ky_index_subset[jnd])\n",
    "        acq_new.append_acquisition(cacq)\n",
    "\n",
    "    acq_new.sort() \n",
    "    \n",
    "    return(acq_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint TV reconstruction of two MR images\n",
    "\n",
    "Assume we want to reconstruct two MR images $u$ and $v$ and utilse the similarity between both images using a joint TV ($JTV$) operator we can formulate the reconstruction problem as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(u^{*}, v^{*}) \\in \\underset{u,v}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{1} u - g\\|^{2}_{2} +  \\frac{1}{2} \\| A_{2} v - h\\|^{2}_{2} + \\alpha\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* $JTV_{\\eta, \\lambda}(u, v) = \\sum \\sqrt{ \\lambda|\\nabla u|^{2} + (1-\\lambda)|\\nabla v|^{2} + \\eta^{2}}$\n",
    "* $A_{1}$, $A_{2}$: __MR__ `AcquisitionModel`\n",
    "* $g_{1}$, $g_{2}$: __MR__ `AcquisitionData`\n",
    "\n",
    "\n",
    "### Solving this problem \n",
    "\n",
    "In order to solve the above minimization problem, we will use an alternating minimisation approach, where one variable is fixed and we solve wrt to the other variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u^{k+1} & = \\underset{u}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{1} u - g\\|^{2}_{2} + \\alpha_{1}\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v^{k}) \\quad \\text{subproblem 1}\\\\\n",
    "v^{k+1} & = \\underset{v}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{2} v - h\\|^{2}_{2} + \\alpha_{2}\\,\\mathrm{JTV}_{\\eta, 1-\\lambda}(u^{k+1}, v) \\quad \\text{subproblem 2}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "We are going to use a gradient descent approach to solve each of these subproblems alternatingly.\n",
    "\n",
    "The *regularisation parameter* `alpha` should be different for each subproblem. But not to worry at this stage. Maybe we should use $\\alpha_{1}, \\alpha_{2}$ in front of the two JTVs and a $\\lambda$, $1-\\lambda$ for the first JTV and $1-\\lambda$, $\\lambda$, for the second JTV with $0<\\lambda<1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on several other notebooks and hence certain steps will be carried out with minimal documentation. If you want more explainations, then we would like to ask you to refer to the corresponding notebooks which are mentioned in the following list. The steps we are going to carry out are\n",
    "\n",
    "  - (A) Get a T1 and T2 map from brainweb which we are going to use as ground truth $u_{gt}$ and $v_{gt}$ for our reconstruction (further information: `introduction` notebook)\n",
    "  \n",
    "  - (B) Create __MR__ `AcquisitionModel` $A_{1}$ and $A_{2}$ and simulate undersampled __MR__ `AcquisitionData` $g_{1}$ and $g_{2}$ (further information: `acquisition_model_mr_pet_ct` notebook)\n",
    "  \n",
    "  - (C) Set up the joint TV reconstruction problem\n",
    "  \n",
    "  - (D) Solve the joint TV reconstruction problem (further information on gradient descent: `gradient_descent_mr_pet_ct` notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (A) Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_arr  = vol['T2']\n",
    "T1_arr   = vol['T1']\n",
    "\n",
    "# Normalise image data\n",
    "T2_arr /= numpy.max(T2_arr)\n",
    "T1_arr /= numpy.max(T1_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = T1_arr.shape[0]//2\n",
    "plot_2d_image([1,2,1], T1_arr[slice_show, :, :], 'T1', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], T2_arr[slice_show, :, :], 'T2', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we got to two images with T1 and T2 contrast BUT they brain looks a bit small. Spoiler alert: We are going to reconstruct MR images with a FOV 256 x 256 voxels. As the above image covers 344 x 344 voxels, the brain would only cover a small part of our MR FOV. In order to ensure the brain fits well into our MR FOV, we are going to scale the images.\n",
    "\n",
    "In order to do this we are going to use an image `rescale` from the skimage package and simply rescale the image by a factor 2 and then crop it. To speed things up, we are going to already select a single slice because also our MR scan is going to be 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rescale\n",
    "\n",
    "# Select central slice\n",
    "central_slice = T1_arr.shape[0]//2\n",
    "T1_arr = T1_arr[central_slice, :, :]\n",
    "T2_arr = T2_arr[central_slice, :, :]\n",
    "\n",
    "# Rescale by a factor 2.0\n",
    "T1_arr = rescale(T1_arr, 2,0)\n",
    "T2_arr = rescale(T2_arr, 2.0)\n",
    "\n",
    "# Select a central ROI with 256 x 256 \n",
    "# We could also skip this because it is automaticall done by crop_and_fill() \n",
    "# but we would like to check if we did the right thing\n",
    "idim = [256, 256]\n",
    "offset = (numpy.array(T1_arr.shape) - numpy.array(idim)) // 2\n",
    "T1_arr = T1_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "T2_arr = T2_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "\n",
    "# Now we make sure our image is of shape (1, 256, 256) again because in __SIRF__ even 2D images \n",
    "# are expected to have 3 dimensions.\n",
    "T1_arr = T1_arr[numpy.newaxis,...]\n",
    "T2_arr = T2_arr[numpy.newaxis,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = T1_arr.shape[0]//2\n",
    "plot_2d_image([1,2,1], T1_arr[slice_show, :, :], 'T1', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], T2_arr[slice_show, :, :], 'T2', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that looks better. Now we have got images we can use for our MR simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Simulate undersampled MR AcquisitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MR AcquisitionData\n",
    "mr_acq = mr.AcquisitionData(exercises_data_path('MR', 'PTB_ACRPhantom_GRAPPA') \n",
    "                            + '/ptb_resolutionphantom_fully_ismrmrd.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CSM\n",
    "preprocessed_data = mr.preprocess_acquisition_data(mr_acq)\n",
    "\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 200\n",
    "csm.calculate(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate image template\n",
    "recon = mr.FullySampledReconstructor()\n",
    "recon.set_input(preprocessed_data)\n",
    "recon.process()\n",
    "im_mr = recon.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the coil maps\n",
    "plt.figure();\n",
    "csm_arr = numpy.abs(csm.as_array())\n",
    "\n",
    "plot_2d_image([1,2,1], csm_arr[0, 0, :, :], 'Coil 0', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], csm_arr[2, 0, :, :], 'Coil 2', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use these coilmaps to simulate our MR raw data. Nevertheless, they are obtained from a phantom scan which unfortunately has got some signal voids inside. If we used these coil maps directly, then these signal voids would cause artefacts. We are therefore going to interpolate the coil maps first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to calculate a mask from the `ImageData` `im_mr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_mr_arr = numpy.squeeze(numpy.abs(im_mr.as_array()))\n",
    "im_mr_arr /= numpy.max(im_mr_arr)\n",
    "\n",
    "mask = numpy.zeros_like(im_mr_arr)\n",
    "mask[im_mr_arr > 0.2] = 1\n",
    "\n",
    "plt.figure();\n",
    "plot_2d_image([1,1,1], mask, 'Mask', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to interpolate between the values defined by the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Target grid for a square image\n",
    "xi = yi = numpy.arange(0, im_mr_arr.shape[0])\n",
    "xi, yi = numpy.meshgrid(xi, yi)\n",
    "\n",
    "# Define grid points in mask\n",
    "idx = numpy.where(mask == 1)\n",
    "x = xi[idx[0], idx[1]]\n",
    "y = yi[idx[0], idx[1]]\n",
    "\n",
    "# Go through each coil and interpolate linearly\n",
    "csm_arr = csm.as_array()\n",
    "for cnd in range(csm_arr.shape[0]):\n",
    "    cdat = csm_arr[cnd, 0, idx[0], idx[1]]\n",
    "    cdat_intp = griddata((x,y), cdat, (xi,yi), method='linear')\n",
    "    csm_arr[cnd, 0, :, :] = cdat_intp\n",
    "    \n",
    "# No extrapolation was done by griddate and we will set these values to 0\n",
    "csm_arr[numpy.isnan(csm_arr)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the coil maps\n",
    "plt.figure();\n",
    "\n",
    "plot_2d_image([1,2,1], numpy.abs(csm_arr[0, 0, :, :]), 'Coil 0', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], numpy.abs(csm_arr[2, 0, :, :]), 'Coil 2', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the world's best interpolation but it will do for the moment. Let's replace the data in the coils maps with the new interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csm.fill(csm_arr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create the two __MR__ `AcquisitionModel` $A_{1}$ and $A_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create undersampled acquisition data\n",
    "us_factor = 2\n",
    "num_ctr_lines = 30\n",
    "pattern = 'random'\n",
    "acq_us = create_undersampled_kspace(preprocessed_data, us_factor, num_ctr_lines, pattern)\n",
    "\n",
    "# Create two MR acquisition models\n",
    "A1 = mr.AcquisitionModel(acq_us, im_mr)\n",
    "A1.set_coil_sensitivity_maps(csm)\n",
    "\n",
    "A2 = mr.AcquisitionModel(acq_us, im_mr)\n",
    "A2.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simulate undersampled __MR__ `AcquisitionData` $g_{1}$ and $g_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR\n",
    "u_gt = crop_and_fill(im_mr, T1_arr)\n",
    "g1 = A1.forward(u_gt)\n",
    "\n",
    "v_gt = crop_and_fill(im_mr, T2_arr)\n",
    "g2 = A2.forward(v_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we are going to add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_arr = g1.as_array()\n",
    "g1_max = numpy.max(numpy.abs(g1_arr))\n",
    "g1_arr += (numpy.random.random(g1_arr.shape) - 0.5 + 1j*(numpy.random.random(g1_arr.shape) - 0.5)) * g1_max * 0.01\n",
    "g1.fill(g1_arr)\n",
    "\n",
    "g2_arr = g2.as_array()\n",
    "g2_max = numpy.max(numpy.abs(g2_arr))\n",
    "g2_arr += (numpy.random.random(g2_arr.shape) - 0.5 + 1j*(numpy.random.random(g2_arr.shape) - 0.5)) * g2_max * 0.01\n",
    "g2.fill(g2_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check we are going to apply the backward/adjoint operation to do a simply image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reconstruction\n",
    "u_simple = A1.backward(g1)\n",
    "v_simple = A2.backward(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "plot_2d_image([1,2,1], numpy.abs(u_simple.as_array())[0, :, :], '$u_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], numpy.abs(v_simple.as_array())[0, :, :], '$v_{simple}$', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images look quite poor compared to the ground truth input images, because they are reconstructed from an undersampled k-space. In addition, you can see a strange \"structure\" going through the centre of the brain. This has something to do with the coil maps. As mentioned above, our coil maps have these two \"holes\" in the centre and this creates this artefacts. Nevertheless, this is not going to be a problem for our reconstruction as we will see later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C) Set up the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used mainly __SIRF__ functionality, now we are going to use __CIL__ in order to set up the reconstruction problem and then solve it. In order to be able to reconstruct both $u$ and $v$ at the same time, we will make use of `BlockDataContainer`. In the following we will define an operator which allows us to project a $(u,v)$ `BlockDataContainer` object into either $u$ or $v$. In literature, this operator is called **[Projection Map (or Canonical Projection)](https://proofwiki.org/wiki/Definition:Projection_(Mapping_Theory))** and is defined as:\n",
    "\n",
    "$$ \\pi_{i}: X_{1}\\times\\cdots\\times X_{n}\\rightarrow X_{i}$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\\pi_{i}(x_{0},\\dots,x_{i},\\dots,x_{n}) = x_{i},$$\n",
    "\n",
    "mapping an element $x$ from a Cartesian Product $X =\\prod_{k=1}^{n}X_{k}$ to the corresponding element $x_{i}$ determined by the index $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionMap(LinearOperator):\n",
    "    \n",
    "    def __init__(self, domain_geometry, index, range_geometry=None):\n",
    "        \n",
    "        self.index = index\n",
    "        if range_geometry is None:\n",
    "            range_geometry = domain_geometry.geometries[self.index]\n",
    "            \n",
    "        super(ProjectionMap, self).__init__(domain_geometry=domain_geometry, \n",
    "                                           range_geometry=range_geometry)   \n",
    "        \n",
    "    def direct(self,x,out=None):\n",
    "                        \n",
    "        if out is None:\n",
    "            return x[self.index]\n",
    "        else:\n",
    "            out.fill(x[self.index])\n",
    "    \n",
    "    def adjoint(self,x, out=None):\n",
    "        \n",
    "        if out is None:\n",
    "            tmp = self.domain_geometry().allocate()\n",
    "            tmp[self.index].fill(x)            \n",
    "            return tmp\n",
    "        else:\n",
    "            out[self.index].fill(x) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we define the `SmoothJointTV` class. Our plan is to use the Gradient descent (`GD`) algorithm to solve the above problems. This implements the `__call__` method required to monitor the objective value and the `gradient` method that evaluates the gradient of `JTV`.\n",
    "\n",
    "For the two subproblems, the first variations with respect to $u$ and $v$ variables are:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& A_{1}^{T}*(A_{1}u - g_{1}) - \\alpha_{1} \\mathrm{div}\\bigg( \\frac{\\nabla u}{|\\nabla(u, v)|_{2,\\eta,\\lambda}}\\bigg)\\\\\n",
    "& A_{2}^{T}*(A_{2}v - g_{2}) - \\alpha_{2} \\mathrm{div}\\bigg( \\frac{\\nabla v}{|\\nabla(u, v)|_{2,\\eta,1-\\lambda}}\\bigg)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $$|\\nabla(u, v)|_{2,\\eta,\\lambda} = \\sqrt{ \\lambda|\\nabla u|^{2} + (1-\\lambda)|\\nabla v|^{2} + \\eta^{2}}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothJointTV(Function):\n",
    "              \n",
    "    def __init__(self, eta, axis, lambda_par):\n",
    "                \n",
    "        r'''\n",
    "        :param eta: smoothing parameter making SmoothJointTV differentiable \n",
    "        '''\n",
    "\n",
    "        super(SmoothJointTV, self).__init__(L=8)\n",
    "        \n",
    "        # smoothing parameter\n",
    "        self.eta = eta   \n",
    "        \n",
    "        # GradientOperator\n",
    "        FDy = FiniteDifferenceOperator(u_simple, direction=1)\n",
    "        FDx = FiniteDifferenceOperator(u_simple, direction=2)\n",
    "        self.grad = BlockOperator(FDy, FDx)\n",
    "                \n",
    "        # Which variable to differentiate\n",
    "        self.axis = axis\n",
    "        \n",
    "        if self.eta==0:\n",
    "            raise ValueError('Need positive value for eta')\n",
    "            \n",
    "        self.lambda_par=lambda_par    \n",
    "                                    \n",
    "                            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        r\"\"\" x is BlockDataContainer that contains (u,v). Actually x is a BlockDataContainer that contains 2 BDC.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, BlockDataContainer):\n",
    "            raise ValueError('__call__ expected BlockDataContainer, got {}'.format(type(x))) \n",
    "\n",
    "        tmp = numpy.abs((self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.eta**2).sqrt().sum())\n",
    "\n",
    "        return tmp    \n",
    "                        \n",
    "             \n",
    "    def gradient(self, x, out=None):\n",
    "        \n",
    "        denom = (self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.eta**2).sqrt()         \n",
    "        \n",
    "        if self.axis==0:            \n",
    "            num = self.lambda_par*self.grad.direct(x[0])                        \n",
    "        else:            \n",
    "            num = (1-self.lambda_par)*self.grad.direct(x[1])            \n",
    "\n",
    "        if out is None:    \n",
    "            tmp = self.grad.range.allocate()\n",
    "            tmp[self.axis].fill(self.grad.adjoint(num.divide(denom)))\n",
    "            return tmp\n",
    "        else:                                \n",
    "            self.grad.adjoint(num.divide(denom), out=out[self.axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put everything together and define our two objective functions which solve the two subproblems which we defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = 0.05\n",
    "alpha2 = 0.05\n",
    "lambda_par = 0.5\n",
    "eta = 1e-12\n",
    "\n",
    "# BlockGeometry for the two modalities\n",
    "bg = BlockGeometry(u_simple, v_simple)\n",
    "\n",
    "# Projection map, depending on the unkwown variable\n",
    "L1 = ProjectionMap(bg, index=0)\n",
    "L2 = ProjectionMap(bg, index=1)\n",
    "\n",
    "# Fidelity terms based on the acqusition data\n",
    "f1 = 0.5*L2NormSquared(b=g1)\n",
    "f2 = 0.5*L2NormSquared(b=g2)\n",
    "\n",
    "# JTV for each of the subproblems\n",
    "JTV1 = alpha1*SmoothJointTV(eta=eta, axis=0, lambda_par = lambda_par )\n",
    "JTV2 = alpha2*SmoothJointTV(eta=eta, axis=1, lambda_par = 1-lambda_par)\n",
    "\n",
    "# Compose the two objective functions\n",
    "objective1 = OperatorCompositionFunction(f1, CompositionOperator(A1, L1)) + JTV1\n",
    "objective2 = OperatorCompositionFunction(f2, CompositionOperator(A2, L2)) + JTV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (D) Solve the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with zero-filled images\n",
    "x0 = bg.allocate(0.0)\n",
    "\n",
    "# We use a fixed step-size for the gradient descent approach\n",
    "step_size = 0.1\n",
    "\n",
    "# We are also going to log the value of the objective functions\n",
    "obj1_val_it = []\n",
    "obj2_val_it = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    gd1 = GD(x0, objective1, step_size=step_size, \\\n",
    "          max_iteration = 4, update_objective_interval = 1)\n",
    "    gd1.run(verbose=1)\n",
    "    \n",
    "    # We skip the first one because it gets repeated\n",
    "    obj1_val_it.extend(gd1.objective[1:])\n",
    "    \n",
    "    # Here we are going to do a little \"trick\" in order to better see, when each subproblem is optimised, we\n",
    "    # are going to append NaNs to the objective function which is currently not optimised. The NaNs will not\n",
    "    # show up in the final plot and hence we can nicely see each subproblem.\n",
    "    obj2_val_it.extend(numpy.ones_like(gd1.objective[1:])*numpy.nan)\n",
    "    \n",
    "    gd2 = GD(gd1.solution, objective2, step_size=step_size, \\\n",
    "          max_iteration = 4, update_objective_interval = 1)\n",
    "    gd2.run(verbose=1)\n",
    "    \n",
    "    obj2_val_it.extend(gd2.objective[1:])\n",
    "    obj1_val_it.extend(numpy.ones_like(gd2.objective[1:])*numpy.nan)\n",
    "    \n",
    "    x0.fill(gd2.solution)\n",
    "    \n",
    "    print('* * * * * * Outer Iteration ', i, ' * * * * * *\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can look at the images $u_{jtv}$ and $v_{jtv}$ and compare them to the simple reconstruction $u_{simple}$ and $v_{simple}$ and the original ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_jtv = numpy.squeeze(numpy.abs(x0[0].as_array()))\n",
    "v_jtv = numpy.squeeze(numpy.abs(x0[1].as_array()))\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([2,3,1], numpy.squeeze(numpy.abs(u_simple.as_array()[0, :, :])), '$u_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,2], u_jtv, '$u_{JTV}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,3], numpy.squeeze(numpy.abs(u_gt.as_array()[0, :, :])), '$u_{gt}$', cmap=\"Greys_r\") \n",
    "\n",
    "plot_2d_image([2,3,4], numpy.squeeze(numpy.abs(v_simple.as_array()[0, :, :])), '$v_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,5], v_jtv, '$v_{JTV}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,6], numpy.squeeze(numpy.abs(v_gt.as_array()[0, :, :])), '$v_{gt}$', cmap=\"Greys_r\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(obj1_val_it, 'o-', label='subproblem 1')\n",
    "plt.plot(obj2_val_it, '+-', label='subproblem 2')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Value of objective function')\n",
    "plt.title('Objective functions')\n",
    "plt.legend()\n",
    "\n",
    "# Logarithmic y-axis\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a good demonstration for a synergistic image reconstruction of two different images. The following gives a few suggestions of what to do next and also how to extend this notebook to other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of iterations\n",
    "In our problem we have several regularisation parameters such as $\\alpha_{1}$, $\\alpha_{2}$ and $\\lambda$. In addition, the number of inner iterations for each subproblem (currently set to 3) and the number of outer iterations (currently set to 10) also determine the final solution. Of course, for infinite number of total iterations it shouldn't matter but usually we don't have that much time.\n",
    "\n",
    "__TODO__: Change the number of iterations and see what happens to the objective functions. For a given number of total iterations, do you think it is better to have a high number of inner or high number of outer iterations? Why? Does this also depend on the undersampling factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial misalignment\n",
    "In the above example we simulated our data such that there is a perfect spatial match between $u$ and $v$. For real world applications this usually cannot be assumed. \n",
    "\n",
    "__TODO__: Add spatial misalignment between $u$ and $v$. This can be achieved e.g. by calling `numpy.roll` on `T2_arr` before calling `v_gt = crop_and_fill(im_mr, T2_arr)`. What is the effect on the reconstructed images? For a more \"advanced\" misalignment, have a look at notebook `BrainWeb`.\n",
    "\n",
    "__TODO__: One way to minimize spatial misalignment is to use image registration to ensure both $u$ and $v$ are well aligned. In the notebook `sirf_registration` you find information about how to register two images and also how to resample one image based on the spatial transformation estimated from the registration. Try to use this to correct for the misalignment you introduced above. For a real world example, at which point in the code would you have to carry out the registration+resampling? (some more information can also be found at the end of [MAPEM_Bowsher notebook](MAPEM_Bowsher.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathologies\n",
    "The images $u$ and $v$ show the same anatomy, just with a different contrast. Clinically more useful are of course images which show complementary image information.\n",
    "\n",
    "__TODO__: Add a pathology to either $u$ and $v$ and see how this effects the reconstruction. For something more advanced, have a loot at the notebook `BrainWeb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single anatomical prior\n",
    "So far we have alternated between two reconstruction problems. Another option is to do a single regularised reconstruction and simply use a previously reconstructed image for regularisation.\n",
    "\n",
    "__TODO__: Adapt the above code such that $u$ is reconstructed first without regularisation and is then used for a regularised reconstruction of $v$ without any further updates of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementary k-space trajectories\n",
    "We used the same k-space trajectory for $u$ and $v$. This is of course not ideal for such an optimisation, because the same k-space trajectory also means the same pattern of undersampling artefacts. Of course the artefacts in each image will be different because of the different image content but it still would be better if $u$ and $v$ were acquired with different k-space trajectories.\n",
    "\n",
    "__TODO__: Create two different k-space trajectories and compare the results to a reconstruction using the same k-space trajectories. \n",
    "\n",
    "__TODO__: Try different undersampling factors and compare results for _regular_ and _random_ undersampling patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other regularisation options\n",
    "In this example we used a TV-based regularisation, but of course other regularisers could also be used, such as directional TV.\n",
    "\n",
    "__TODO__: Have a look at the __CIL__ notebook `02_Dynamic_CT` and adapt the `SmoothJointTV` class above to use directional TV. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
