{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint TV for mutiple PET/SPECT images\n",
    "This demonstration shows how to do a synergistic reconstruction of a PET and a SPECT image. Both images show the same underlying anatomy but of course with different tracer distribution and projector. In order to make use of this similarity a joint total variation (TV) operator is used as a regularisation in an iterative image reconstruction approach. \n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Authors: Christoph Kolbitsch, Evangelos Papoutsellis, Edoardo Pasca  \n",
    "Updated for PET/SPECT: Sam Porter \\\n",
    "First version: 16th of June 2021  \n",
    "Updated: 26nd of June 2021  \n",
    "Updated for PET/SPECT: 31st of March 2022\n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2021 Rutherford Appleton Laboratory STFC.    \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.  \n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Positron Emission Tomography and Magnetic Resonance imaging\n",
    "(http://www.ccppetmr.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline\n",
    "%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is installed that we need\n",
    "!pip install brainweb nibabel --user\n",
    "!pip install ipython --user\n",
    "!pip install plotly --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "\n",
    "# Import SIRF functionality\n",
    "import sirf.STIR\n",
    "import notebook_setup\n",
    "from sirf.Utilities import examples_data_path\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "# Import CIL functionality\n",
    "from cil.framework import  BlockDataContainer, BlockGeometry\n",
    "from cil.optimisation.functions import Function, OperatorCompositionFunction,  KullbackLeibler\n",
    "from cil.optimisation.operators import GradientOperator,  CompositionOperator, LinearOperator, ProjectionMap\n",
    "from cil.optimisation.algorithms import GD\n",
    "from IPython.display import HTML\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "\n",
    "msg = sirf.STIR.MessageRedirector('info.txt', 'warnings.txt', 'errors.txt')\n",
    "\n",
    "data_path = examples_data_path('PET')\n",
    "data_path_SPECT = examples_data_path('SPECT')\n",
    "cd_to_working_dir(\"Synergistic\", \"JTV_PET_SPECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink = 0.3)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def make_cylindrical_FOV(image):\n",
    "    \"\"\"truncate to cylindrical FOV\"\"\"\n",
    "    filter = sirf.STIR.TruncateToCylinderProcessor()\n",
    "    filter.apply(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint TV reconstruction of two PET/SPECT images\n",
    "\n",
    "Assuming we want to reconstruct two PET\\SPECT images $u$ and $v$ and utilise the similarity between both images using a joint TV ($JTV$) operator we can formulate the reconstruction problem as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(u^{*}, v^{*}) \\in \\underset{u,v}{\\operatorname{argmin}}\\biggl \\{ \\mathcal{D}\\bigl(A_1u,g_1) +  \\mathcal{D}\\bigl(A_2v,g_2) + \\alpha\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v) \\biggl \\}.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "* $\\mathcal{D}\\bigl(Ax,g\\bigl)$ is a  `Data Fidelity Term` - we'll use the Kullbach Liebler divergence:\n",
    "$$ \\mathcal{D}(g|Au) = \\sum_i g_i\\log \\frac{g_i}{(Au+\\eta)_i} -(Au+\\eta)_i + g_i$$\n",
    "where $i$ is summed over LORs and\n",
    "* $JTV_{\\eta, \\lambda}(u, v) = \\sum \\sqrt{ \\lambda|\\nabla u|^{2} + (1-\\lambda)|\\nabla v|^{2} + \\eta^{2}}$\n",
    "* $A_{1}$, $A_{2}$: __PET/SPECT__ `AcquisitionModel`\n",
    "* $g_{1}$, $g_{2}$: __PET/SPECT__ `AcquisitionData`\n",
    "\n",
    "\n",
    "### Solving this problem \n",
    "\n",
    "In order to solve the above minimization problem, we will use an alternating minimisation approach, where one variable is fixed and we solve wrt. to the other variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u^{k+1} & = \\underset{u}{\\operatorname{argmin}} \\biggl \\{ \\mathcal{D}_{KL}\\bigl(A_1u,g_1) + \\alpha_{1}\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v^{k}) \\biggl \\} \\quad \\text{subproblem 1}\\\\\n",
    "v^{k+1} & = \\underset{v}{\\operatorname{argmin}} \\biggl \\{ \\mathcal{D}_{KL}\\bigl(A_2v,g_2) + \\alpha_{2}\\,\\mathrm{JTV}_{\\eta, 1-\\lambda}(u^{k+1}, v) \\biggl \\} \\quad \\text{subproblem 2}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "We are going to use a gradient descent approach to solve each of these subproblems alternatingly.\n",
    "\n",
    "The *regularisation parameter* $\\alpha$ should be different for each subproblem in order to account for the differences in the reconstruction of different modalities. We will use $\\alpha_{1}, \\alpha_{2}$ in front of the two JTVs and a $\\lambda$, $1-\\lambda$ for the first JTV and $1-\\lambda$, $\\lambda$, for the second JTV with $0<\\lambda<1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on several other notebooks and hence certain steps will be carried out with minimal documentation. If you want more explainations, then we would like to ask you to refer to the corresponding notebooks which are mentioned in the following list. The steps we are going to carry out are\n",
    "\n",
    "  - (A) Get an FDG and amyloid PET image from brainweb which we are going to use as ground truth $u_{gt}$ and $v_{gt}$ for our reconstruction (further information: `introduction` notebook)\n",
    "  \n",
    "  - (B) Create __PET/SPECT__ `AcquisitionModel` $A_{1}$ and $A_{2}$ (further information: `acquisition_model_mr_pet_ct` notebook)\n",
    "  \n",
    "  - (C) Set up the joint TV reconstruction problem\n",
    "  \n",
    "  - (D) Solve the joint TV reconstruction problem (further information on gradient descent: `gradient_descent_mr_pet_ct` notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (A) Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)\n",
    "    vol_amyl = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1, PetClass=brainweb.Amyloid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMap_arr = vol['uMap']\n",
    "amyl_arr = vol_amyl['PET']\n",
    "fdg_arr = vol['PET']\n",
    "\n",
    "# Reduce values in uMap to make more realistic\n",
    "uMap_arr /= numpy.max(uMap_arr)*10\n",
    "\n",
    "# Normalise fdg and amyloid images\n",
    "fdg_arr /= numpy.max(fdg_arr)\n",
    "amyl_arr /= numpy.max(amyl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = fdg_arr.shape[0]//2\n",
    "plot_2d_image([1,3,1], fdg_arr[slice_show, :, :], 'FDG')\n",
    "plot_2d_image([1,3,2], amyl_arr[slice_show, :, :], 'Amyloid')\n",
    "plot_2d_image([1,3,3], uMap_arr[slice_show, :, :], 'uMap', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we now have two images with FDG and Amyloid tracer distribution BUT the brain looks a bit small. We are going to reconstruct images with a FOV of 120 x 120 voxels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select central slice\n",
    "central_slice = fdg_arr.shape[0]//2\n",
    "fdg_arr = fdg_arr[central_slice, :, :]\n",
    "amyl_arr = amyl_arr[central_slice, :, :]\n",
    "uMap_arr = uMap_arr[central_slice, :, :]\n",
    "\n",
    "# Select a central ROI with 120x120\n",
    "idim = [120,120]\n",
    "offset = (numpy.array(fdg_arr.shape) - numpy.array(idim)) // 2\n",
    "fdg_arr = fdg_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "amyl_arr = amyl_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "uMap_arr = uMap_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "\n",
    "# Now we make sure our image is of shape (1, 120, 120) again because in __SIRF__ even 2D images \n",
    "# are expected to have 3 dimensions.\n",
    "fdg_arr = fdg_arr[numpy.newaxis,...]\n",
    "amyl_arr = amyl_arr[numpy.newaxis,...]\n",
    "uMap_arr = uMap_arr[numpy.newaxis,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templ_sino = sirf.STIR.AcquisitionData(os.path.join(data_path, 'template_sinogram.hs')) # create an empty template sinogram using a template\n",
    "templ_sino_SPECT = sirf.STIR.AcquisitionData(os.path.join(data_path_SPECT, 'template_sinogram.hs')) # create an empty sinogram using a template\n",
    "dim = fdg_arr.shape\n",
    "im = sirf.STIR.ImageData(templ_sino)\n",
    "voxel_size=im.voxel_sizes()\n",
    "im.initialise(dim,voxel_size)\n",
    "fdg = im.clone().fill(fdg_arr)\n",
    "amyl = im.clone().fill(amyl_arr)\n",
    "uMap = im.clone().fill(uMap_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = fdg_arr.shape[0]//2\n",
    "plot_2d_image([1,3,1], numpy.squeeze(fdg.as_array()), 'FDG')\n",
    "plot_2d_image([1,3,2], numpy.squeeze(amyl.as_array()), 'Amyloid',)\n",
    "plot_2d_image([1,3,3], numpy.squeeze(uMap.as_array()), 'uMap', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better. Now we have got images we can use for our simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Simulate noisy AcquisitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acquisition_model(uMap, templ_sino, detector_efficiency = 1, attenuation = True, itype = 'PET'):\n",
    "    \"\"\" Function to create PET/SPECT acquisition model \"\"\"\n",
    "    if itype == 'PET':\n",
    "        am = sirf.STIR.AcquisitionModelUsingRayTracingMatrix()\n",
    "        # Set up sensitivity due to attenuation and detector efficiency\n",
    "        am.set_num_tangential_LORs(5)\n",
    "        if attenuation is True:\n",
    "            asm_attn = sirf.STIR.AcquisitionSensitivityModel(uMap, am)\n",
    "            asm_attn.set_up(templ_sino)\n",
    "            bin_eff = templ_sino.get_uniform_copy(detector_efficiency)\n",
    "            print('applying attenuation (please wait, may take a while)...')\n",
    "            asm_attn.unnormalise(bin_eff)\n",
    "            asm = sirf.STIR.AcquisitionSensitivityModel(bin_eff)\n",
    "            am.set_acquisition_sensitivity(asm)\n",
    "    elif itype == 'SPECT':\n",
    "        acq_model_matrix = sirf.STIR.SPECTUBMatrix()\n",
    "        acq_model_matrix.set_resolution_model(0.2,0.2,full_3D=False)\n",
    "        if attenuation is True:\n",
    "            acq_model_matrix.set_attenuation_image(uMap)\n",
    "        am = sirf.STIR.AcquisitionModelUsingMatrix(acq_model_matrix)\n",
    "        \n",
    "    else:\n",
    "        raise Exception('Please ensure type = \"PET\" or \"SPECT\"')\n",
    "    am.set_up(templ_sino, uMap)\n",
    "    return am\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(proj_data,noise_factor = 1):  \n",
    "    \"\"\" Function to add noise to PET/SPECT Acquistion Data \"\"\"\n",
    "    proj_data_arr = proj_data.as_array() / noise_factor\n",
    "    # Data should be >=0 anyway, but add abs just to be safe\n",
    "    proj_data_arr = numpy.abs(proj_data_arr)\n",
    "    noisy_proj_data_arr = numpy.random.poisson(proj_data_arr).astype('float32');\n",
    "    noisy_proj_data = proj_data.clone()\n",
    "    noisy_proj_data.fill(noisy_proj_data_arr);\n",
    "    return noisy_proj_data*noise_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create the two __PET/SPECT__ `AcquisitionModel` $A_{1}$ and $A_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_model = get_acquisition_model(uMap, templ_sino)\n",
    "anorm = acq_model.norm() #norm of the operator to avoid any scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_model_SPECT = get_acquisition_model(uMap, templ_sino_SPECT, itype=\"SPECT\")\n",
    "anorm_SPECT = acq_model_SPECT.norm() #norm of the operator to avoid any scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simulate `AcquisitionData` $g_{1}$ and $g_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward project our images into data space\n",
    "data_fdg = acq_model.forward(fdg)\n",
    "data_amyl = acq_model_SPECT.forward(amyl)\n",
    "c_slice = data_fdg.shape[1]//2 # central slice\n",
    "sirf.STIR.show_2D_array(\"PET sino\",data_fdg.as_array()[0,c_slice,:,:])\n",
    "sirf.STIR.show_2D_array(\"SPECT sino\",data_amyl.as_array()[0,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we are going to add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise\n",
    "noisy_data_fdg = add_noise(data_fdg)\n",
    "noisy_data_amyl = add_noise(data_amyl)\n",
    "sirf.STIR.show_2D_array(\"noisy sino PET\",noisy_data_fdg.as_array()[0,c_slice,:,:])\n",
    "sirf.STIR.show_2D_array(\"noisy sino SPECT\",noisy_data_amyl.as_array()[0,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check we've done everything right and to give us something to compare our reconstruction to we are going to apply the backward/adjoint operation to do a simple image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reconstruction\n",
    "u_simple = acq_model.backward(noisy_data_fdg)/anorm**2\n",
    "v_simple = acq_model_SPECT.backward(noisy_data_amyl)/anorm_SPECT**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "plot_2d_image([1,2,1], numpy.abs(u_simple.as_array())[0, :, :], '$u_{simple}$')\n",
    "plot_2d_image([1,2,2], numpy.abs(v_simple.as_array())[0, :, :], '$v_{simple}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these images look quite poor compared to the ground truth input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C) Set up the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used mainly __SIRF__ functionality, now we are going to use __CIL__ in order to set up the reconstruction problem and then solve it. In order to be able to reconstruct both $u$ and $v$ at the same time, we will make use of `BlockDataContainer` that stores the two images in one object that can be used by __CIL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we define the `SmoothJointTV` class. Our plan is to use a slightly altered Gradient Descent (`GD`) algorithm to solve the above problems. This implements the `__call__` method required to monitor the objective value and the `gradient` method that evaluates the gradient of `JTV`.\n",
    "\n",
    "For the two subproblems, the first variations with respect to $u$ and $v$ variables are:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\biggl(A_1\\mathcal{1}-\\frac{g_{1}}{A_{1}u+\\eta}\\biggl) - \\alpha_{1} \\mathrm{div}\\bigg( \\frac{\\nabla u}{|\\nabla(u, v)|_{2,\\eta,\\lambda}}\\bigg)\\\\\n",
    "& \\biggl(A_2\\mathcal{1}-\\frac{g_{2}}{A_{2}v+\\eta}\\biggl) - \\alpha_{2} \\mathrm{div}\\bigg( \\frac{\\nabla v}{|\\nabla(u, v)|_{2,\\eta,1-\\lambda}}\\bigg)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $$|\\nabla(u, v)|_{2,\\eta,\\lambda} = \\sqrt{ \\lambda|\\nabla u|^{2} + (1-\\lambda)|\\nabla v|^{2} + \\eta^{2}}.$$\n",
    "\n",
    "and $ \\eta $ means we can avoid dividing by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothJointTV(Function):\n",
    "              \n",
    "    def __init__(self, eta, axis, lambda_par, domain_geometry):\n",
    "                \n",
    "        r'''\n",
    "        :param eta: smoothing parameter making SmoothJointTV differentiable \n",
    "        '''\n",
    "\n",
    "        super(SmoothJointTV, self).__init__(L=8)\n",
    "        \n",
    "        # smoothing parameter\n",
    "        self.eta = eta   \n",
    "        \n",
    "        # GradientOperator\n",
    "        self.grad = GradientOperator(domain_geometry, backend='numpy')\n",
    "                \n",
    "        # Which variable to differentiate\n",
    "        self.axis = axis\n",
    "        \n",
    "        if self.eta==0:\n",
    "            raise ValueError('Need positive value for eta')\n",
    "            \n",
    "        self.lambda_par=lambda_par    \n",
    "                                    \n",
    "                            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        r\"\"\" x is BlockDataContainer that contains (u,v). Actually x is a BlockDataContainer that contains 2 BDC.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, BlockDataContainer):\n",
    "            raise ValueError('__call__ expected BlockDataContainer, got {}'.format(type(x))) \n",
    "\n",
    "        tmp = numpy.abs((self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.eta**2).sqrt().sum())\n",
    "\n",
    "        return tmp    \n",
    "                        \n",
    "             \n",
    "    def gradient(self, x, out=None):\n",
    "        \n",
    "        denom = (self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.eta**2).sqrt()         \n",
    "        \n",
    "        if self.axis==0:            \n",
    "            num = self.lambda_par*self.grad.direct(x[0])                        \n",
    "        else:            \n",
    "            num = (1-self.lambda_par)*self.grad.direct(x[1])            \n",
    "\n",
    "        if out is None:    \n",
    "            tmp = self.grad.range.allocate()\n",
    "            tmp[self.axis].fill(self.grad.adjoint(num.divide(denom)))\n",
    "            return tmp\n",
    "        else:                                \n",
    "            self.grad.adjoint(num.divide(denom), out=out[self.axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now slightly alter the gradient descent `update` method in order to enforce positivity. This update projects our image onto the set of positive numbers and modifies our algorithm slightly to something known as the Gradient Projection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateGD(self):\n",
    "    '''Single iteration of Gradient Descent'''\n",
    "    self.objective_function.gradient(self.x, out=self.x_update)\n",
    "    self.x_update.multiply(self.step_size, out = self.x_update)\n",
    "    self.x.subtract(self.x_update, out = self.x)\n",
    "\n",
    "    # remove any negative values\n",
    "    self.x.add(self.x.abs(), out = self.x) ## new line\n",
    "    self.x.divide(2, out = self.x)         ## new line\n",
    "\n",
    "GD.update = updateGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put everything together and define our two objective functions which solve the two subproblems which we defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha1 = 0.05\n",
    "alpha2 = 0.05\n",
    "lambda_par = 0.5\n",
    "eta = 1e-12\n",
    "eta_sino = templ_sino.get_uniform_copy(eta) # uniform sino for KL diveregence smoothing\n",
    "eta_sino_SPECT = templ_sino_SPECT.get_uniform_copy(eta) # uniform sino for KL diveregence smoothing\n",
    "\n",
    "# BlockGeometry for the two modalities\n",
    "bg = BlockGeometry(u_simple, v_simple)\n",
    "\n",
    "# Projection map from BlockDataContainer to PET or SPECT image\n",
    "L1 = ProjectionMap(bg, index=0)\n",
    "L2 = ProjectionMap(bg, index=1)\n",
    "\n",
    "# Fidelity terms based on the acqusition data\n",
    "f1 = KullbackLeibler(b=noisy_data_fdg, eta=eta_sino)\n",
    "f2 = KullbackLeibler(b=noisy_data_amyl, eta=eta_sino_SPECT)\n",
    "\n",
    "# JTV for each of the subproblems\n",
    "JTV1 = alpha1*SmoothJointTV(eta=eta, axis=0, lambda_par = lambda_par , domain_geometry = uMap)\n",
    "JTV2 = alpha2*SmoothJointTV(eta=eta, axis=1, lambda_par = 1-lambda_par, domain_geometry = uMap)\n",
    "\n",
    "# Compose the two objective functions\n",
    "op1 = CompositionOperator(acq_model, L1) # project our block data container to only image u space then apply our acquision model\n",
    "op2 = CompositionOperator(acq_model_SPECT, L2)\n",
    "objective1 = OperatorCompositionFunction(f1 , op1) + JTV1 # L1(u,v) = F(Au) + R(u,v)\n",
    "objective2 = OperatorCompositionFunction(f2 , op2) + JTV2 # L2(u,v) = F(Bv) + R(u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (D) Solve the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with a cylindrical uniform image\n",
    "init = make_cylindrical_FOV(fdg.get_uniform_copy(1))\n",
    "x0 = BlockDataContainer(init,init)\n",
    "\n",
    "step_size = 0.25\n",
    "\n",
    "# Here we add the functionality for a preconditioned gradient step to improve the convergence rate\n",
    "sens_im_arr = acq_model.backward(templ_sino.get_uniform_copy(1)).as_array()\n",
    "sens_im_arr[sens_im_arr == 0] = 1\n",
    "sens_im = fdg.clone().fill(sens_im_arr)\n",
    "\n",
    "sens_im_arr_SPECT = acq_model_SPECT.backward(templ_sino_SPECT.get_uniform_copy(1)).as_array()\n",
    "sens_im_arr_SPECT[sens_im_arr_SPECT == 0] = 1\n",
    "sens_im_SPECT = fdg.clone().fill(sens_im_arr_SPECT)\n",
    "\n",
    "sens_bdc = BlockDataContainer(sens_im,sens_im_SPECT)\n",
    "\n",
    "outeriter = 4\n",
    "inneriter = 4\n",
    "\n",
    "# We are also going to log the value of the objective functions\n",
    "obj1_val_it = []\n",
    "obj2_val_it = []\n",
    "# and the images\n",
    "images = []\n",
    "\n",
    "for i in range(outeriter):\n",
    "    # update the preconditioner with the current image\n",
    "    precond = x0/sens_bdc\n",
    "    step = precond * step_size\n",
    "    images.append(x0.copy())\n",
    "\n",
    "    gd1 = GD(x0, objective1, step_size=step, \\\n",
    "          max_iteration = inneriter, update_objective_interval = 1)\n",
    "    gd1.run(verbose=1)\n",
    "    \n",
    "    # We skip the first one because it gets repeated\n",
    "    obj1_val_it.extend(gd1.objective[1:])\n",
    "    \n",
    "    # Here we are going to do a little \"trick\" in order to better see, when each subproblem is optimised, we\n",
    "    # are going to append NaNs to the objective function which is currently not optimised. The NaNs will not\n",
    "    # show up in the final plot and hence we can nicely see each subproblem.\n",
    "    obj2_val_it.extend(numpy.ones_like(gd1.objective[1:])*numpy.nan)\n",
    "    \n",
    "    gd2 = GD(gd1.solution, objective2, step_size=step, \\\n",
    "          max_iteration = inneriter, update_objective_interval = 1)\n",
    "    gd2.run(verbose=1)\n",
    "    \n",
    "    obj2_val_it.extend(gd2.objective[1:])\n",
    "    obj1_val_it.extend(numpy.ones_like(gd2.objective[1:])*numpy.nan)\n",
    "    \n",
    "    x0 = gd2.solution.clone()\n",
    "    \n",
    "    print('* * * * * * Outer Iteration ', i+1, ' * * * * * *\\n')\n",
    "\n",
    "images.append(x0.copy())\n",
    "print('* * * * * * Reconstruction Complete * * * * * *\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can look at the images $u_{jtv}$ and $v_{jtv}$ and compare them to the simple reconstruction $u_{simple}$ and $v_{simple}$ and the original ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_jtv = numpy.squeeze(numpy.abs(x0[0].as_array()))\n",
    "v_jtv = numpy.squeeze(numpy.abs(x0[1].as_array()))\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([2,3,1], numpy.squeeze(numpy.abs(u_simple.as_array())), '$u_{simple}$')\n",
    "plot_2d_image([2,3,2], u_jtv, '$u_{JTV}$')\n",
    "plot_2d_image([2,3,3], numpy.squeeze(numpy.abs(fdg_arr)), '$u_{gt}$') \n",
    "\n",
    "plot_2d_image([2,3,4], numpy.squeeze(numpy.abs(v_simple.as_array())), '$v_{simple}$')\n",
    "plot_2d_image([2,3,5], v_jtv, '$v_{JTV}$')\n",
    "plot_2d_image([2,3,6], numpy.squeeze(numpy.abs(amyl_arr)), '$v_{gt}$') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(obj1_val_it, 'o-', label='subproblem PET')\n",
    "plt.plot(obj2_val_it, '+-', label='subproblem SPECT')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Value of objective function')\n",
    "plt.title('Objective functions')\n",
    "plt.legend()\n",
    "\n",
    "# Logarithmic y-axis\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at how the images changed throughout the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = 'iframe_connected'\n",
    "plt.ioff()\n",
    "# choose which subproblem to display\n",
    "subprob = 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "arrays = []\n",
    "ims = []\n",
    "for i, image in enumerate(images):\n",
    "    array = numpy.squeeze(image[subprob].as_array())\n",
    "    arrays.append(array)\n",
    "    im = ax.imshow(array, animated = True, vmin=0, vmax=fdg_arr.max())\n",
    "    im.set_clim(0,array.max())\n",
    "    title = plt.text(0.5,1.01,str(i), ha=\"center\",va=\"bottom\",color=numpy.random.rand(3),\n",
    "                     transform=ax.transAxes, fontsize=\"large\")\n",
    "    if i == 0:\n",
    "        ax.imshow(array)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    ims.append([im,title,])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval = 750, blit = True, repeat_delay = 1000)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% delete temporary files\n",
    "wdpath = os.getcwd()\n",
    "for filename in glob.glob(os.path.join(wdpath, \"tmp*\")):\n",
    "    os.remove(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a good demonstration for a synergistic image reconstruction of two different images. The following gives a few suggestions of what to do next and also how to extend this notebook to other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of iterations\n",
    "In our problem we have several regularisation parameters such as $\\alpha_{1}$, $\\alpha_{2}$ and $\\lambda$. In addition, the number of inner iterations for each subproblem (currently set to 3) and the number of outer iterations (currently set to 10) also determine the final solution. Of course, for infinite number of total iterations it shouldn't matter but usually we don't have that much time.\n",
    "\n",
    "__TODO__: Change the number of iterations and see what happens to the objective functions. For a given number of total iterations, do you think it is better to have a high number of inner or high number of outer iterations? Why? Does this also depend on the undersampling factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial misalignment\n",
    "In the above example we simulated our data such that there is a perfect spatial match between $u$ and $v$. For real world applications this usually cannot be assumed. \n",
    "\n",
    "__TODO__: Add spatial misalignment between $u$ and $v$. This can be achieved e.g. by calling `numpy.roll` on `amyl_arr` before calling `v_gt = crop_and_fill(im_mr, amyl_arr)`. What is the effect on the reconstructed images? For a more \"advanced\" misalignment, have a look at notebook `BrainWeb`.\n",
    "\n",
    "__TODO__: One way to minimize spatial misalignment is to use image registration to ensure both $u$ and $v$ are well aligned. In the notebook `sirf_registration` you find information about how to register two images and also how to resample one image based on the spatial transformation estimated from the registration. Try to use this to correct for the misalignment you introduced above. For a real world example, at which point in the code would you have to carry out the registration+resampling? (some more information can also be found at the end of [MAPEM_Bowsher notebook](MAPEM_Bowsher.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathologies\n",
    "The images $u$ and $v$ show the same anatomy, just with a different distributions. Clinically more useful are of course images which show complementary image information.\n",
    "\n",
    "__TODO__: Add a pathology to either $u$ and $v$ and see how this effects the reconstruction. For something more advanced, have a loot at the notebook `BrainWeb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single anatomical prior\n",
    "So far we have alternated between two reconstruction problems. Another option is to do a single regularised reconstruction and simply use a previously reconstructed image for regularisation.\n",
    "\n",
    "__TODO__: Adapt the above code such that $u$ is reconstructed first without regularisation and is then used for a regularised reconstruction of $v$ without any further updates of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other regularisation options\n",
    "In this example we used a TV-based regularisation, but of course other regularisers could also be used, such as directional TV.\n",
    "\n",
    "__TODO__: Have a look at the __CIL__ notebook `02_Dynamic_CT` and adapt the `SmoothJointTV` class above to use directional TV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other optimisation algorithms\n",
    "In this example we used a Gradient Descent for ease of understanding, but other optimisation algorithms will be more appropriate and more efficient.\n",
    "\n",
    "__TODO__: Have a look at the __CIL__ Documentaion and see if you can implement other algorithms such as PDHG or ADMM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution Modelling\n",
    "In this example we do not use any resolution modelling for PET or SPECT\n",
    "\n",
    "__TODO__: Implement resolution modelling and see what difference it makes to the reconstructed images"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
