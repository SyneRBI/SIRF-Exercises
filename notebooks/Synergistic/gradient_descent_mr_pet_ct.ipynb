{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent for MR, PET and CT\n",
    "This demonstration shows how to do image reconstruction using gradient descent for different modalities. \n",
    "\n",
    "It builds on the the notebook *acquisition_model_mr_pet_ct.ipynb*. The first part of the notebook which creates acquisition models and simulates data from the brainweb is the same code but with fewer comments. If anything is unclear, then please refer to the other notebook to get some further information.\n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: At present, this notebook actually implements gradient ascent for PET, and it uses some sign tricks for CT/MR. This is because the `sirf.STIR` objective functions correspond to log-likelihood (or posterior), which needs to be maximised, while this is not so common in CT/MR. We will have to edit this notebook to make this clearer in the next version!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christoph Kolbitsch, Edoardo Pasca  \n",
    "First version: 23rd of April 2021  \n",
    "Updated: 26nd of June 2021  \n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2015 - 2017, 2021 Rutherford Appleton Laboratory STFC.  \n",
    "Copyright 2015 - 2019 University College London.   \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Synergistic Reconstruction for Biomedical Imaging (SyneRBI) (http://www.synerbi.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib widget\n",
    "\n",
    "# Setup the working directory for the notebook\n",
    "import notebook_setup\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "cd_to_working_dir('Synergistic', 'GD_MR_PET_CT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import MR, PET and CT functionality\n",
    "import sirf.Gadgetron as mr\n",
    "import sirf.STIR as pet\n",
    "import cil.framework as ct\n",
    "\n",
    "from sirf.Utilities import examples_data_path\n",
    "from cil.optimisation.functions import LeastSquares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if your installation comes with the ASTRA. If not, we won't be able to illustrate CT forward projections via CIL unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from cil.plugins.astra.operators import ProjectionOperator as ap\n",
    "    have_astra = True\n",
    "except:\n",
    "    have_astra = False\n",
    "    print(\"CIL ASTRA plugin is not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    \n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb. We will use a FDG image for PET and the PET uMAP for CT. MR usually provides qualitative images with an image contrast proportional to difference in T1, T2 or T2* depending on the sequence parameters. Nevertheless, we will make our life easy, by directly using the T1 map provided by the brainweb for MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDG_arr  = vol['PET']\n",
    "T1_arr   = vol['T1']\n",
    "uMap_arr = vol['uMap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = FDG_arr.shape[0]//2\n",
    "plot_2d_image([1,3,1], FDG_arr[slice_show, 100:-100, 100:-100], 'FDG', cmap=\"hot\")\n",
    "plot_2d_image([1,3,2], T1_arr[slice_show, 100:-100, 100:-100], 'T1', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,3,3], uMap_arr[slice_show, 100:-100, 100:-100], 'uMap', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will set up the acquisition models for __MR__, __PET__ and __CT__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create MR AcquisitionData\n",
    "mr_acq = mr.AcquisitionData(examples_data_path('MR') + '/grappa2_1rep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. calculate CSM\n",
    "preprocessed_data = mr.preprocess_acquisition_data(mr_acq)\n",
    "preprocessed_data.sort()\n",
    "\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 50\n",
    "csm.calculate(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. calculate image template\n",
    "recon = mr.FullySampledReconstructor()\n",
    "recon.set_input(preprocessed_data)\n",
    "recon.process()\n",
    "im_mr = recon.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create AcquisitionModel\n",
    "acq_mod_mr = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "\n",
    "# Supply csm to the acquisition model \n",
    "acq_mod_mr.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create PET AcquisitionData\n",
    "templ_sino = pet.AcquisitionData(examples_data_path('PET') + \"/thorax_single_slice/template_sinogram.hs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create a template PET ImageData\n",
    "im_pet = pet.ImageData(templ_sino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create AcquisitionModel\n",
    "\n",
    "# create PET acquisition model\n",
    "acq_mod_pet = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_mod_pet.set_up(templ_sino, im_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define AcquisitionGeometry\n",
    "angles = numpy.linspace(0, 360, 50, True, dtype=numpy.float32)\n",
    "ag2d = ct.AcquisitionGeometry.create_Cone2D((0,-1000), (0, 500))\\\n",
    "          .set_panel(128,pixel_size=3.104)\\\n",
    "          .set_angles(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get ImageGeometry\n",
    "ct_ig = ag2d.get_ImageGeometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create ImageData\n",
    "im_ct = ct_ig.allocate(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create AcquisitionModel\n",
    "if have_astra:\n",
    "    acq_mod_ct = ap(ct_ig, ag2d, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the acquisition models to create simulated raw data and then do a simple reconstruction to have some initial images (i.e. starting point) for our gradient descent algorithms. For each modality we will:\n",
    "\n",
    " * Fill an image template (`im_mr`, `im_pet`, `im_ct`)\n",
    " * Create raw data (`raw_mr`, `raw_pet`, `raw_ct`)\n",
    " * Reconstruct an initial guess of our image using `backward`/`adjoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR\n",
    "im_mr = crop_and_fill(im_mr, T1_arr)\n",
    "raw_mr = acq_mod_mr.forward(im_mr)\n",
    "bwd_mr = acq_mod_mr.backward(raw_mr)\n",
    "\n",
    "# PET\n",
    "im_pet = crop_and_fill(im_pet, FDG_arr)\n",
    "raw_pet = acq_mod_pet.forward(im_pet)\n",
    "bwd_pet = acq_mod_pet.backward(raw_pet)\n",
    "\n",
    "# CT\n",
    "if have_astra:\n",
    "    im_ct = crop_and_fill(im_ct, uMap_arr)\n",
    "    raw_ct = acq_mod_ct.direct(im_ct)\n",
    "    bwd_ct = acq_mod_ct.adjoint(raw_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent or ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are basically two things we need to be able to run a gradient descent algorithm. First we need an objective function (`obj_func`) which calculates the difference between the acquired raw data and our current image estimate. Second, we need to know the gradient of the objective function (`obj_func_grad`), because we need to know how we have to update our current image estimate in order to decrease the value of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `obj_func` and `obj_func_grad` are modality specific and so here we will go through all modalities and define them. Let's start with __PET__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise in __PET__ follows a Poisson distribution and hence we can use a Poisson log-likelihood as our objective function. Luckily enough this is already part of __SIRF__ and hence we can simply create the objective function by providing the raw __PET__ data and __PET__ image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun_pet_sirf = pet.make_Poisson_loglikelihood(raw_pet)\n",
    "obj_fun_pet_sirf.set_up(bwd_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `obj_func_pet` cannot be called directly but `obj_func_pet.value()` has to be used, we write a quick wrapper around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_pet(curr_image_estimate):\n",
    "    return(obj_fun_pet_sirf.value(curr_image_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is also already implemented and can simply be calculated by calling `obj_fun_pet.gradient`. Nevertheless, to be more explicit and consistent with the other modalities we will define a new function to do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_grad_pet(curr_image_estimate):\n",
    "    # The 0 here means, only the gradient for subset 0 is returned. \n",
    "    # We will just accept this as is here, because subsets are too advanced for this demo.\n",
    "    return(obj_fun_pet_sirf.gradient(curr_image_estimate, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __PET__ (and also in __CT__) we need to make sure that all the image values are positive, so we will create a small function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positive(image_data):\n",
    "    # The idea is to create an image with all 0s (zero_image) and then we can take the maximum over each voxel.\n",
    "    # If the voxel value is larger than 0, then the original value is returned. If it is smaller than 0, then \n",
    "    # the value of the zero_image, i.e. 0, is returned.\n",
    "    zero_image = image_data.clone()\n",
    "    zero_image.fill(0.0)\n",
    "    image_data = image_data.maximum(zero_image)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, __PET__ is all done, now we will continue with __CT__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For __CT__ we use a least squares objective function which is already available from __CIL__ . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if have_astra:\n",
    "    least_squares_cil = LeastSquares(acq_mod_ct, raw_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we have the same function interface as for __PET__ and __MR__ we will also quickly wrap these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_ct(curr_image_estimate):\n",
    "    return(least_squares_cil(curr_image_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_grad_ct(curr_image_estimate):\n",
    "    # We are returning the negative gradient, because we are going to add it later to our image estimate\n",
    "    return(-least_squares_cil.gradient(curr_image_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last but not least __MR__ . If you want to know more about the objective function of __MR__ and its gradient, then please have a look at the notebook *d_undersampled_reconstructions.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_mr(curr_image_estimate):\n",
    "    c =  acq_mod_mr.forward(curr_image_estimate) - raw_mr\n",
    "    return(0.5 * c.norm() ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_grad_mr(curr_image_estimation):\n",
    "    # We are returning the negative gradient, because we are going to add it later to our image estimate\n",
    "    return(-acq_mod_mr.backward(acq_mod_mr.forward(curr_image_estimate) - raw_mr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our `obj_func` and `obj_func_grad` we will select one modality and then implement the gradient descent/ascent appproach. We also need an image `init_image` to start with. Here we will simply use the simple reconstruction which we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_modality = 'pet' # pet, ct, mr\n",
    "\n",
    "if curr_modality.lower() == 'pet':\n",
    "    obj_fun = obj_fun_pet\n",
    "    obj_fun_grad = obj_fun_grad_pet\n",
    "    init_image = bwd_pet\n",
    "elif curr_modality.lower() == 'ct':\n",
    "    if not has_astra:\n",
    "        raise ImportError('ASTRA toolbox is not installed')\n",
    "    obj_fun = obj_fun_ct\n",
    "    obj_fun_grad = obj_fun_grad_ct\n",
    "    init_image = bwd_ct  \n",
    "elif curr_modality.lower() == 'mr':\n",
    "    obj_fun = obj_fun_mr\n",
    "    obj_fun_grad = obj_fun_grad_mr\n",
    "    init_image = bwd_mr  \n",
    "else:\n",
    "    raise NameError('{:} not recognised'.format(curr_modality,))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the probably most important paramter for gradient descent, the infamous __step-size__ . Unfortunately, the gradient only gives us the direction along which we need to update the image, but does not tell us by how much we have to go in this direction. Therefore, we need to define how far we step along the gradient direction in each iteration by hand. \n",
    "\n",
    "To make sure the step-size is adapted to each modality as much as possible, we won't define the step-size directly, but we will calculate the step-size as `tau` times the norm of the current image estimate. Nevertheless, this is still just a guess. \n",
    "\n",
    "If the step-size is too small, we have very slow convergence. If the step-size is too large, then we are overstepping our target image and the objective function starts to oscillate. The value below is reasonable for __PET__, but for the other modalities you will have to optimise it to ensure good convergence.\n",
    "\n",
    "Note that in the implementation below, we have used a \"relative step-size\" `tau` such that the norm of the increment (or step) is `tau curr_image_estmate.norm()`, which is intuitive, but theoretically and practically not well-motivated. Feel free to experiment with other step-size rules (see the end of this notebook for some suggestions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative step-size\n",
    "tau = .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second most important parameter is of course the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "num_iters = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create an image as a starting point for the gradient descent algorithm and we will also create a numpy array, where we can store the image estimate at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial image\n",
    "curr_image_estimate = init_image.clone()\n",
    "\n",
    "# Array for images at each iteration\n",
    "image_iteration = numpy.ndarray((num_iters + 1,), dtype=object)\n",
    "image_iteration[0] = curr_image_estimate\n",
    "\n",
    "# Variable to store the current value of the objective function\n",
    "obj_func_values = numpy.zeros(shape=(num_iters + 1,))\n",
    "obj_func_values[0] = obj_fun(curr_image_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally write down our gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, num_iters+1):  \n",
    "    # First we calculate the gradient to find out how we need to update our current image estimate\n",
    "    grad = obj_fun_grad(curr_image_estimate)\n",
    "\n",
    "    # Compute step-size relative to current image-norm\n",
    "    step_size = tau * curr_image_estimate.norm() / grad.norm()\n",
    "\n",
    "    # Perform gradient ascent step\n",
    "    curr_image_estimate = curr_image_estimate + step_size * grad\n",
    "\n",
    "    # In PET and CT we have to ensure values are positive. \n",
    "    if curr_modality.lower() == 'ct' or curr_modality.lower() == 'pet':\n",
    "        curr_image_estimate = make_positive(curr_image_estimate)\n",
    "    \n",
    "    # Compute objective function value for plotting, and write some diagnostics\n",
    "    obj_func_values[i] = obj_fun(curr_image_estimate)\n",
    "    \n",
    "    # We use numpy.abs here to be compatible with MR complex data\n",
    "    image_iteration[i] = curr_image_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot objective function values\n",
    "plt.figure()\n",
    "plt.title('Objective function value')\n",
    "plt.plot(obj_func_values, 'o-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a slice for different iterations\n",
    "plt.figure();\n",
    "for i in range(4):\n",
    "    curr_it = i*8\n",
    "    image = numpy.abs(image_iteration[curr_it].as_array())\n",
    "    centre_slice = image.shape[0]//2\n",
    "    if len(image.shape) == 3: # PET, MR\n",
    "        plot_2d_image([2,2,i+1], image[centre_slice,:,:], 'It '+str(curr_it), cmap=\"viridis\")\n",
    "    else: # CT\n",
    "        plot_2d_image([2,2,i+1], image, 'It '+str(curr_it), cmap=\"viridis\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could now experiment witih different step-size schemes. Some suggestions:\n",
    "- change to a fixed step size, i.e. remove the `grad.norm()`\n",
    "- use a decreasing step-size (sometimes called \"relaxation\"), e.g. $\\alpha/(n+\\beta)$ with $n$ the iteration number\n",
    "- for MR and CT, the maximum step-size can be determined  from the norm of the forward model\n",
    "- back-tracking line search\n",
    "\n",
    "You could also have a look at the `cil.optimisation.algorithms.GD` algorithms from CIL, as illustrated in the [01_optimisation_gd_fista CIL-demo](\n",
    "https://github.com/TomographicImaging/CIL-Demos/blob/main/demos/2_Iterative/01_optimisation_gd_fista.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
