{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint TV for mutiple PET images\n",
    "This demonstration shows how to do a synergistic reconstruction of two PET images. Both PET images show the same underlying anatomy but of course with different tracer distribution. In order to make use of this similarity a joint total variation (TV) operator is used as a regularisation in an iterative image reconstruction approach. \n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Authors: Christoph Kolbitsch, Evangelos Papoutsellis, Edoardo Pasca  \n",
    "Updated for PET: Sam Porter \\\n",
    "First version: 16th of June 2021  \n",
    "Updated: 26nd of June 2021  \n",
    "Updated for PET: 31st of March 2022\n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2021 Rutherford Appleton Laboratory STFC.    \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.  \n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Positron Emission Tomography and Magnetic Resonance imaging\n",
    "(http://www.ccppetmr.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline\n",
    "%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is installed that we need\n",
    "!pip install brainweb nibabel --user\n",
    "!pip install ipython --user\n",
    "!pip install plotly --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "import os\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "\n",
    "# Import SIRF functionality\n",
    "import notebook_setup\n",
    "import sirf.Gadgetron as mr\n",
    "from sirf.Utilities import examples_data_path\n",
    "import sirf.STIR as pet\n",
    "import plotly.io as pio\n",
    "\n",
    "# Import CIL functionality\n",
    "from cil.framework import  BlockDataContainer, BlockGeometry\n",
    "from cil.optimisation.functions import Function, OperatorCompositionFunction, KullbackLeibler\n",
    "from cil.optimisation.operators import GradientOperator, CompositionOperator, LinearOperator, ProjectionMap\n",
    "from cil.optimisation.algorithms import GD\n",
    "from IPython.display import HTML\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "\n",
    "msg = pet.MessageRedirector('info.txt', 'warnings.txt', 'errors.txt')\n",
    "\n",
    "data_path = examples_data_path('PET')\n",
    "\n",
    "cd_to_working_dir(\"Synergistic\", \"JTV_PET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink = 0.3)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    \n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)\n",
    "\n",
    "def make_cylindrical_FOV(image):\n",
    "    \"\"\"truncate to cylindrical FOV\"\"\"\n",
    "    filter = pet.TruncateToCylinderProcessor()\n",
    "    filter.apply(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint TV reconstruction of two PET/SPECT images\n",
    "\n",
    "Assume we want to reconstruct two PET\\SPECT images $u$ and $v$ and utilse the similarity between both images using a joint TV ($JTV$) operator we can formulate the reconstruction problem as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(u^{*}, v^{*}) \\in \\underset{u,v}{\\operatorname{argmin}}\\biggl \\{ \\mathcal{D}\\bigl(A_1u,g_1) +  \\mathcal{D}\\bigl(A_2v,g_2) + \\alpha\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v) \\biggl \\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "* $\\mathcal{D}\\bigl(Au,g)$ `Data Fidelity Term` - we'll use the Kullbach Liebler divergence:\n",
    "$$ \\mathcal{D}(g|Au) = \\sum_i \\log g_i\\frac{g_i}{(Au)_i} -(Au+\\eta)_i + g_i$$ $$\n",
    "* $JTV_{\\eta, \\lambda}(u, v) = \\sum \\sqrt{ \\lambda|\\nabla u|^{2} + (1-\\lambda)|\\nabla v|^{2} + \\eta^{2}}$\n",
    "* $A_{1}$, $A_{2}$: __PET/SPECT__ `AcquisitionModel`\n",
    "* $g_{1}$, $g_{2}$: __PET/SPECT__ `AcquisitionData`\n",
    "\n",
    "\n",
    "### Solving this problem \n",
    "\n",
    "In order to solve the above minimization problem, we will use an alternating minimisation approach, where one variable is fixed and we solve wrt to the other variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u^{k+1} & = \\underset{u}{\\operatorname{argmin}} \\biggl \\{ \\mathcal{D}_{KL}\\bigl(A_1u,g_1) + \\alpha_{1}\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v^{k}) \\biggl \\} \\quad \\text{subproblem 1}\\\\\n",
    "v^{k+1} & = \\underset{v}{\\operatorname{argmin}} \\biggl \\{ \\mathcal{D}_{KL}\\bigl(A_2v,g_2) + \\alpha_{2}\\,\\mathrm{JTV}_{\\eta, 1-\\lambda}(u^{k+1}, v) \\biggl \\} \\quad \\text{subproblem 2}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "We are going to use a gradient descent approach to solve each of these subproblems alternatingly.\n",
    "\n",
    "The *regularisation parameter* `alpha` should be different for each subproblem. But not to worry at this stage. Maybe we should use $\\alpha_{1}, \\alpha_{2}$ in front of the two JTVs and a $\\lambda$, $1-\\lambda$ for the first JTV and $1-\\lambda$, $\\lambda$, for the second JTV with $0<\\lambda<1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on several other notebooks and hence certain steps will be carried out with minimal documentation. If you want more explainations, then we would like to ask you to refer to the corresponding notebooks which are mentioned in the following list. The steps we are going to carry out are\n",
    "\n",
    "  - (A) Get an FDG and amyloid PET image from brainweb which we are going to use as ground truth $u_{gt}$ and $v_{gt}$ for our reconstruction (further information: `introduction` notebook)\n",
    "  \n",
    "  - (B) Create __PET/SPECT__ `AcquisitionModel` $A_{1}$ and $A_{2}$ (further information: `acquisition_model_mr_pet_ct` notebook)\n",
    "  \n",
    "  - (C) Set up the joint TV reconstruction problem\n",
    "  \n",
    "  - (D) Solve the joint TV reconstruction problem (further information on gradient descent: `gradient_descent_mr_pet_ct` notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (A) Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)\n",
    "    vol_amyl = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1, PetClass=brainweb.Amyloid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMap_arr = vol['uMap']\n",
    "amyl_arr = vol_amyl['PET']\n",
    "fdg_arr = vol['PET']\n",
    "\n",
    "# Reduce values in uMamp to make more realistic\n",
    "uMap_arr /= numpy.max(uMap_arr)*10\n",
    "\n",
    "# Normalise fdg and amyloid images\n",
    "fdg_arr /= numpy.max(fdg_arr)\n",
    "amyl_arr /= numpy.max(amyl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = fdg_arr.shape[0]//2\n",
    "plot_2d_image([1,3,1], fdg_arr[slice_show, :, :], 'FDG')\n",
    "plot_2d_image([1,3,2], amyl_arr[slice_show, :, :], 'Amyloid')\n",
    "plot_2d_image([1,3,3], uMap_arr[slice_show, :, :], 'uMap', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we got to two images with FDG and Amyloid tracer distribution BUT they brain looks a bit small. Spoiler alert: We are going to reconstruct images with a FOV 120x120 voxels\n",
    "\n",
    "In order to do this we are going to use an image `rescale` from the skimage package and simply rescale the image by a factor 2 and then crop it. To speed things up, we are going to already select a single slice because also our MR scan is going to be 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select central slice\n",
    "central_slice = fdg_arr.shape[0]//2\n",
    "fdg_arr = fdg_arr[central_slice, :, :]\n",
    "amyl_arr = amyl_arr[central_slice, :, :]\n",
    "uMap_arr = uMap_arr[central_slice, :, :]\n",
    "# Select a central ROI with 120x120\n",
    "# We could also skip this because it is automaticall done by crop_and_fill() \n",
    "# but we would like to check if we did the right thing\n",
    "idim = [120, 120]\n",
    "offset = (numpy.array(fdg_arr.shape) - numpy.array(idim)) // 2\n",
    "fdg_arr = fdg_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "amyl_arr = amyl_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "uMap_arr = uMap_arr[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1]]\n",
    "\n",
    "# Now we make sure our image is of shape (1, 120, 120) again because in __SIRF__ even 2D images \n",
    "# are expected to have 3 dimensions.\n",
    "fdg_arr = fdg_arr[numpy.newaxis,...]\n",
    "amyl_arr = amyl_arr[numpy.newaxis,...]\n",
    "uMap_arr = uMap_arr[numpy.newaxis,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templ_sino = pet.AcquisitionData(os.path.join(data_path, 'template_sinogram.hs'))\n",
    "dim = fdg_arr.shape\n",
    "im = pet.ImageData(templ_sino)\n",
    "voxel_size=im.voxel_sizes()\n",
    "im.initialise(dim,voxel_size)\n",
    "fdg = crop_and_fill(im, fdg_arr)\n",
    "amyl = crop_and_fill(im, amyl_arr)\n",
    "uMap = crop_and_fill(im, uMap_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = fdg_arr.shape[0]//2\n",
    "plot_2d_image([1,3,1], numpy.squeeze(fdg.as_array()), 'FDG')\n",
    "plot_2d_image([1,3,2], numpy.squeeze(amyl.as_array()), 'Amyloid',)\n",
    "plot_2d_image([1,3,3], numpy.squeeze(uMap.as_array()), 'uMap', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that looks better. Now we have got images we can use for our simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Simulate noisy AcquisitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acquisition_model(uMap, templ_sino, detector_efficiency = 1, attenuation = True):\n",
    "    \"\"\" Function to create PET acquisition model \"\"\"\n",
    "    am = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "    # Set up sensitivity due to attenuation\n",
    "    am.set_num_tangential_LORs(5)\n",
    "    if attenuation is True:\n",
    "        asm_attn = pet.AcquisitionSensitivityModel(uMap, am)\n",
    "        asm_attn.set_up(templ_sino)\n",
    "        bin_eff = templ_sino.get_uniform_copy(detector_efficiency)\n",
    "        print('applying attenuation (please wait, may take a while)...')\n",
    "        asm_attn.unnormalise(bin_eff)\n",
    "        asm = pet.AcquisitionSensitivityModel(bin_eff)\n",
    "        am.set_acquisition_sensitivity(asm)\n",
    "    am.set_up(templ_sino, uMap)\n",
    "    return am\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(proj_data,noise_factor = 0.05):  \n",
    "    \"\"\" Function to add noise to PET/SPECT Acquistion Data \"\"\"\n",
    "    proj_data_arr = proj_data.as_array() / noise_factor\n",
    "    # Data should be >=0 anyway, but add abs just to be safe\n",
    "    proj_data_arr = numpy.abs(proj_data_arr)\n",
    "    noisy_proj_data_arr = numpy.random.poisson(proj_data_arr).astype('float32');\n",
    "    noisy_proj_data = proj_data.copy()\n",
    "    noisy_proj_data.fill(noisy_proj_data_arr);\n",
    "    return noisy_proj_data*noise_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create the two __PET__ `AcquisitionModel` $A_{1}$ and $A_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_model = get_acquisition_model(uMap, templ_sino)\n",
    "anorm = acq_model.norm() #norm of the operator to avoid any scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simulate `AcquisitionData` $g_{1}$ and $g_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward project our images into data space\n",
    "data_fdg = acq_model.forward(fdg)\n",
    "data_amyl = acq_model.forward(amyl)\n",
    "c_slice = data_fdg.shape[1]//2 # central slice\n",
    "pet.show_2D_array(\"sino\",data_fdg.as_array()[0,c_slice,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we are going to add some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise\n",
    "noisy_data_fdg = add_noise(data_fdg)\n",
    "noisy_data_amyl = add_noise(data_amyl)\n",
    "pet.show_2D_array(\"noisy sino\",noisy_data_fdg.as_array()[0,c_slice,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check we are going to apply the backward/adjoint operation to do a simply image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reconstruction\n",
    "u_simple = acq_model.backward(noisy_data_fdg)/anorm\n",
    "v_simple = acq_model.backward(noisy_data_amyl)/anorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "plot_2d_image([1,2,1], numpy.abs(u_simple.as_array())[0, :, :], '$u_{simple}$')\n",
    "plot_2d_image([1,2,2], numpy.abs(v_simple.as_array())[0, :, :], '$v_{simple}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images look quite poor compared to the ground truth input images, because they are reconstructed by a simple back-projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C) Set up the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used mainly __SIRF__ functionality, now we are going to use __CIL__ in order to set up the reconstruction problem and then solve it. In order to be able to reconstruct both $u$ and $v$ at the same time, we will make use of `BlockDataContainer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we define the `SmoothJointTV` class. Our plan is to use a modified Gradient descent (`GD`) algorithm to solve the above problems. This implements the `__call__` method required to monitor the objective value and the `gradient` method that evaluates the gradient of `JTV`.\n",
    "\n",
    "For the two subproblems, the first variations with respect to $u$ and $v$ variables are:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\biggl(1-\\frac{g_{1}}{A_{1}u+\\eta}\\biggl) - \\alpha_{1} \\mathrm{div}\\bigg( \\frac{\\nabla u}{|\\nabla(u, v)|_{2,\\eta,\\lambda}}\\bigg)\\\\\n",
    "& \\biggl(1-\\frac{g_{2}}{A_{2}v+\\eta}\\biggl) - \\alpha_{2} \\mathrm{div}\\bigg( \\frac{\\nabla v}{|\\nabla(u, v)|_{2,\\eta,1-\\lambda}}\\bigg)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $$|\\nabla(u, v)|_{2,\\eta,\\lambda} = \\sqrt{ \\lambda|\\nabla u|^{2} + (1-\\lambda)|\\nabla v|^{2} + \\eta^{2}}.$$\n",
    "\n",
    "and $ \\eta $ means we can avoid dividing by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothJointTV(Function):\n",
    "              \n",
    "    def __init__(self, eta, axis, lambda_par, domain_geometry):\n",
    "                \n",
    "        r'''\n",
    "        :param eta: smoothing parameter making SmoothJointTV differentiable \n",
    "        '''\n",
    "\n",
    "        super(SmoothJointTV, self).__init__(L=8)\n",
    "        \n",
    "        # smoothing parameter\n",
    "        self.eta = eta   \n",
    "        \n",
    "        # GradientOperator\n",
    "        self.grad = GradientOperator(domain_geometry)\n",
    "                \n",
    "        # Which variable to differentiate\n",
    "        self.axis = axis\n",
    "        \n",
    "        if self.eta<=0:\n",
    "            raise ValueError('Need positive value for eta')\n",
    "            \n",
    "        self.lambda_par=lambda_par    \n",
    "                                    \n",
    "                            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        r\"\"\" x is BlockDataContainer that contains (u,v). Actually x is a BlockDataContainer that contains 2 BDC.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, BlockDataContainer):\n",
    "            raise ValueError('__call__ expected BlockDataContainer, got {}'.format(type(x))) \n",
    "\n",
    "        tmp = (self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.eta**2).sqrt().sum()\n",
    "\n",
    "        return tmp    \n",
    "                        \n",
    "             \n",
    "    def gradient(self, x, out=None):\n",
    "        \n",
    "        denom = (self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.eta**2).sqrt()         \n",
    "        \n",
    "        if self.axis==0:            \n",
    "            num = self.lambda_par*self.grad.direct(x[0])                        \n",
    "        else:            \n",
    "            num = (1-self.lambda_par)*self.grad.direct(x[1])            \n",
    "\n",
    "        if out is None:    \n",
    "            tmp = self.grad.range.allocate()\n",
    "            tmp[self.axis].fill(self.grad.adjoint(num.divide(denom)))\n",
    "            return tmp\n",
    "        else:                                \n",
    "            self.grad.adjoint(num.divide(denom), out=out[self.axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now slightly alter the gradient descent `update` method in order to enforce positivity. This update projects our image onto the set of positive numbers and modifies our algorithm slightly to something known as the Gradient Projection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateGD(self):\n",
    "        '''Single iteration of Gradient Descent'''\n",
    "        \n",
    "        self.objective_function.gradient(self.x, out=self.x_update)\n",
    "        self.x_update.multiply(self.step_size, out = self.x_update)\n",
    "        self.x.subtract(self.x_update, out = self.x)\n",
    "\n",
    "        # remove any negative values\n",
    "        self.x.add(self.x.abs(), out = self.x) ## new line\n",
    "        self.x.divide(2, out = self.x)            ## new line\n",
    "    \n",
    "GD.update = updateGD    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put everything together and define our two objective functions which solve the two subproblems which we defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha1 = 0.5\n",
    "alpha2 = 0.5\n",
    "lambda_par = 0.5\n",
    "eta = 1e-5\n",
    "eta_sino = templ_sino.get_uniform_copy(eta) # uniform sino for KL diveregence smoothing\n",
    "\n",
    "# BlockGeometry for the two modalities\n",
    "bg = BlockGeometry(u_simple, v_simple)\n",
    "\n",
    "# Projection map to image u or v\n",
    "L1 = ProjectionMap(bg, index=0)\n",
    "L2 = ProjectionMap(bg, index=1)\n",
    "\n",
    "# Fidelity terms based on the acqusition data\n",
    "f1 = KullbackLeibler(b=noisy_data_fdg, eta=eta_sino)\n",
    "f2 = KullbackLeibler(b=noisy_data_amyl, eta=eta_sino)\n",
    "\n",
    "# JTV for each of the subproblems\n",
    "JTV1 = alpha1*SmoothJointTV(eta=eta, axis=0, lambda_par = lambda_par , domain_geometry = uMap)\n",
    "JTV2 = alpha2*SmoothJointTV(eta=eta, axis=1, lambda_par = 1-lambda_par, domain_geometry = uMap)\n",
    "\n",
    "# Compose the two objective functions\n",
    "op1 = CompositionOperator(acq_model, L1)\n",
    "op2 = CompositionOperator(acq_model, L2)\n",
    "\n",
    "objective1 = OperatorCompositionFunction(f1 , op1) + JTV1\n",
    "objective2 = OperatorCompositionFunction(f2 , op2) + JTV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (D) Solve the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with a cylindrical uniform image\n",
    "init = make_cylindrical_FOV(fdg.get_uniform_copy(1))\n",
    "x0 = BlockDataContainer(init,init)\n",
    "\n",
    "step_size = 0.25\n",
    "\n",
    "# Here we add the functionality for a preconditioned gradient step to improve the convergence rate\n",
    "sens_im_arr = acq_model.backward(templ_sino.get_uniform_copy(1)).as_array()\n",
    "sens_im_arr[sens_im_arr == 0] = 1\n",
    "sens_im = fdg.clone().fill(sens_im_arr)\n",
    "\n",
    "precond_im = fdg.get_uniform_copy(1)/sens_im\n",
    "\n",
    "outeriter = 4\n",
    "inneriter = 4\n",
    "\n",
    "# We are also going to log the value of the objective functions\n",
    "obj1_val_it = []\n",
    "obj2_val_it = []\n",
    "# and the images\n",
    "images = []\n",
    "\n",
    "for i in range(outeriter):\n",
    "    \n",
    "    precond = (x0.copy()/sens_im).abs()\n",
    "\n",
    "    step = precond * step_size\n",
    "    images.append(x0.copy())\n",
    "\n",
    "    gd1 = GD(x0, objective1, step_size=step, \\\n",
    "          max_iteration = inneriter, update_objective_interval = 1)\n",
    "    gd1.run(verbose=1)\n",
    "    \n",
    "    # We skip the first one because it gets repeated\n",
    "    obj1_val_it.extend(gd1.objective[1:])\n",
    "    \n",
    "    # Here we are going to do a little \"trick\" in order to better see, when each subproblem is optimised, we\n",
    "    # are going to append NaNs to the objective function which is currently not optimised. The NaNs will not\n",
    "    # show up in the final plot and hence we can nicely see each subproblem.\n",
    "    obj2_val_it.extend(numpy.ones_like(gd1.objective[1:])*numpy.nan)\n",
    "    \n",
    "    gd2 = GD(gd1.solution, objective2, step_size=step, \\\n",
    "          max_iteration = inneriter, update_objective_interval = 1)\n",
    "    gd2.run(verbose=1)\n",
    "    \n",
    "    obj2_val_it.extend(gd2.objective[1:])\n",
    "    obj1_val_it.extend(numpy.ones_like(gd2.objective[1:])*numpy.nan)\n",
    "    \n",
    "    x0 = gd2.solution.clone()\n",
    "    \n",
    "    print('* * * * * * Outer Iteration ', i+1, ' * * * * * *\\n')\n",
    "images.append(x0.copy())\n",
    "print('* * * * * * Reconstruction Complete * * * * * *\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can look at the images $u_{jtv}$ and $v_{jtv}$ and compare them to the simple reconstruction $u_{simple}$ and $v_{simple}$ and the original ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_jtv = numpy.squeeze(numpy.abs(x0[0].as_array()))\n",
    "v_jtv = numpy.squeeze(numpy.abs(x0[1].as_array()))\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([2,3,1], numpy.squeeze(numpy.abs(u_simple.as_array())), '$u_{simple}$')\n",
    "plot_2d_image([2,3,2], u_jtv, '$u_{JTV}$')\n",
    "plot_2d_image([2,3,3], numpy.squeeze(numpy.abs(fdg_arr)), '$u_{gt}$') \n",
    "\n",
    "plot_2d_image([2,3,4], numpy.squeeze(numpy.abs(v_simple.as_array())), '$v_{simple}$')\n",
    "plot_2d_image([2,3,5], v_jtv, '$v_{JTV}$')\n",
    "plot_2d_image([2,3,6], numpy.squeeze(numpy.abs(amyl_arr)), '$v_{gt}$') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(obj1_val_it, 'o-', label='subproblem 1')\n",
    "plt.plot(obj2_val_it, '+-', label='subproblem 2')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Value of objective function')\n",
    "plt.title('Objective functions')\n",
    "plt.legend()\n",
    "\n",
    "# Logarithmic y-axis\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at how the images changed throughout the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = 'iframe_connected'\n",
    "plt.ioff()\n",
    "# choose which subproblem to display\n",
    "subprob = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "arrays = []\n",
    "ims = []\n",
    "for i, image in enumerate(images):\n",
    "    array = numpy.squeeze(image[subprob].as_array())\n",
    "    arrays.append(array)\n",
    "    im = ax.imshow(array, animated = True, vmin=0, vmax=fdg_arr.max())\n",
    "    im.set_clim(0,array.max())\n",
    "    title = plt.text(0.5,1.01,str(i), ha=\"center\",va=\"bottom\",color=numpy.random.rand(3),\n",
    "                     transform=ax.transAxes, fontsize=\"large\")\n",
    "    if i == 0:\n",
    "        ax.imshow(array)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    ims.append([im,title,])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval = 750, blit = True, repeat_delay = 1000)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% delete temporary files\n",
    "wdpath = os.getcwd()\n",
    "for filename in glob.glob(os.path.join(wdpath, \"tmp*\")):\n",
    "    os.remove(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a good demonstration for a synergistic image reconstruction of two different images. The following gives a few suggestions of what to do next and also how to extend this notebook to other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of iterations\n",
    "In our problem we have several regularisation parameters such as $\\alpha_{1}$, $\\alpha_{2}$ and $\\lambda$. In addition, the number of inner iterations for each subproblem (currently set to 3) and the number of outer iterations (currently set to 10) also determine the final solution. Of course, for infinite number of total iterations it shouldn't matter but usually we don't have that much time.\n",
    "\n",
    "__TODO__: Change the number of iterations and see what happens to the objective functions. For a given number of total iterations, do you think it is better to have a high number of inner or high number of outer iterations? Why? Does this also depend on the undersampling factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial misalignment\n",
    "In the above example we simulated our data such that there is a perfect spatial match between $u$ and $v$. For real world applications this usually cannot be assumed. \n",
    "\n",
    "__TODO__: Add spatial misalignment between $u$ and $v$. This can be achieved e.g. by calling `numpy.roll` on `amyl_arr` before calling `v_gt = crop_and_fill(im_mr, amyl_arr)`. What is the effect on the reconstructed images? For a more \"advanced\" misalignment, have a look at notebook `BrainWeb`.\n",
    "\n",
    "__TODO__: One way to minimize spatial misalignment is to use image registration to ensure both $u$ and $v$ are well aligned. In the notebook `sirf_registration` you find information about how to register two images and also how to resample one image based on the spatial transformation estimated from the registration. Try to use this to correct for the misalignment you introduced above. For a real world example, at which point in the code would you have to carry out the registration+resampling? (some more information can also be found at the end of [MAPEM_Bowsher notebook](MAPEM_Bowsher.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathologies\n",
    "The images $u$ and $v$ show the same anatomy, just with a different distributions. Clinically more useful are of course images which show complementary image information.\n",
    "\n",
    "__TODO__: Add a pathology to either $u$ and $v$ and see how this effects the reconstruction. For something more advanced, have a look at the notebook `BrainWeb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single anatomical prior\n",
    "So far we have alternated between two reconstruction problems. Another option is to do a single regularised reconstruction and simply use a previously reconstructed image for regularisation.\n",
    "\n",
    "__TODO__: Adapt the above code such that $u$ is reconstructed first without regularisation and is then used for a regularised reconstruction of $v$ without any further updates of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other regularisation options\n",
    "In this example we used a TV-based regularisation, but of course other regularisers could also be used, such as directional TV.\n",
    "\n",
    "__TODO__: Have a look at the __CIL__ notebook `02_Dynamic_CT` and adapt the `SmoothJointTV` class above to use directional TV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other optimisation algorithms\n",
    "In this example we used a Gradient Descent for ease of understanding, but other optimisation algorithms will be more appropriate and more efficient.\n",
    "\n",
    "__TODO__: Have a look at the __CIL__ Documentaion and see if you can implement other algorithms such as PDHG or ADMM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution Modelling\n",
    "In this example we do not use any resolution modelling for PET\n",
    "\n",
    "__TODO__: Implement resolution modelling and see what difference it makes to the reconstructed images"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
