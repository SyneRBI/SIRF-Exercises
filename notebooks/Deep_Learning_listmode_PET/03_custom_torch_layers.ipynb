{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863d394a",
   "metadata": {},
   "source": [
    "Creating custom layers in pytorch\n",
    "=================================\n",
    "\n",
    "In this notebook, we will learn how to create custom layers in pytorch that use functions outside the pytorch framework.\n",
    "We will create a custom layer that multiplies the input tensor with a square matrix.\n",
    "For demonostration purposes, we will create a simple layer that multiplies a 1D torch input vector with a square matrix,\n",
    "where the matrix multiplication is done using numpy functions.\n",
    "\n",
    "Learning objectives of this notebook\n",
    "------------------------------------\n",
    "\n",
    "1. Learn how to create custom layers in pytorch that are compatible with the autograd framework.\n",
    "2. Understand the importance of implementing the backward pass of the custom layer correctly.\n",
    "3. Learn how to test the gradient backpropagation through the custom layer using the `torch.autograd.gradcheck` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6bbea",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# seed all torch random generators\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# choose the torch device\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "# length of the input vector\n",
    "n = 7\n",
    "\n",
    "# define our square matrix\n",
    "A: np.ndarray = np.arange(n ** 2).reshape(n, n).astype(np.float64) / (n ** 2)\n",
    "# define the 1D pytorch tensor: not that the shape is (1,1,n) including the batch and channel dimensions\n",
    "x_t = torch.tensor(np.arange(n).reshape(1, 1, n).astype(np.float64), device=dev) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9094b8",
   "metadata": {},
   "source": [
    "Approach 1: The naive approach\n",
    "------------------------------\n",
    "\n",
    "We will first try a naive approach where we create a custom layer by subclassing torch.nn.Module\n",
    "and implementing the forward pass by conversion between numpy and torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareMatrixMultiplicationLayer(torch.nn.Module):\n",
    "    def __init__(self, mat: np.ndarray) -> None:\n",
    "        super().__init__()\n",
    "        self._mat: np.ndarray = mat\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # convert the input tensor to numpy\n",
    "        x_np = x.detach().cpu().numpy()\n",
    "        # nympy matrix multiplication\n",
    "        y_np = self._mat @ x_np[0, 0, ...]\n",
    "        # convert back to torch tensor\n",
    "        y = torch.tensor(y_np, device=x.device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb070a2",
   "metadata": {},
   "source": [
    "We setup a simple feedforward network interlacing the 3 minimals convolutional layers and 3 square matrix multiplication layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803723d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self, mat, cnn) -> None:\n",
    "        super().__init__()\n",
    "        self._matrix_layer = SquareMatrixMultiplicationLayer(mat)\n",
    "        self._cnn = cnn\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self._cnn(x)\n",
    "        x2 = self._matrix_layer(x1)\n",
    "        x3 = self._cnn(x2)\n",
    "        x4 = self._matrix_layer(x3)\n",
    "        x5 = self._cnn(x4)\n",
    "        x6 = self._matrix_layer(x5)\n",
    "\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bf860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a simple CNN consisting of 2 convolutional layers and 1 ReLU activation\n",
    "cnn1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(1, 3, (3,), padding=\"same\", bias=False, dtype=torch.float64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(3, 1, (3,), padding=\"same\", bias=False, dtype=torch.float64),\n",
    ").to(dev)\n",
    "\n",
    "# setup the network\n",
    "net1 = Net1(A, cnn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473fe3c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# forward pass of our input vector through the network\n",
    "pred1 = net1(x_t)\n",
    "print(f\"pred1: {pred1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d51ca",
   "metadata": {},
   "source": [
    "We see that the forward pass works as expected. Now we will setup a dummy loss and try backpropagate the gradients\n",
    "using the naive approach for our custom matrix multiplication layer.\n",
    "Baclpropagation of the gradients is the central step in training neural networks. It involves calculating the gradients of\n",
    "the loss function with respect to the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a dummy target (label / high quality reference image) tensor\n",
    "target = 2 * x_t\n",
    "# define an MSE loss\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "# calculate the loss between the prediction and the target\n",
    "loss1 = loss_fct(pred1, target)\n",
    "print(f\"loss1: {loss1.item()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6458bf9",
   "metadata": {},
   "source": [
    "Calculation of the loss still runs fine. Now let's try to backpropagate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c51d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    loss1.backward()\n",
    "except RuntimeError:\n",
    "    print(\"Error in gradient backpropagation using naive approach\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d290dd2",
   "metadata": {},
   "source": [
    "Exercise 3.1\n",
    "------------\n",
    "We see that the backpropagation of the gradients fails with the naive approach.\n",
    "Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8810a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Approach 2: Subclassing torch.autograd.Function\n",
    "-----------------------------------------------\n",
    "\n",
    "The correct way to create custom layers in pytorch is to subclass torch.autograd.Function\n",
    "which involves implementing the forward and backward pass of the layer.\n",
    "In the backward pass we have to implement the Jacobian transpose vector product of the layer.\n",
    "For details, see [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#optional-reading-vector-calculus-using-autograd)\n",
    "and [here](https://pytorch.org/docs/stable/notes/extending.func.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3cd2d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# define the custom layer by subclassing torch.autograd.Function and implementing the forward and backward pass\n",
    "\n",
    "\n",
    "class NPSquareMatrixMultiplicationLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, mat: np.ndarray) -> torch.Tensor:\n",
    "\n",
    "        # we use the context object ctx to store the matrix and other variables that we need in the backward pass\n",
    "        ctx.mat = mat\n",
    "        ctx.device = x.device\n",
    "        ctx.shape = x.shape\n",
    "        ctx.dtype = x.dtype\n",
    "\n",
    "        # convert to numpy\n",
    "        x_np = x.cpu().numpy()\n",
    "        # numpy matrix multiplication\n",
    "        y_np = mat @ x_np[0, 0, ...]\n",
    "        # convert back to torch tensor\n",
    "        y = torch.tensor(y_np, device=ctx.device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> tuple[torch.Tensor | None, None]:\n",
    "        if grad_output is None:\n",
    "            return None, None\n",
    "        else:\n",
    "            # convert to numpy\n",
    "            grad_output_np = grad_output.cpu().numpy()\n",
    "            # calculate the Jacobian transpose vector product in numpy and convert back to torch tensor\n",
    "            back = (\n",
    "                torch.tensor(\n",
    "                    ctx.mat.T @ grad_output_np[0, 0, ...],\n",
    "                    device=ctx.device,\n",
    "                    dtype=ctx.dtype,\n",
    "                )\n",
    "                .unsqueeze(0)\n",
    "                .unsqueeze(0)\n",
    "            )\n",
    "\n",
    "            return back, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9cdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new network incl. the custom matrix multiplication layer using the \"correct\" approach\n",
    "# To use our custom layer in the network, we have to use the apply method of the custom layer class.\n",
    "\n",
    "\n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self, mat, cnn) -> None:\n",
    "        super().__init__()\n",
    "        self._matrix_layer = NPSquareMatrixMultiplicationLayer.apply\n",
    "        self._mat = mat\n",
    "        self._cnn = cnn\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self._cnn(x)\n",
    "        x2 = self._matrix_layer(x1, self._mat)\n",
    "        x3 = self._cnn(x2)\n",
    "        x4 = self._matrix_layer(x3, self._mat)\n",
    "        x5 = self._cnn(x4)\n",
    "        x6 = self._matrix_layer(x5, self._mat)\n",
    "\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef93f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the same CNN as above\n",
    "cnn2 = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(1, 3, (3,), padding=\"same\", bias=False, dtype=torch.float64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(3, 1, (3,), padding=\"same\", bias=False, dtype=torch.float64),\n",
    ").to(dev)\n",
    "cnn2.load_state_dict(cnn1.state_dict())\n",
    "\n",
    "# setup the network - only difference is the custom layer\n",
    "net2 = Net2(A, cnn2)\n",
    "\n",
    "# predict again\n",
    "pred2 = net2(x_t)\n",
    "print(f\"pred2: {pred2}\\n\")\n",
    "\n",
    "loss2 = loss_fct(pred2, target)\n",
    "print(f\"loss2: {loss2.item()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1c4d5",
   "metadata": {},
   "source": [
    "Note that the prediction still works and gives the same result as before. Also the loss calculation yield the same results as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()\n",
    "\n",
    "# print backpropagated gradients that of all parameters of CNN layers of our network\n",
    "print(\"backpropagated gradients using correct approach\")\n",
    "print([p.grad for p in net2._cnn.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ff1c4",
   "metadata": {},
   "source": [
    "In contrast to the naive approach, the backpropagation of the gradients works fine now, meaning that this network is ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b383db",
   "metadata": {},
   "source": [
    "Testing gradient backpropagation through the layer\n",
    "--------------------------------------------------\n",
    "\n",
    "When defining new custom layers, it is crucial to test whether the backward pass is implemented correctly.\n",
    "Otherwise the gradient backpropagation though the layer will be incorrect, and optimizing the model parameters will not work.\n",
    "To test the gradient backpropagation, we can use the `torch.autograd.gradcheck` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3def037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a test input tensor - requires grad must be True!\n",
    "t_t = torch.rand(x_t.shape, device=dev, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# test the gradient backpropagation through the custom numpy matrix multiplication layer\n",
    "matrix_layer = NPSquareMatrixMultiplicationLayer.apply\n",
    "gradcheck = torch.autograd.gradcheck(matrix_layer, (t_t, A), fast_mode=True)\n",
    "\n",
    "print(f\"gradient check of NPSquareMatrixMultiplicationLayer: {gradcheck}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cb3ba",
   "metadata": {},
   "source": [
    "Exercise 3.2\n",
    "------------\n",
    "Temporarily change the backward pass of the custom layer such that is is not correct anymore\n",
    "(e.g. by multiplying the output with 0.95) and rerun the gradient check. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
