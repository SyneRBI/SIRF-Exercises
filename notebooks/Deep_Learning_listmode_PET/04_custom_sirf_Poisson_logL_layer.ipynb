{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e927b2e5",
   "metadata": {},
   "source": [
    "Creating custom Poisson log likelihood gradient step and OSEM update layers\n",
    "===========================================================================\n",
    "\n",
    "Learning objectives\n",
    "-------------------\n",
    "\n",
    "1. Implement the forward and backward pass of a custom (pytorch autograd compatible) layer that\n",
    "   calculates the gradient Poisson log-likelihood.\n",
    "2. Understand how to test whether the (backward pass) of the custom layer is implemented correctly,\n",
    "   such that gradient backpropagation works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sirf.STIR\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sirf.Utilities import examples_data_path\n",
    "\n",
    "# acq_time must be 1min\n",
    "acq_time: str = \"1min\"\n",
    "\n",
    "data_path: Path = Path(examples_data_path(\"PET\")) / \"mMR\"\n",
    "list_file: str = str(data_path / \"list.l.hdr\")\n",
    "norm_file: str = str(data_path / \"norm.n.hdr\")\n",
    "attn_file: str = str(data_path / \"mu_map.hv\")\n",
    "\n",
    "output_path: Path = Path(f\"recons_{acq_time}\")\n",
    "emission_sinogram_output_prefix: str = str(output_path / \"emission_sinogram\")\n",
    "scatter_sinogram_output_prefix: str = str(output_path / \"scatter_sinogram\")\n",
    "randoms_sinogram_output_prefix: str = str(output_path / \"randoms_sinogram\")\n",
    "attenuation_sinogram_output_prefix: str = str(output_path / \"acf_sinogram\")\n",
    "num_scatter_iter: int = 3\n",
    "\n",
    "lm_recon_output_file: str = str(output_path / \"lm_recon\")\n",
    "nxny: tuple[int, int] = (127, 127)\n",
    "num_subsets: int = 21\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "# engine's messages go to files, except error messages, which go to stdout\n",
    "_ = sirf.STIR.MessageRedirector(\"info.txt\", \"warn.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f071a",
   "metadata": {},
   "source": [
    "Load listmode data and create the acquisition model\n",
    "---------------------------------------------------\n",
    "\n",
    "In this demo example, we use a simplified acquisition model that only implements the geometric forward projection.\n",
    "The effects of normalization, attenuation, scatter, randoms, are ignored but can be added as shown in the last\n",
    "example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad999928",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirf.STIR.AcquisitionData.set_storage_scheme(\"memory\")\n",
    "listmode_data = sirf.STIR.ListmodeData(list_file)\n",
    "acq_data_template = listmode_data.acquisition_data_template()\n",
    "\n",
    "acq_data = sirf.STIR.AcquisitionData(\n",
    "    str(Path(f\"{emission_sinogram_output_prefix}_f1g1d0b0.hs\"))\n",
    ")\n",
    "\n",
    "# select acquisition model that implements the geometric\n",
    "# forward projection by a ray tracing matrix multiplication\n",
    "acq_model = sirf.STIR.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_model.set_num_tangential_LORs(1)\n",
    "\n",
    "randoms = sirf.STIR.AcquisitionData(str(Path(f\"{randoms_sinogram_output_prefix}.hs\")))\n",
    "\n",
    "ac_factors = sirf.STIR.AcquisitionData(\n",
    "    str(Path(f\"{attenuation_sinogram_output_prefix}.hs\"))\n",
    ")\n",
    "asm_attn = sirf.STIR.AcquisitionSensitivityModel(ac_factors)\n",
    "\n",
    "asm_norm = sirf.STIR.AcquisitionSensitivityModel(norm_file)\n",
    "asm = sirf.STIR.AcquisitionSensitivityModel(asm_norm, asm_attn)\n",
    "\n",
    "asm.set_up(acq_data)\n",
    "acq_model.set_acquisition_sensitivity(asm)\n",
    "\n",
    "scatter_estimate = sirf.STIR.AcquisitionData(\n",
    "    str(Path(f\"{scatter_sinogram_output_prefix}_{num_scatter_iter}.hs\"))\n",
    ")\n",
    "acq_model.set_background_term(randoms + scatter_estimate)\n",
    "\n",
    "# setup an initial (template) image based on the acquisition data template\n",
    "initial_image = acq_data_template.create_uniform_image(value=1, xy=nxny)\n",
    "\n",
    "# load the reconstructed image from notebook 01\n",
    "lm_ref_recon = sirf.STIR.ImageData(f\"{lm_recon_output_file}.hv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e4472",
   "metadata": {},
   "source": [
    "Setup of the Poisson log likelihood listmode objective function\n",
    "---------------------------------------------------------------\n",
    "\n",
    "Using the listmode data and the acquisition model, we can now setup the Poisson log likelihood objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d20277",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "lm_obj_fun = (\n",
    "    sirf.STIR.PoissonLogLikelihoodWithLinearModelForMeanAndListModeDataWithProjMatrixByBin()\n",
    ")\n",
    "lm_obj_fun.set_acquisition_model(acq_model)\n",
    "lm_obj_fun.set_acquisition_data(listmode_data)\n",
    "lm_obj_fun.set_num_subsets(num_subsets)\n",
    "lm_obj_fun.set_cache_max_size(1000000000)\n",
    "lm_obj_fun.set_cache_path(str(output_path))\n",
    "print(\"setting up listmode objective function ...\")\n",
    "lm_obj_fun.set_up(initial_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f9910",
   "metadata": {},
   "source": [
    "Setup of a pytorch layer that computes the gradient of the Poisson log likelihood objective function\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "Using our listmode objective function, we can now implement a custom pytorch layer that computes the gradient\n",
    "of the Poisson log likelihood using the `gradient()` method using a subset of the listmode data.\n",
    "\n",
    "This layer maps a torch minibatch tensor to another torch tensor of the same shape.\n",
    "The shape of the minibatch tensor is [batch_size=1, channel_size=1, spatial dimensions].\n",
    "For the implementation we subclass `torch.autograd.Function` and implement the `forward()` and\n",
    "`backward()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08c001",
   "metadata": {},
   "source": [
    "Exercise 4.1\n",
    "------------\n",
    "\n",
    "Using your knowledge of the Poisson log likelihood gradient (exercise 0.1) and the content of the notebook 03\n",
    "on custom layers, implement the forward and backward pass of a custom layer that calculates the gradient of the\n",
    "Poisson log likelihood using a SIRF objective function as shown in the figure below.\n",
    "\n",
    "# ![](figs/poisson_logL_grad_layer.drawio.svg)\n",
    "\n",
    "The next cell contains the skeleton of the custom layer. You need to fill in the missing parts in the forward and\n",
    "backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2049496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIRFPoissonlogLGradLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x: torch.Tensor,\n",
    "        objective_function,\n",
    "        sirf_template_image: sirf.STIR.ImageData,\n",
    "        subset: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"(listmode) Poisson loglikelihood gradient layer forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : context object\n",
    "            used to store objects that we need in the backward pass\n",
    "        x : torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the image\n",
    "        objective_function : sirf (listmode) objective function\n",
    "            the objective function that we use to calculate the gradient\n",
    "        sirf_template_image : sirf.STIR.ImageData\n",
    "            image template that we use to convert between torch tensors and sirf images\n",
    "        subset : int\n",
    "            subset number used for the gradient calculation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the image\n",
    "            containing the gradient of the (listmode) Poisson log likelihood at x\n",
    "        \"\"\"\n",
    "        # we use the context object ctx to store objects that we need in the backward pass\n",
    "        ctx.device = x.device\n",
    "        ctx.objective_function = objective_function\n",
    "        ctx.dtype = x.dtype\n",
    "        ctx.subset = subset\n",
    "        ctx.sirf_template_image = sirf_template_image\n",
    "\n",
    "        # ==============================================================\n",
    "        # ==============================================================\n",
    "        # YOUR CODE HERE\n",
    "        # ==============================================================\n",
    "        # ==============================================================\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx, grad_output: torch.Tensor | None\n",
    "    ) -> tuple[torch.Tensor | None, None, None, None]:\n",
    "        \"\"\"(listmode) Poisson loglikelihood gradient layer backward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : context object\n",
    "            used to store objects that we need in the backward pass\n",
    "        grad_output : torch.Tensor | None\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the gradient (called v in the autograd tutorial)\n",
    "            https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#optional-reading-vector-calculus-using-autograd\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[torch.Tensor | None, None, None, None]\n",
    "            the Jacobian-vector product of the Poisson log likelihood gradient layer\n",
    "        \"\"\"\n",
    "\n",
    "        if grad_output is None:\n",
    "            return None, None, None, None\n",
    "        else:\n",
    "            ctx.sirf_template_image.fill(grad_output.cpu().numpy()[0, 0, ...])\n",
    "\n",
    "            # ==============================================================\n",
    "            # ==============================================================\n",
    "            # YOUR CODE HERE\n",
    "            # --------------\n",
    "            #\n",
    "            # calculate the Jacobian-vector product of the Poisson log likelihood gradient layer\n",
    "            # Hints: (1) try to derive the Jacobian of the gradient of the Poisson log likelihood gradient first\n",
    "            #        (2) the sirf.STIR objective function has a method called `multiply_with_Hessian`\n",
    "            #\n",
    "            # ==============================================================\n",
    "            # =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9dae6",
   "metadata": {},
   "source": [
    "To view the solution to the exercise, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load snippets/solution_4_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensor and add the minibatch and channel dimensions\n",
    "x_t = (\n",
    "    torch.tensor(\n",
    "        lm_ref_recon.as_array(), device=dev, dtype=torch.float32, requires_grad=False\n",
    "    )\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(0)\n",
    ")\n",
    "\n",
    "# setup our custom Poisson log likelihood gradient layer\n",
    "poisson_logL_grad_layer = SIRFPoissonlogLGradLayer.apply\n",
    "# perform the forward pass (calcuate the gradient of the Poisson log likelihood at x_t)\n",
    "grad_x = poisson_logL_grad_layer(x_t, lm_obj_fun, initial_image, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71a89e",
   "metadata": {},
   "source": [
    "Implementing a OSEM update layer using our custom Poisson log likelihood gradient layer\n",
    "=======================================================================================\n",
    "\n",
    "Using our custom Poisson log likelihood gradient layer, we can now implement a custom OSEM update layer.\n",
    "Note that the OSEM update can be decomposed into a simple feedforward network consisting of basic arithmetic\n",
    "operations that are implemented in pytorch (pointwise multiplication and addition) as shown in the figure below.\n",
    "\n",
    "# ![](figs/osem_layer.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8eb52",
   "metadata": {},
   "source": [
    "Exercise 4.2\n",
    "------------\n",
    "Implement the forward pass of a OSEM update layer using the Poisson log likelihood gradient layer that we implemented\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4db256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSEMUpdateLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        objective_function,\n",
    "        sirf_template_image: sirf.STIR.ImageData,\n",
    "        subset: int,\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        \"\"\"OSEM update layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        objective_function : sirf (listmode) objective function\n",
    "            the objective function that we use to calculate the gradient\n",
    "        sirf_template_image : sirf.STIR.ImageData\n",
    "            image template that we use to convert between torch tensors and sirf images\n",
    "        subset : int\n",
    "            subset number used for the gradient calculation\n",
    "        device : str\n",
    "            device used for the calculations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the OSEM\n",
    "            update of the input image using the Poisson log likelihood objective function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._objective_function = objective_function\n",
    "        self._sirf_template_image: sirf.STIR.ImageData = sirf_template_image\n",
    "        self._subset: int = subset\n",
    "\n",
    "        self._poisson_logL_grad_layer = SIRFPoissonlogLGradLayer.apply\n",
    "\n",
    "        # setup a tensor containng the inverse of the subset sensitivity image adding the minibatch and channel dimensions\n",
    "        self._inv_sens_image: torch.Tensor = 1.0 / torch.tensor(\n",
    "            objective_function.get_subset_sensitivity(subset).as_array(),\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        # replace positive infinity values with 0 (voxels with 0 sensitivity)\n",
    "        torch.nan_to_num(self._inv_sens_image, posinf=0, out=self._inv_sens_image)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward pass of the OSEM update layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            OSEM update image\n",
    "        \"\"\"\n",
    "\n",
    "        # =======================================================================\n",
    "        # =======================================================================\n",
    "        # YOUR CODE HERE\n",
    "        # USE ONLY BASIC ARITHMETIC OPERATIONS BETWEEN TORCH TENSORS!\n",
    "        # =======================================================================\n",
    "        # ======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe0398",
   "metadata": {},
   "source": [
    "To view the solution to the exercise, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load snippets/solution_4_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98031064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the OSEM update layer for subset 0\n",
    "osem_layer0 = OSEMUpdateLayer(lm_obj_fun, initial_image, 0, dev)\n",
    "# perform the forward pass\n",
    "osem_updated_x_t = osem_layer0(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8639e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show the input and output of the OSEM update layer\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4), tight_layout=True)\n",
    "ax[0].imshow(x_t.cpu().numpy()[0, 0, 71, ...], cmap=\"Greys\")\n",
    "ax[1].imshow(osem_updated_x_t.cpu().numpy()[0, 0, 71, ...], cmap=\"Greys\")\n",
    "ax[2].imshow(\n",
    "    osem_updated_x_t.cpu().numpy()[0, 0, 71, ...] - x_t.cpu().numpy()[0, 0, 71, ...],\n",
    "    cmap=\"seismic\",\n",
    "    vmin=-0.01,\n",
    "    vmax=0.01,\n",
    ")\n",
    "ax[0].set_title(\"input image\")\n",
    "ax[1].set_title(\"OSEM updated image\")\n",
    "ax[2].set_title(\"diffence image\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bd485",
   "metadata": {},
   "source": [
    "Testing the backward pass of the custom layers\n",
    "----------------------------------------------\n",
    "\n",
    "As mentioned in the previous notebook, it is important to test whether the backward pass\n",
    "of the custom layer is implemented correctly using the `torch.autograd.gradcheck` function.\n",
    "**However, we won't do this here** - but rather disuss the implementation - because:\n",
    "- it can take long time\n",
    "- because we are using float32, we have to adapt the tolerances\n",
    "- the sirf.STIR gradient calculation is not exactly deterministic, due to parallelization and numerical precision\n",
    "  which also requires to adapt the tolerances for non-deterministic functions\n",
    "\n",
    "**If you implement a new layer, and you are not 100% sure that the backward pass is correct, you should always test it!**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
