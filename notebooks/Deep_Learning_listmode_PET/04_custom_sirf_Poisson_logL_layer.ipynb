{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be16b0bf",
   "metadata": {},
   "source": [
    "Creating custom Poisson log likelihood gradient step and OSEM update layers\n",
    "===========================================================================\n",
    "\n",
    "Now that we know how to perform an listmode OSEM update in sirf.STIR and we \n",
    "know how to implement custom layers in pytorch, we can combine both to create\n",
    "a custom Poisson log likelihood gradient step and OSEM update layer.\n",
    "\n",
    "Learning objectives\n",
    "-------------------\n",
    "\n",
    "1. Implement the forward and backward pass of a custom (pytorch autograd compatible) layer that\n",
    "   calculates the gradient Poisson log-likelihood.\n",
    "2. Understand how to test whether the (backward pass) of the custom layer is implemented correctly,\n",
    "   such that gradient backpropagation works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f819228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sirf.STIR\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sirf.Utilities import examples_data_path\n",
    "\n",
    "# acq_time must be 1min\n",
    "acq_time: str = \"1min\"\n",
    "\n",
    "data_path: Path = Path(examples_data_path(\"PET\")) / \"mMR\"\n",
    "list_file: str = str(data_path / \"list.l.hdr\")\n",
    "norm_file: str = str(data_path / \"norm.n.hdr\")\n",
    "attn_file: str = str(data_path / \"mu_map.hv\")\n",
    "\n",
    "output_path: Path = data_path / f\"lm_recons_{acq_time}\"\n",
    "emission_sinogram_output_prefix: str = str(output_path / \"emission_sinogram\")\n",
    "scatter_sinogram_output_prefix: str = str(output_path / \"scatter_sinogram\")\n",
    "randoms_sinogram_output_prefix: str = str(output_path / \"randoms_sinogram\")\n",
    "attenuation_sinogram_output_prefix: str = str(output_path / \"acf_sinogram\")\n",
    "num_scatter_iter: int = 3\n",
    "\n",
    "lm_recon_output_file: str = str(output_path / \"lm_recon\")\n",
    "nxny: tuple[int, int] = (127, 127)\n",
    "num_subsets: int = 21\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "# engine's messages go to files, except error messages, which go to stdout\n",
    "_ = sirf.STIR.MessageRedirector(\"info.txt\", \"warn.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4cf14",
   "metadata": {},
   "source": [
    "Load listmode data and create the acquisition model\n",
    "---------------------------------------------------\n",
    "\n",
    "In this demo example, we use a simplified acquisition model that only implements the geometric forward projection.\n",
    "The effects of normalization, attenuation, scatter, randoms, are ignored but can be added as shown in the last\n",
    "example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirf.STIR.AcquisitionData.set_storage_scheme(\"memory\")\n",
    "listmode_data = sirf.STIR.ListmodeData(list_file)\n",
    "acq_data_template = listmode_data.acquisition_data_template()\n",
    "\n",
    "acq_data = sirf.STIR.AcquisitionData(\n",
    "    str(Path(f\"{emission_sinogram_output_prefix}_f1g1d0b0.hs\"))\n",
    ")\n",
    "\n",
    "# select acquisition model that implements the geometric\n",
    "# forward projection by a ray tracing matrix multiplication\n",
    "acq_model = sirf.STIR.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_model.set_num_tangential_LORs(1)\n",
    "\n",
    "randoms = sirf.STIR.AcquisitionData(str(Path(f\"{randoms_sinogram_output_prefix}.hs\")))\n",
    "\n",
    "ac_factors = sirf.STIR.AcquisitionData(\n",
    "    str(Path(f\"{attenuation_sinogram_output_prefix}.hs\"))\n",
    ")\n",
    "asm_attn = sirf.STIR.AcquisitionSensitivityModel(ac_factors)\n",
    "\n",
    "asm_norm = sirf.STIR.AcquisitionSensitivityModel(norm_file)\n",
    "asm = sirf.STIR.AcquisitionSensitivityModel(asm_norm, asm_attn)\n",
    "\n",
    "asm.set_up(acq_data)\n",
    "acq_model.set_acquisition_sensitivity(asm)\n",
    "\n",
    "scatter_estimate = sirf.STIR.AcquisitionData(\n",
    "    str(Path(f\"{scatter_sinogram_output_prefix}_{num_scatter_iter}.hs\"))\n",
    ")\n",
    "acq_model.set_background_term(randoms + scatter_estimate)\n",
    "\n",
    "# setup an initial (template) image based on the acquisition data template\n",
    "initial_image = acq_data_template.create_uniform_image(value=1, xy=nxny)\n",
    "\n",
    "# load the reconstructed image from notebook 01\n",
    "lm_ref_recon = sirf.STIR.ImageData(f\"{lm_recon_output_file}.hv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a3d7d0",
   "metadata": {},
   "source": [
    "Setup of the Poisson log likelihood listmode objective function\n",
    "---------------------------------------------------------------\n",
    "\n",
    "Using the listmode data and the acquisition model, we can now setup the Poisson log likelihood objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29412f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "lm_obj_fun = (\n",
    "    sirf.STIR.PoissonLogLikelihoodWithLinearModelForMeanAndListModeDataWithProjMatrixByBin()\n",
    ")\n",
    "lm_obj_fun.set_acquisition_model(acq_model)\n",
    "lm_obj_fun.set_acquisition_data(listmode_data)\n",
    "lm_obj_fun.set_num_subsets(num_subsets)\n",
    "lm_obj_fun.set_cache_max_size(1000000000)\n",
    "lm_obj_fun.set_cache_path(str(output_path))\n",
    "print(\"setting up listmode objective function ...\")\n",
    "lm_obj_fun.set_up(initial_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56994e9",
   "metadata": {},
   "source": [
    "Setup of a pytorch layer that computes the gradient of the Poisson log likelihood objective function\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "Using our listmode objective function, we can now implement a custom pytorch layer that computes the gradient\n",
    "of the Poisson log likelihood using the `gradient()` method using a subset of the listmode data.\n",
    "\n",
    "This layer maps a torch minibatch tensor to another torch tensor of the same shape.\n",
    "The shape of the minibatch tensor is [batch_size=1, channel_size=1, spatial dimensions].\n",
    "For the implementation we subclass `torch.autograd.Function` and implement the `forward()` and\n",
    "`backward()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a4caf",
   "metadata": {},
   "source": [
    "Exercise 4.1\n",
    "------------\n",
    "\n",
    "Using your knowledge of the Poisson log likelihood gradient (exercise 0.1) and the content of the notebook 03\n",
    "on custom layers, implement the forward and backward pass of a custom layer that calculates the gradient of the\n",
    "Poisson log likelihood using a SIRF objective function as shown in the figure below.\n",
    "\n",
    "# ![](figs/poisson_logL_grad_layer.drawio.svg)\n",
    "\n",
    "The next cell contains the skeleton of the custom layer. You need to fill in the missing parts in the forward and\n",
    "backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIRFPoissonlogLGradLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x: torch.Tensor,\n",
    "        objective_function,\n",
    "        sirf_template_image: sirf.STIR.ImageData,\n",
    "        subset: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"(listmode) Poisson loglikelihood gradient layer forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : context object\n",
    "            used to store objects that we need in the backward pass\n",
    "        x : torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the image\n",
    "        objective_function : sirf (listmode) objective function\n",
    "            the objective function that we use to calculate the gradient\n",
    "        sirf_template_image : sirf.STIR.ImageData\n",
    "            image template that we use to convert between torch tensors and sirf images\n",
    "        subset : int\n",
    "            subset number used for the gradient calculation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the image\n",
    "            containing the gradient of the (listmode) Poisson log likelihood at x\n",
    "        \"\"\"\n",
    "        # we use the context object ctx to store objects that we need in the backward pass\n",
    "        ctx.device = x.device\n",
    "        ctx.objective_function = objective_function\n",
    "        ctx.dtype = x.dtype\n",
    "        ctx.subset = subset\n",
    "        ctx.sirf_template_image = sirf_template_image\n",
    "\n",
    "        # ==============================================================\n",
    "        # ==============================================================\n",
    "        # YOUR CODE HERE\n",
    "        # ==============================================================\n",
    "        # ==============================================================\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx, grad_output: torch.Tensor | None\n",
    "    ) -> tuple[torch.Tensor | None, None, None, None]:\n",
    "        \"\"\"(listmode) Poisson loglikelihood gradient layer backward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : context object\n",
    "            used to store objects that we need in the backward pass\n",
    "        grad_output : torch.Tensor | None\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the gradient (called v in the autograd tutorial)\n",
    "            https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#optional-reading-vector-calculus-using-autograd\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[torch.Tensor | None, None, None, None]\n",
    "            the Jacobian-vector product of the Poisson log likelihood gradient layer\n",
    "        \"\"\"\n",
    "\n",
    "        if grad_output is None:\n",
    "            return None, None, None, None\n",
    "        else:\n",
    "            ctx.sirf_template_image.fill(grad_output.cpu().numpy()[0, 0, ...])\n",
    "\n",
    "            # ==============================================================\n",
    "            # ==============================================================\n",
    "            # YOUR CODE HERE\n",
    "            # --------------\n",
    "            #\n",
    "            # calculate the Jacobian-vector product of the Poisson log likelihood gradient layer\n",
    "            # Hints: (1) try to derive the Jacobian of the gradient of the Poisson log likelihood gradient first\n",
    "            #        (2) the sirf.STIR objective function has a method called `multiply_with_Hessian`\n",
    "            #\n",
    "            # ==============================================================\n",
    "            # =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76074093",
   "metadata": {},
   "source": [
    "To view the solution to the exercise, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load snippets/solution_4_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensor and add the minibatch and channel dimensions\n",
    "x_t = (\n",
    "    torch.tensor(\n",
    "        lm_ref_recon.as_array(), device=dev, dtype=torch.float32, requires_grad=False\n",
    "    )\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(0)\n",
    ")\n",
    "\n",
    "# setup our custom Poisson log likelihood gradient layer\n",
    "poisson_logL_grad_layer = SIRFPoissonlogLGradLayer.apply\n",
    "# perform the forward pass (calcuate the gradient of the Poisson log likelihood at x_t)\n",
    "grad_x = poisson_logL_grad_layer(x_t, lm_obj_fun, initial_image, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333a7e3",
   "metadata": {},
   "source": [
    "Implementing a OSEM update layer using our custom Poisson log likelihood gradient layer\n",
    "=======================================================================================\n",
    "\n",
    "Using our custom Poisson log likelihood gradient layer, we can now implement a custom OSEM update layer.\n",
    "Note that the OSEM update can be decomposed into a simple feedforward network consisting of basic arithmetic\n",
    "operations that are implemented in pytorch (pointwise multiplication and addition) as shown in the figure below.\n",
    "\n",
    "# ![](figs/osem_layer.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee3155",
   "metadata": {},
   "source": [
    "Exercise 4.2\n",
    "------------\n",
    "Implement the forward pass of a OSEM update layer using the Poisson log likelihood gradient layer that we implemented\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae356bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSEMUpdateLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        objective_function,\n",
    "        sirf_template_image: sirf.STIR.ImageData,\n",
    "        subset: int,\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        \"\"\"OSEM update layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        objective_function : sirf (listmode) objective function\n",
    "            the objective function that we use to calculate the gradient\n",
    "        sirf_template_image : sirf.STIR.ImageData\n",
    "            image template that we use to convert between torch tensors and sirf images\n",
    "        subset : int\n",
    "            subset number used for the gradient calculation\n",
    "        device : str\n",
    "            device used for the calculations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the OSEM\n",
    "            update of the input image using the Poisson log likelihood objective function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._objective_function = objective_function\n",
    "        self._sirf_template_image: sirf.STIR.ImageData = sirf_template_image\n",
    "        self._subset: int = subset\n",
    "\n",
    "        self._poisson_logL_grad_layer = SIRFPoissonlogLGradLayer.apply\n",
    "\n",
    "        # setup a tensor containng the inverse of the subset sensitivity image adding the minibatch and channel dimensions\n",
    "        self._inv_sens_image: torch.Tensor = 1.0 / torch.tensor(\n",
    "            objective_function.get_subset_sensitivity(subset).as_array(),\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        # replace positive infinity values with 0 (voxels with 0 sensitivity)\n",
    "        torch.nan_to_num(self._inv_sens_image, posinf=0, out=self._inv_sens_image)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward pass of the OSEM update layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            minibatch tensor of shape [1,1,spatial_dimensions] containing the image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            OSEM update image\n",
    "        \"\"\"\n",
    "\n",
    "        # =======================================================================\n",
    "        # =======================================================================\n",
    "        # YOUR CODE HERE\n",
    "        # USE ONLY BASIC ARITHMETIC OPERATIONS BETWEEN TORCH TENSORS!\n",
    "        # =======================================================================\n",
    "        # ======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37bc2d",
   "metadata": {},
   "source": [
    "To view the solution to the exercise, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d30108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load snippets/solution_4_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26730f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the OSEM update layer for subset 0\n",
    "osem_layer0 = OSEMUpdateLayer(lm_obj_fun, initial_image, 0, dev)\n",
    "# perform the forward pass\n",
    "osem_updated_x_t = osem_layer0(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89124fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show the input and output of the OSEM update layer\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4), tight_layout=True)\n",
    "ax[0].imshow(x_t.cpu().numpy()[0, 0, 71, ...], cmap=\"Greys\")\n",
    "ax[1].imshow(osem_updated_x_t.cpu().numpy()[0, 0, 71, ...], cmap=\"Greys\")\n",
    "ax[2].imshow(\n",
    "    osem_updated_x_t.cpu().numpy()[0, 0, 71, ...] - x_t.cpu().numpy()[0, 0, 71, ...],\n",
    "    cmap=\"seismic\",\n",
    "    vmin=-0.01,\n",
    "    vmax=0.01,\n",
    ")\n",
    "ax[0].set_title(\"input image\")\n",
    "ax[1].set_title(\"OSEM updated image\")\n",
    "ax[2].set_title(\"diffence image\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f7a6f",
   "metadata": {},
   "source": [
    "Testing the backward pass of the custom layers\n",
    "----------------------------------------------\n",
    "\n",
    "As mentioned in the previous notebook, it is important to test whether the backward pass\n",
    "of the custom layer is implemented correctly using the `torch.autograd.gradcheck` function.\n",
    "**However, we won't do this here** - but rather disuss the implementation - because:\n",
    "- it can take long time\n",
    "- because we are using float32, we have to adapt the tolerances\n",
    "- the sirf.STIR gradient calculation is not exactly deterministic, due to parallelization and numerical precision\n",
    "  which also requires to adapt the tolerances for non-deterministic functions\n",
    "\n",
    "**If you implement a new layer, and you are not 100% sure that the backward pass is correct, you should always test it!**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
