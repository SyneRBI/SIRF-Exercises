{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import struct\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "* The data you need are under ```/home/jovyan/share/TBP/RECON```\n",
    "* You will need to copy ```__run__OSMAPOSL.par``` in your work folder\n",
    "* You will need to copy ```root_header.hroot``` in your work folder\n",
    "* Optional: You can copy ```total_2.root``` in this folder and in ```root_header.hroot``` edit the ```name of data file``` to point to the local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copyfile(os.path.join(base_path, '__run__OSMAPOSL.par'), '__run__OSMAPOSL.par')\n",
    "shutil.copyfile(os.path.join(base_path, 'root_header.hroot'), 'root_header.hroot')\n",
    "shutil.copyfile(os.path.join(base_path, 'total_2.root'), 'total_2.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basis for our project\n",
    "\n",
    "base_path = \"/home/jovyan/share/TBP/RECON\"\n",
    "work_path = \"/home/jovyan/share/USER\"\n",
    "\n",
    "if base_path == work_path:\n",
    "    print(\"DO NOT GO ANY FURTHER YOU WILL BE REPLACING YOUR EXISTS CACHE FILES. CHANGE OUTPUT PATH.\")\n",
    "\n",
    "if not os.path.exists(work_path):\n",
    "    os.mkdir(work_path)\n",
    "os.chdir(work_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open ```__run__OSMAPOSL.par```\n",
    "* Find the line ```maximum absolute segment number to process``` and set to a small number as 10\n",
    "* Find the line ```normalisation pro data filename``` and set to the path ```/home/jovyan/share/TBP/RECON/ATTEN/acf.hs```\n",
    "* Find the line ```recompute cache``` and set to \"0\"\n",
    "* If you wish rename the ```output filename prefix``` to something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we try to avoid reading the ROOT files as listmode data as only make use of the optimized cache files\n",
    "* From the cache files we need to remove the events that have a segment number larger than that the reconstruction expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This must be the same number you set in the __run__OSMAPOSL.par\n",
    "# Reduce the segment range\n",
    "numSegments = 10\n",
    "# Min Segment to keep\n",
    "min_seg = -numSegments\n",
    "max_seg = numSegments\n",
    "print(\"Number of Segments to keep \", numSegments)\n",
    "print(\"Min segment in output\", min_seg)\n",
    "print(\"Max segment in output\", max_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Find all cacheFiles in path\n",
    "#\n",
    "cacheFilenames = glob.glob(os.path.join(base_path, 'my*'))\n",
    "print(cacheFilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sanity Test. If the numbers below do not look reasonable. Abort!\n",
    "#\n",
    "with open(cacheFilenames[0], 'rb') as f:\n",
    "    for i in range(10):\n",
    "        record = f.read(28)\n",
    "        Seg, TOF, View, Axial, Tang, Val, Frm = struct.unpack('5i1f1i', record)\n",
    "        print(\"Bin: \", i, Seg, TOF, View, Axial, Tang, Frm, Val)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_read = 0\n",
    "for curr_cache in cacheFilenames: \n",
    "    print(\"Processing cache file\", curr_cache)\n",
    "    output_list = []\n",
    "\n",
    "    with open(curr_cache,'rb') as f:\n",
    "        while 1: \n",
    "            record = f.read(28)\n",
    "\n",
    "            if len(record) != 28: \n",
    "                break\n",
    "            Seg, TOF, View, Axial, Tang, Val, Frm = struct.unpack('5i1f1i',record)\n",
    "            if Seg <= min_seg or Seg >= max_seg:\n",
    "                continue\n",
    "                \n",
    "            output_list.append([Seg, TOF, View, Axial, Tang, Val, Frm])\n",
    "            events_read += 1\n",
    "            if events_read % 500000 == 0: \n",
    "                print(\"Events read: \", events_read)\n",
    "            # print(\"Bin: \", output_list[-1])\n",
    "    f.close()\n",
    "\n",
    "    output_cache_filepath = os.path.basename(curr_cache)\n",
    "    print(\"OUTPUT at\", output_cache_filepath)\n",
    "    with open (str(output_cache_filepath), 'wb') as f:\n",
    "        for bin in output_list: \n",
    "            f.write(struct.pack('5i1f1i', bin[0],\\\n",
    "                                bin[1],bin[2],\\\n",
    "                                    bin[3],bin[4],\n",
    "                                    bin[5], bin[6]))\n",
    "    f.close()\n",
    "\n",
    "print(\"Finished!\")\n",
    "print(\"Events in cache:\", events_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you can run ```OSMAPOSL __run__OSMAPOSL.par```\n",
    "* And wait.\n",
    "* If you run the reconstruction with this settings first time you need to have ```recompute sensitivity``` set to ```1```. But reconstructing again with the same settings you can skip recomputing the sensitivity.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = 'sens.v'\n",
    "array = np.fromfile(image_filename, dtype=np.float32).reshape([503, 271, 271])\n",
    "print(\"Max value: \", array.max())\n",
    "plt.imshow(array[:, :, 120], vmin = 0, vmax=array.max()*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = '_del_2.v'\n",
    "array = np.fromfile(image_filename, dtype=np.float32).reshape([503, 271, 271])\n",
    "print(\"Max value: \", array.max())\n",
    "plt.imshow(array[:, :, 140], vmin = 0, vmax=array.max()*0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing gaps\n",
    "* It has been proposed to reduce the cost by introducing gaps in the geometry. \n",
    "* It is straingforward to introduce gaps in the radial positions, and exclude detectors\n",
    "* The following function ```is_gap``` will exclude blocks or modules (buckets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reject blocks \n",
    "reject_blks = False\n",
    "# Reject i_th block\n",
    "blk_rejection = 3\n",
    "\n",
    "# Reject buckets\n",
    "reject_buckets = True\n",
    "# Reject i_th bucket\n",
    "bucket_rejection = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't edit these numbers\n",
    "num_detectors = 840\n",
    "num_rings = 252\n",
    "ring_spacing = 2.85\n",
    "num_xtal_per_blk_trans = 7\n",
    "num_xtal_per_blk_axial = 6\n",
    "num_blk_per_bucket_trans = 5\n",
    "num_blk_per_bucket_axial = 14\n",
    "num_buckets = 24\n",
    "num_full_rings = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_pair(Seg, View, Axial, Tang):\n",
    "    d1 =  (View + (Tang //2) + num_detectors) % num_detectors\n",
    "    d2 =  (View - ((Tang + 1) //2) + num_detectors / 2) % num_detectors;\n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "def is_gap(Seg, View, Axial, Tang):\n",
    "    \n",
    "    d1, d2 = get_detector_pair(Seg, View, Axial, Tang)\n",
    "\n",
    "    if reject_blks:\n",
    "        if(d1 // num_blk_per_bucket_trans % blk_rejection): \n",
    "            return True # is gap\n",
    "        if(d2 // num_blk_per_bucket_trans % blk_rejection): \n",
    "            return True # is gap\n",
    "    \n",
    "    if reject_buckets:\n",
    "        if(d1 // num_buckets % bucket_rejection): \n",
    "            return True # is gap\n",
    "        if(d2 // num_buckets % bucket_rejection): \n",
    "            return True # is gap\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_read = 0\n",
    "for curr_cache in cacheFilenames: \n",
    "    print(\"Processing cache file\", curr_cache)\n",
    "    output_list = []\n",
    "\n",
    "    with open(curr_cache,'rb') as f:\n",
    "        while 1: \n",
    "            record = f.read(28)\n",
    "\n",
    "            if len(record) != 28: \n",
    "                break\n",
    "            Seg, TOF, View, Axial, Tang, Val, Frm = struct.unpack('5i1f1i',record)\n",
    "            if Seg <= min_seg or Seg >= max_seg:\n",
    "                continue\n",
    "\n",
    "            if is_gap(Seg, View, Axial, Tang):\n",
    "                continue\n",
    "                \n",
    "            output_list.append([Seg, TOF, View, Axial, Tang, Val, Frm])\n",
    "            events_read += 1\n",
    "            if events_read % 500000 == 0: \n",
    "                print(\"Events read: \", events_read)\n",
    "            # print(\"Bin: \", output_list[-1])\n",
    "    f.close()\n",
    "\n",
    "    output_cache_filepath = os.path.basename(curr_cache)\n",
    "    print(\"OUTPUT at\", output_cache_filepath)\n",
    "    with open (str(output_cache_filepath), 'wb') as f:\n",
    "        for bin in output_list: \n",
    "            f.write(struct.pack('5i1f1i', bin[0],\\\n",
    "                                bin[1],bin[2],\\\n",
    "                                    bin[3],bin[4],\n",
    "                                    bin[5], bin[6]))\n",
    "    f.close()\n",
    "\n",
    "print(\"Finished!\")\n",
    "print(\"Events in cache:\", events_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now run the reconstruction\n",
    "* If the num of segments is the same you can skip the calculation of the sensitivity image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = '_del_2.v'\n",
    "array = np.fromfile(image_filename, dtype=np.float32).reshape([503, 271, 271])\n",
    "print(\"Max value: \", array.max())\n",
    "plt.imshow(array[:, :, 140], vmin = 0, vmax=array.max()*0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reject blocks \n",
    "reject_blks = True\n",
    "# Reject i_th block\n",
    "blk_rejection = 3\n",
    "\n",
    "# Reject buckets\n",
    "reject_buckets = False\n",
    "# Reject i_th bucket\n",
    "bucket_rejection = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_read = 0\n",
    "for curr_cache in cacheFilenames: \n",
    "    print(\"Processing cache file\", curr_cache)\n",
    "    output_list = []\n",
    "\n",
    "    with open(curr_cache,'rb') as f:\n",
    "        while 1: \n",
    "            record = f.read(28)\n",
    "\n",
    "            if len(record) != 28: \n",
    "                break\n",
    "            Seg, TOF, View, Axial, Tang, Val, Frm = struct.unpack('5i1f1i',record)\n",
    "            if Seg <= min_seg or Seg >= max_seg:\n",
    "                continue\n",
    "\n",
    "            if is_gap(Seg, View, Axial, Tang):\n",
    "                continue\n",
    "                \n",
    "            output_list.append([Seg, TOF, View, Axial, Tang, Val, Frm])\n",
    "            events_read += 1\n",
    "            if events_read % 500000 == 0: \n",
    "                print(\"Events read: \", events_read)\n",
    "            # print(\"Bin: \", output_list[-1])\n",
    "    f.close()\n",
    "\n",
    "    output_cache_filepath = os.path.basename(curr_cache)\n",
    "    print(\"OUTPUT at\", output_cache_filepath)\n",
    "    with open (str(output_cache_filepath), 'wb') as f:\n",
    "        for bin in output_list: \n",
    "            f.write(struct.pack('5i1f1i', bin[0],\\\n",
    "                                bin[1],bin[2],\\\n",
    "                                    bin[3],bin[4],\n",
    "                                    bin[5], bin[6]))\n",
    "    f.close()\n",
    "\n",
    "print(\"Finished!\")\n",
    "print(\"Events in cache:\", events_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = '_del_2.v'\n",
    "array = np.fromfile(image_filename, dtype=np.float32).reshape([503, 271, 271])\n",
    "print(\"Max value: \", array.max())\n",
    "plt.imshow(array[:, :, 140], vmin = 0, vmax=array.max()*0.35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
