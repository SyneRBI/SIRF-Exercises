{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing OSEM (or another reconstruction algorithm) yourself with SIRF\n",
    "This notebook invites you to write MLEM and OSEM yourself using SIRF functionality, i.e. Do It Yourself OSEM!\n",
    "\n",
    "You should have completed the [OSEM_reconstruction notebook](OSEM_reconstruction.ipynb) first. The [ML_reconstruction notebook](ML_reconstruction.ipynb) might help as well.\n",
    "\n",
    "The notebook is currently set-up to use prepared data with a single slice of an XCAT phantom, with a low resolution scanner, such that all results can be obtained easily on a laptop. Of course, your code will have to work for any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Kris Thielemans  \n",
    "First version: June 2021\n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2021 University College London.\n",
    "\n",
    "This is software developed for the Collaborative Computational Project in Synergistic Reconstruction for Biomedical Imaging (http://www.ccpsynerbi.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% make sure figures appears inline and animations works\n",
    "%matplotlib widget\n",
    "\n",
    "# Setup the working directory for the notebook\n",
    "import notebook_setup\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "cd_to_working_dir('PET', 'OSEM_reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Initial imports etc\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sirf.STIR as pet\n",
    "from sirf.Utilities import examples_data_path\n",
    "from sirf_exercises import exercises_data_path\n",
    "\n",
    "# define the directory with input files for this notebook\n",
    "data_path = os.path.join(examples_data_path('PET'), 'thorax_single_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up redirection of STIR messages to files\n",
    "msg_red = pet.MessageRedirector('info.txt', 'warnings.txt', 'errors.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will first create some simulated data from ground-truth images\n",
    "see previous notebooks for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read in images\n",
    "image = pet.ImageData(os.path.join(data_path, 'emission.hv'))\n",
    "attn_image = pet.ImageData(os.path.join(data_path, 'attenuation.hv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save max for future displays\n",
    "im_slice = image.dimensions()[0]//2\n",
    "cmax = image.max()*.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% create acquisition model\n",
    "acq_model = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "template = pet.AcquisitionData(os.path.join(data_path, 'template_sinogram.hs'))\n",
    "acq_model.set_up(template, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% simulate data using forward projection\n",
    "acquired_data = acq_model.forward(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Expectatin Maximisation (MLEM)\n",
    "Also called EMML. This is a standard algorithm, derived by using EM for the PET (or SPECT) problem. See the paper:\n",
    "\n",
    "Shepp, L. A., and Y. Vardi. ‘Maximum Likelihood Reconstruction for Emission Tomography’. IEEE Transactions on Medical Imaging 1, no. 2 (1982): 113-122+.\n",
    "\n",
    "Our notation here: $x$ is the image, $y$ the measured data with $A$ the system matrix. This is different from the Shepp and Vardi paper, which uses $\\lambda$ for the image, $n^*$ for the measured data, $p(b,d)$ for the elements of the system matrix, and it has no background.\n",
    "\n",
    "In our notation, the model for the mean of the data (i.e., modelling the expected measurement, given an image $x$) is\n",
    "$$ \\bar{y}=A x + b$$\n",
    "\n",
    "The MLEM update is\n",
    "$$ x^{\\mathrm{new}} = \\frac{x}{A^t 1} A^t \\left(\\frac{y}{A x + b}\\right)$$\n",
    "\n",
    "You hopefully recognise that the denominator of the factor on the right corresponds to the `forward` model applied ot the image $x$. Multiplication with the $A^t$ is the `backward` operation. So, we have used all the main operations already. We just need to do element-wise multiplication an division operation, but that's easy!\n",
    "\n",
    "Let's first compute $A^t 1$, as this is a image that won't change over iterations. Note that the $1$ here represents a one-vector, i.e., an image filled with ones. It is often called the \"sensivity image\" as it is (proportional to) the probability that an event emitted in a voxel is detected by the scanner (without scattering). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = acq_model.backward(acquired_data.get_uniform_copy(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialise the algorithm with a uniform image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_image = image.get_uniform_copy(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can do one MLEM iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotient = acquired_data/acq_model.forward(estimated_image)  # y / (Ax + b)\n",
    "mult_update = acq_model.backward(quotient)/sensitivity       # A^t * quotient / A^t1\n",
    "estimated_image *= mult_update                               # update (in place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(estimated_image.as_array()[im_slice,:,:])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mult_update.as_array()[im_slice,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can of course duplicate some of these lines, or re-execute the above cells. However, it makes more sense to write a function to do this. Something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLEM(acquired_data, acq_model, initial_image, num_iterations):\n",
    "    estimated_image = initial_image.clone()\n",
    "    #  some stuff here\n",
    "    return estimated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_image = MLEM(acquired_data, acq_model,image.get_uniform_copy(1), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was hopefully not too hard. There are a few problems though that you might encounter.\n",
    "- your image might display nice, but on closer investigation will probably contain NaNs (Not a Number). These come from divisions: 0/0 is not defined. They can occur in 2 places:\n",
    "    - division in the acquisition data term. This should in theory not occur if you start with a strictly positive image that is large enough to \"cover\" all of the projection data. Of course, in practice it could happen that your image is smaller than the Field of View (FOV), and you might need to handle this anyway.\n",
    "    - division of the images. This will occur wherever the sensitivity image is zero. This difficulty is not a surprise: if a voxel cannot be measured, its ML estimate is undefined.\n",
    " \n",
    "In our current example, we have the second problem, as by default the projector uses a circular FOV. You might want to add a post-processing step that sets those values to zero. (Check out the `numpy.isnan` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The STIR implementation of MLEM (`OSMAPOSL`) which we are using takes care of these corner cases, as well as any negatives in the data (arising when pre-correcting the data, as although this should not be done in theory, some people are interested in this anyway)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordered Subsets Expectation Maximisation (OSEM)\n",
    "As discussed in previous notebooks, MLEM is great but slow. OSEM was introduced in\n",
    "\n",
    "Hudson, H.M., and R.S. Larkin. ‘Accelerated Image Reconstruction Using Ordered Subsets of Projection Data’. IEEE Transactions on Medical Imaging 13, no. 4 (December 1994): 601–9. https://doi.org/10.1109/42.363108.\n",
    "\n",
    "The update formula is exactly the same, except that at every update, only a subset of the data is used, i.e. restricting the data $y$, background $b$ and the matrix $A$ to a subset of all the rows. Clearly, for $N$ subsets, this reduces the number of computations for one image update with a factor $N$. While each update might be somewhat less accurate, it certainly works well in initial iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we implement this in SIRF? Luckily, an `sirf.STIR` acquisition model can be told to use only a subset of the data. The class documentation will show you that you can set `num_subsets` and `subset_num`.\n",
    "\n",
    "(There is currently no way to change how the subsets are chosen, but only the number of subsets).\n",
    "\n",
    "Note: the call to `forward` and `backward` also supports extra arguments for specifying the subsets. However, these are deprecated and will be removed in a future release.\n",
    "\n",
    "Some interesting things to try:\n",
    "  - how does varying the `subset_num` affect the projection?\n",
    "  - how does varying the `num_subsets` affect the projection sparsity?\n",
    "  - what happens if you sum over all the subsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_model.num_subsets = 4\n",
    "acq_model.subset_num = 0  # for 4 subsets, use a number between 0 and 3\n",
    "data = acq_model.forward(image)\n",
    "data.show(im_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, when we use this above, SIRF is not able to restrict the data to a particular subset. There are 3 ways around that:\n",
    "- ignore it and do the divisions over all of the data anyway. This will lead to 1/0 etc, but as those elements of the data are not backprojected, it won't change the final image.\n",
    "- construct several \"masks\" as `AcquisitionData by forward projection an image full of ones for every subsets and doing some thresholding.\n",
    "- use `sirf.STIR.AcquisitionData.get_subset()` (available since SIRF 3.4, and not illustrated here yet)\n",
    "\n",
    "Clearly, the first option is easiest (although it does mean there is some overhead in computing extra additions/divisions). Let's see if it works ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = acq_model.backward(acquired_data/data)\n",
    "check.show(im_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be in a position to write your own OSEM algorithm. Don't forget that for a strict implementation of OSEM, you need to compute \"subset sensitivities\" for the division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OSEM(acquired_data, acq_model, initial_image, num_iterations):\n",
    "    estimated_image=initial_image.clone()\n",
    "    # some stuff here - hint, this will be similar to your solution for MLEM\n",
    "    # but you will have to additionally iterate over your subsets \n",
    "    return estimated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "Hopefully you have learned that taking an algorithm from a paper and implementing it yourself should be easy enough in SIRF. However, you probably also learned that you might encounter some subtleties that are often not so easy to spot when you read a paper.\n",
    "\n",
    "The STIR `OSMAPOSL` implementation attempts to take care of these subtleties. It of course also avoids overheads such as the divisions with the subsets. Finally, it uses a multi-threaded implementation of the computation of the update that might be a bit faster than using calling the `forward` and `backward` operations directly (although these are multi-threaded as well).\n",
    "\n",
    "When trying to implement an algorithm of a paper, there is often a choice at what \"level\" you choose for your code. In the above, we went to the projector-level. In the [ML_reconstruction notebook](ML_reconstruction.ipynb) we constructed an objective function and used the `update` and `objective_function_value` members to do a lot of the hard work. Similarly, the [MAP_EM notebook](MAP_EM.ipynb) that you could tackle now writes a MAP algorithm in terms of (OS)EM functionality. All choices will probably work ok, but there are various trade-offs between verbosity, flexibility, extendability to consider, but we won't go into that here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
