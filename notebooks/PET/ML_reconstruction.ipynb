{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demonstration of maximum-likelihood reconstruction with SIRF\n",
    "This demonstration shows how to monitor progress of a SIRF reconstructor (currently using OSEM as an example) and implement a (simplistic) gradient-ascent algorithm using SIRF. This notebook can be extended to use regularised reconstruction as well.\n",
    "\n",
    "Please complete the [OSEM_reconstruction notebook](OSEM_reconstruction.ipynb) first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Kris Thielemans and Evgueni Ovtchinnikov  \n",
    "First version: 8th of September 2016  \n",
    "Second version: 17th of May 2018  \n",
    "Third version: June 2021\n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2015 - 2017 Rutherford Appleton Laboratory STFC  \n",
    "Copyright 2015 - 2018, 2021 University College London\n",
    "\n",
    "This is software developed for the Collaborative Computational Project in Synergistic Reconstruction for Biomedical Imaging (http://www.ccpsynerbi.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on terminology\n",
    "\n",
    "Because we are maximising the likelihood, SIRF generally wants to *maximise* the objective function. Many optimisation books, and CIL, are written for minimisation. You would therefore have to multiply the objective function with `-1`.\n",
    "\n",
    "OSEM and other algorithms use \"subsets\" of the data to compute an image update. `sirf.STIR` uses subsets in \"views\" and the following terminology\n",
    "- `num_subsets`: the number of subsets\n",
    "- `subset_num`: the subset that you're using now (range `0`...`num_subsets-1`)\n",
    "- sub-iteration: one image update using one subset of the data\n",
    "- `num_subiterations`: the total number of image updates used by the algorithm.\n",
    "\n",
    "Therefore, a (\"full\") iteration updates the image `num_subiterations` times. A full iteration is also called an \"epoch\".\n",
    "\n",
    "OSEM et al. use \"ordered subsets\", i.e. they go through the subsets in a fixed order (currently not changeable in `sirf.STIR`). Other algorithms like \"stochastic gradient ascent\" use subsets in a random order. These are currently not illustrated here, but could easily be implemented based on the code in this notebook (by using `recon.set_subset_num`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% make sure figures appears inline and animations works\n",
    "%matplotlib widget\n",
    "\n",
    "# Setup the working directory for the notebook\n",
    "import notebook_setup\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "cd_to_working_dir('PET', 'ML_reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "#import scipy\n",
    "#from scipy import optimize\n",
    "import sirf.STIR as pet\n",
    "from sirf.Utilities import examples_data_path\n",
    "from sirf_exercises import exercises_data_path\n",
    "\n",
    "# define the directory with input files for this notebook\n",
    "data_path = os.path.join(examples_data_path('PET'), 'thorax_single_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up redirection of STIR messages to files\n",
    "msg_red = pet.MessageRedirector('info.txt', 'warnings.txt', 'errors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% some handy function definitions\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink=.4)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def make_positive(image_array):\n",
    "    \"\"\"truncate any negatives to zero\"\"\"\n",
    "    image_array[image_array<0] = 0\n",
    "    return image_array\n",
    "\n",
    "def make_cylindrical_FOV(image):\n",
    "    \"\"\"truncate to cylindrical FOV\"\"\"\n",
    "    filter = pet.TruncateToCylinderProcessor()\n",
    "    filter.apply(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some simulated data from ground-truth images\n",
    "This is a repetition of the code in the OSEM notebook, just such that the current notebook is self-contained. However, there are no explanations here.\n",
    "\n",
    "You should be able to adapt the notebook to use your own data as well of course. The actual reconstruction exercises and its evaluation does not require that the input is a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read in images\n",
    "image = pet.ImageData(os.path.join(data_path, 'emission.hv'))*0.05\n",
    "attn_image = pet.ImageData(os.path.join(data_path, 'attenuation.hv'))\n",
    "template = pet.AcquisitionData(os.path.join(data_path, 'template_sinogram.hs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save max for future displays\n",
    "cmax = image.max()*.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create attenuation\n",
    "acq_model_for_attn = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "asm_attn = pet.AcquisitionSensitivityModel(attn_image, acq_model_for_attn)\n",
    "asm_attn.set_up(template)\n",
    "attn_factors = asm_attn.forward(template.get_uniform_copy(1))\n",
    "asm_attn = pet.AcquisitionSensitivityModel(attn_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create acquisition model\n",
    "acq_model = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "# we will increase the number of rays used for every Line-of-Response (LOR) as an example\n",
    "# (it is not required for the exercise of course)\n",
    "acq_model.set_num_tangential_LORs(5)\n",
    "acq_model.set_acquisition_sensitivity(asm_attn)\n",
    "# set-up\n",
    "acq_model.set_up(template,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% simulate some data using forward projection\n",
    "acquired_data=acq_model.forward(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the objective function and  OSMAPOSL reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun = pet.make_Poisson_loglikelihood(acquired_data)\n",
    "obj_fun.set_acquisition_model(acq_model)\n",
    "# we could also add a prior, but we will not do that here (although the rest of the exercise would still work)\n",
    "#obj_fun.set_prior(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = pet.OSMAPOSLReconstructor()\n",
    "recon.set_objective_function(obj_fun)\n",
    "recon.set_num_subsets(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create initial image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous OSEM notebook, we just used a uniform image. Here, we will use a disk that roughly corresponds to the *Field of View (FOV)*. The reason for this is that it makes things easier for display and the gradient ascent code below.\n",
    "\n",
    "An alternative solution would be to tell the `acq_model` to use a square FOV as opposed to a circular one, but that will slow down calculations just a little bit, so we won't do that here (feel free to try!).\n",
    "\n",
    "In addition, the initial value is going to be a bit more important here as we're going to plot the value of the objective function. Obviously, having a descent estimate of the scale of the image will make that plot look more sensible. Feel free to experiment with the value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image=image.get_uniform_copy(cmax / 4)\n",
    "make_cylindrical_FOV(initial_image)\n",
    "# display\n",
    "im_slice = initial_image.dimensions()[0] // 2\n",
    "plt.figure()\n",
    "plot_2d_image([1,1,1],initial_image.as_array()[im_slice,:,:], 'initial image',[0,cmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the OSMAPOSL reconstructor to do all the work\n",
    "This is the same as in the OSEM notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the reconstructor\n",
    "num_subiters=100\n",
    "recon.set_num_subiterations(num_subiters)\n",
    "recon.set_up(initial_image)\n",
    "# do actual recon\n",
    "recon.set_current_estimate(initial_image)\n",
    "recon.process()\n",
    "reconstructed_image=recon.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4))\n",
    "plot_2d_image([1,2,1],image.as_array()[im_slice,:,:,],'ground truth image',[0,cmax*1.2])\n",
    "plot_2d_image([1,2,2],reconstructed_image.as_array()[im_slice,:,:,],'reconstructed image',[0,cmax*1.2])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking control of the iteration process\n",
    "We will now show how to run each sub-iteration from in Python, as opposed to\n",
    "letting the reconstructor do all sub-iterations at once.\n",
    "\n",
    "The lines below are a bit complicated as we save the image at every update, as well as saving the objective function value. That way, we can display various things below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run same reconstruction but saving images and objective function values every sub-iteration\n",
    "num_subiters = 64\n",
    "\n",
    "# create an image object that will be updated during the iterations\n",
    "current_image = initial_image.clone()\n",
    "\n",
    "# create an array to store the values of the objective function at every\n",
    "# sub-iteration (and fill in the first)\n",
    "osem_objective_function_values = [obj_fun.value(current_image)]\n",
    "\n",
    "# create an ndarray to store the images at every sub-iteration\n",
    "all_osem_images = numpy.ndarray(shape=(num_subiters + 1,) + current_image.dimensions())\n",
    "all_osem_images[0,:,:,:] = current_image.as_array()\n",
    "\n",
    "# do the actual updates\n",
    "for i in range(1, num_subiters+1):\n",
    "    recon.update(current_image)\n",
    "    # store results\n",
    "    obj_fun_value = obj_fun.value(current_image)\n",
    "    osem_objective_function_values.append(obj_fun_value)\n",
    "    all_osem_images[i,:,:,:] =  current_image.as_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some plots with these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define a function for plotting images and the updates\n",
    "def plot_progress(all_images, title, subiterations = []):\n",
    "    if len(subiterations) == 0:\n",
    "        num_subiters = all_images[0].shape[0] - 1\n",
    "        subiterations = range(1, num_subiters + 1)\n",
    "    num_rows = len(all_images)\n",
    "\n",
    "    for i in subiterations:\n",
    "        plt.figure()\n",
    "        for r in range(num_rows):\n",
    "            plot_2d_image([num_rows,2,2 * r + 1],\n",
    "                          all_images[r][i,im_slice,:,:],'%s at %d' % (title[r], i), [0,cmax])\n",
    "            plot_2d_image([num_rows,2,2*r+2],\n",
    "                          all_images[r][i,im_slice,:,:]-all_images[r][i - 1,im_slice,:,:],'update',[-cmax*.05,cmax*.05], cmap='seismic')\n",
    "        #plt.pause(.05)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% now call this function to see how we went along\n",
    "# note that in the notebook interface, this might create a box with a vertical slider\n",
    "subiterations = (1,2,4,8,16,32,64)\n",
    "# close all \"open\" images as otherwise we will get warnings (the notebook interface keeps them \"open\" somehow)\n",
    "plt.close('all')    \n",
    "plot_progress([all_osem_images], ['OSEM'],subiterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot objective function values\n",
    "plt.figure()\n",
    "#plt.plot(subiterations, [ osem_objective_function_values[i] for i in subiterations])\n",
    "plt.plot(osem_objective_function_values)\n",
    "plt.title('Objective function values')\n",
    "plt.xlabel('sub-iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot seems to indicate that (OS)EM converges to a stable value of the\n",
    "log-likelihood very quickly. However, as we've seen, the images are still changing.\n",
    "\n",
    "Convince yourself that the likelihood is still increasing (either by zooming into the figure, or by using `plt.ylim`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute some simple ROI values as well. Let's plot those.\n",
    "\n",
    "You might want to convince yourself first that these ROI are in the correct place (but it doesn't matter too much for this exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ROI\n",
    "ROI_lesion = all_osem_images[:,(im_slice,), 65:70, 40:45]\n",
    "ROI_lung = all_osem_images[:,(im_slice,), 75:80, 45:50]\n",
    "\n",
    "ROI_mean_lesion = ROI_lesion.mean(axis=(1,2,3))\n",
    "ROI_std_lesion = ROI_lesion.std(axis=(1,2,3))\n",
    "\n",
    "ROI_mean_lung = ROI_lung.mean(axis=(1,2,3))\n",
    "ROI_std_lung = ROI_lung.std(axis=(1,2,3))\n",
    "\n",
    "plt.figure()\n",
    "#plt.hold('on')\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ROI_mean_lesion,'k',label='lesion')\n",
    "plt.plot(ROI_mean_lung,'r',label='lung')\n",
    "plt.legend()\n",
    "plt.title('ROI mean')\n",
    "plt.xlabel('sub-iterations')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ROI_std_lesion, 'k',label='lesion')\n",
    "plt.plot(ROI_std_lung, 'r',label='lung')\n",
    "plt.legend()\n",
    "plt.title('ROI standard deviation')\n",
    "plt.xlabel('sub-iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots indicate that the log-likelihood is not very sensitive\n",
    "to changes in the image. This is because it measures changes in the projected data, and is an illustration that image reconstruction is an ill-conditioned inverse problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement gradient ascent and compare with OSEM\n",
    "Here we will implement a simple version of Gradient Ascent using SIRF functions.We will use\n",
    "the SIRF capability to return the gradient of the objective function directly.\n",
    "\n",
    "Gradient ascent (GA) works by updating the image in the direction of the gradient\n",
    "\n",
    "    new_image = current_image + step_size * gradient\n",
    "\n",
    "Here we will use a fixed step-size and use \"truncation\" to enforce\n",
    "non-negativity of the image.\n",
    "\n",
    "In the code below, manipulations such as positivity are done via `numpy`, so we use `as_array()` and do all additions etc on `numpy` objects as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define some variables to perform gradient ascent for a few (sub)iterations\n",
    "num_subiters = 32\n",
    "# relative step-size\n",
    "tau = .3\n",
    "\n",
    "# set initial image and store it\n",
    "# also store the value of the objective function for plotting\n",
    "current_image = initial_image.clone()\n",
    "GA_objective_function_values = [obj_fun.value(current_image)]\n",
    "# create an array with all reconstruct images for plotting\n",
    "idata = current_image.as_array()\n",
    "all_images = numpy.ndarray(shape=(num_subiters + 1,) + idata.shape)\n",
    "all_images[0,:,:,:] =  idata;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% perform GA iterations\n",
    "# executing this cell might take a while\n",
    "for i in range(1, num_subiters+1):  \n",
    "    # obtain gradient for subset 0\n",
    "    # with current settings, this means we will only use the data of that subset\n",
    "    # (gradient ascent with subsets is too complicated for this demo)\n",
    "    grad = obj_fun.gradient(current_image, 0)\n",
    "    grad_array = grad.as_array()\n",
    "\n",
    "    # compute step-size as relative to current image-norm\n",
    "    step_size = tau * norm(idata) / norm(grad_array)\n",
    "\n",
    "    # perform gradient ascent step and truncate to positive values\n",
    "    idata = make_positive(idata + step_size*grad_array)\n",
    "    current_image.fill(idata)\n",
    "\n",
    "    # compute objective function value for plotting, and write some diagnostics\n",
    "    obj_fun_value = obj_fun.value(current_image)\n",
    "    GA_objective_function_values.append(obj_fun_value)\n",
    "    all_images[i,:,:,:] = idata;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot objective function values\n",
    "plt.figure()\n",
    "#plt.hold('on')\n",
    "plt.title('Objective function value vs subiterations')\n",
    "plt.plot(GA_objective_function_values,'b')\n",
    "plt.plot(osem_objective_function_values,'r')\n",
    "plt.legend(('gradient ascent', 'OSEM'),loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compare GA and OSEM images\n",
    "plot_progress([all_images, all_osem_images], ['GA' ,'OSEM'],[2,4,8,16,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above implementation used a fixed (relative) step-size. Experiment with different values for `tau` and see how that influences convergence.\n",
    "\n",
    "Steepest gradient ascent will include a line search to estimate the step size. There is a demo\n",
    "in the SIRF code on this. You can [find the code here as well](https://github.com/CCPPETMR/SIRF/blob/master/examples/Python/PET/steepest_ascent.py). You could implement this here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: repeat this analysis with noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above simulation, the `acquired_data` was \"perfect\", i.e. it was the output of the same acquisition model as used for the reconstruction *and* there was no noise in the data. In real life, you will never be so lucky!\n",
    "\n",
    "Of course, performance of a reconstruction algorithm needs to be investigated in more realistic scenarios. We suggest that you use a Poisson realisation of the data, and then repeat the above cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `numpy.random.poisson` function to create a noisy realisation of the simulated data. Of course, we will need to convert the data to `numpy` first via `as_array()`.\n",
    "\n",
    "One thing to watch out for is that Poisson statstics is (solely) determined by the mean (in contrast to the normal distribution, which has separate mean and standard deviation). Therefore, the \"magnitude\" of the simulated data will be very important to determine the noise level. The relevant formula for Poisson statistics is that\n",
    "<center>variance = mean</center>\n",
    "\n",
    "This exercise is set-up such that the mean of the simulated data is \"reasonable\" such that you will get some noise in the data, but not too much. Obviously, if you use other data, you will have to check what happens. You can simply rescale the `acquired_data` (up for less noise, down for more), which will then of course rescale the reconstructed images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Generate a noisy realisation of the data\n",
    "\n",
    "noisy_array=numpy.random.poisson(acquired_data.as_array()).astype('float64')\n",
    "print(' Maximum counts in the data: %d' % noisy_array.max())\n",
    "# stuff into a new AcquisitionData object\n",
    "noisy_data = acquired_data.clone()\n",
    "noisy_data.fill(noisy_array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Display bitmaps of the middle sinogram\n",
    "plt.figure(figsize=(9, 4))\n",
    "plot_2d_image([1,2,1],acquired_data.as_array()[0,im_slice,:,:,],'original',[0,acquired_data.max()])\n",
    "plot_2d_image([1,2,2],noisy_array[0,im_slice,:,:,],'noisy',[0,acquired_data.max()])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the objective function to use the noisy data instead. The rest of the cells above wouldn't need any changes (but do check!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun.set_acquisition_data(noisy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things that you might discover\n",
    "- Without any noise, the OSEM (or MLEM or gradient ascent) reconstructions looked pretty good, and increasing the number of updates is beneficial. However, with noise they will gradually deteriorate. This is a consequence of the illposedness of the reconstruction problem, and shows that regularisation is needed.\n",
    "- With noise, the objective function no longer increases monotonically, but a clear pattern is seen in terms of the number of subsets. We recommend changing the number of subsets to see this how this affects your images.<br>The underlying reason is that OSEM is *not* a convergent algorithm and (nearly always) results in a \"limit-cycle\". Other algorithms that use subsets and do converge exist.\n",
    "- You might have to change the step-size for the gradient ascent algorithm, as also discussed above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
