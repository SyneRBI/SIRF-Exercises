{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-Corrected Image Reconstruction (MCIR)\n",
    "\n",
    "This demonstration shows how to obtain a motion surrogate, estimate motion vector fields and carry out a motion-corrected image reconstruction.\n",
    "\n",
    "This demo is a 'script', i.e. intended to be run step by step in a\n",
    "Python notebook such as Jupyter. It is organised in 'cells'. Jupyter displays these\n",
    "cells nicely and allows you to run each cell on its own."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First version: 14th of June 2021  \n",
    "Updated: 8th of May 2023  \n",
    "Author: Christoph Kolbitsch  \n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2015 - 2021 Rutherford Appleton Laboratory STFC.  \n",
    "Copyright 2015 - 2021 University College London.  \n",
    "Copyright 2015 - 2023 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational Project in Synergistic Reconstruction for Biomedical Imaging \n",
    "(http://www.ccpsynerbi.ac.uk/).\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib widget\n",
    "\n",
    "# Setup the working directory for the notebook\n",
    "import sys\n",
    "sys.path.append('SIRF-Exercises/notebooks/MR')\n",
    "import notebook_setup\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "cd_to_working_dir('MR', 'h_mr_mcir_grpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__version__ = '0.1.0'\n",
    "\n",
    "# import engine module\n",
    "import sirf.Gadgetron as pMR\n",
    "import sirf.Reg as pReg\n",
    "\n",
    "\n",
    "from cil.framework import  BlockDataContainer\n",
    "from cil.optimisation.functions import LeastSquares, ZeroFunction\n",
    "from cil.optimisation.operators import BlockOperator, CompositionOperator\n",
    "from cil.optimisation.algorithms import FISTA\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_TV\n",
    "\n",
    "# import further modules\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal as sp_signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which plots 3D volume(s) in two orthogonal views\n",
    "def plot_rpe_3d(dat, sl_idx, lbl, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(2,len(dat), squeeze=False)\n",
    "    for ind in range(len(dat)):\n",
    "        ax[0,ind].imshow(np.rot90(np.abs(dat[ind][:, sl_idx[0], :]), 1))\n",
    "        ax[0,ind].set_xticks([])\n",
    "        ax[0,ind].set_yticks([])\n",
    "        ax[0,ind].set_ylabel('Foot-Head')\n",
    "        ax[0,ind].set_xlabel('Right-Left')\n",
    "        ax[0,ind].set_title(lbl[ind])\n",
    "        \n",
    "        ax[1,ind].imshow(np.rot90(np.abs(dat[ind][:, :, sl_idx[1]])))\n",
    "        ax[1,ind].set_xticks([])\n",
    "        ax[1,ind].set_yticks([])\n",
    "        ax[1,ind].set_ylabel('Anterior-Posterior')\n",
    "        ax[1,ind].set_xlabel('Right-Left')\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are going to use a 3D data set which has been acquired with a Golden Radial Phase Encoding (GRPE) scheme. For more information on this type of k-space sampling, please have a look at the __MR__ notebook `g_non_cartesian_reconstruction.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# Run this script to make sure the data is downloaded.\n",
    "#bash ../../scripts/download_data.sh -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data_path = exercises_data_path('MR')\n",
    "filename = os.path.join(data_path, '3D_GRPE_motion.h5')\n",
    "acq_data = pMR.AcquisitionData(filename)\n",
    "acq_data.sort_by_time()\n",
    "\n",
    "# Here we are cheating a little bit for the moment, because we have pre-processed the file already. \n",
    "# If we had not done that and would like to load a raw data file directly from the scanner, we would\n",
    "# have to do:\n",
    "# acq_data = pMR.AcquisitionData(pname + fname)\n",
    "# acq_data = pMR.preprocess_acquisition_data(acq_data)\n",
    "# acq_data = pMR.set_grpe_trajectory(acq_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard (i.e. uncorrected) image reconstruction\n",
    "First we are going to carry out a standard MR image reconstruction and because we have done lot's of __MR__ notebooks already, we know the drill: calculate coil maps, set-up acquisition model do the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coil sensitivity maps\n",
    "csm = pMR.CoilSensitivityData()\n",
    "csm.smoothness = 100\n",
    "csm.calculate(acq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up acquisition model\n",
    "E = pMR.AcquisitionModel(acqs=acq_data, imgs=csm)\n",
    "E.set_coil_sensitivity_maps(csm)\n",
    "\n",
    "# Calculate the inverse\n",
    "rec_im = E.inverse(acq_data)\n",
    "im_inv_uncorr = rec_im.as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise result\n",
    "plot_rpe_3d([im_inv_uncorr,], [64, 64], ['Inverse',])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we have an image but it does not look particularly great. Well, we know that the `inverse` of undersampled data still shows undersampling artefacts, so let's use some iterative reconstruction method and see how we can improve this image. We are going to use _FISTA_ from __CIL__ to do the image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR AcquisitionModel\n",
    "E = pMR.AcquisitionModel(acqs=acq_data, imgs=rec_im)\n",
    "E.set_coil_sensitivity_maps(csm)\n",
    "\n",
    "# Starting image\n",
    "x_init = rec_im.clone()\n",
    "\n",
    "# Objective function\n",
    "f = LeastSquares(E, acq_data, c=1)\n",
    "G = ZeroFunction()\n",
    "\n",
    "# Set up FISTA for least squares\n",
    "fista = FISTA(initial=x_init, f=f, g=G)\n",
    "fista.max_iteration = 100\n",
    "fista.update_objective_interval = 5\n",
    "\n",
    "# Run FISTA\n",
    "fista.run(20, verbose=True)\n",
    "\n",
    "# Get the results\n",
    "im_fista_uncorr = fista.get_output().as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise result\n",
    "plot_rpe_3d([im_inv_uncorr, im_fista_uncorr], [64, 64], ['Inverse', 'FISTA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, again not great. This suggests that the undersampling is not really the problem but (you probably guessed it already from the title of the notebook): \n",
    "\n",
    "## Motion\n",
    "\n",
    "Motion is big topic in MRI, because it can have some unexpected effects, because it leads to a modulation of the acquired k-space. Commonly in imaging we think of motion artefacts mainly as blurring, but in MRI motion artefacts depend of course on the type of motion but also on the k-space sampling and the timing. Motion in MRI can lead to blurring but also ghosting (similar to Cartesian undersampling) or streaking or ....\n",
    "\n",
    "If you want to know more about motion and its effects in MRI have a look at this paper:\n",
    "\n",
    "Zaitsev M, Maclaren J, Herbst M. 2015 Motion artifacts in MRI: A complex problem with many partial solutions. J. Magn. Reson. Imaging 42, 887â€“901. (doi:10.1002/jmri.24850)\n",
    "\n",
    "Now back to our notebook. What does it mean for your `AcquisitionModel` if there is motion. Before we had for the `forward`:\n",
    "$$\n",
    "E x = y_c = \\mathcal{F}( C_c \\cdot x).\n",
    "$$\n",
    "Assuming we have $N_ms$ motion states occuring during our data acquisition and each motion state can be described by a transformation (warp operator) $W_i$ then the above equation can be extended to:\n",
    "$$\n",
    "E x = y_c = \\sum_i^{N_{ms}}\\mathcal{F_i}( C_c \\cdot (x \\circ W_i)).\n",
    "$$\n",
    "This means there is still one image $x$ (which we will call the reference image) without any motion artefacts, this gets then transformed to different motion states by $W_i$ and then the acquired k-space is then the some over all motion states. Note that the Fourier transform has also got an index $i$ because in each motion state different k-space points can be acquired.\n",
    "\n",
    "## Motion Correction\n",
    "\n",
    "There are now two basic approaches to reconstruct a motion-corrected image from k-space acquired in different motion states:\n",
    "\n",
    "### Reconstruct-transform-average (RTA)\n",
    "Here the idea is to reconstruct an image for each motion state, transform each image to the reference motion state and then average.\n",
    "\n",
    "### Motion-corrected image reconstruction (MCIR)\n",
    "For MCIR the encoding operator defined above is used to minimize a least-square-problem to directly reconstruct a motion-corrected image from k-space data acquired in different motion states.\n",
    "\n",
    "For more details please have a look at:\n",
    "\n",
    "Brown R et al. 2021 Motion estimation and correction for simultaneous PET/MR using SIRF and CIL. Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 379, 20200208. (doi:10.1098/rsta.2020.0208)\n",
    "\n",
    "### What do we need?\n",
    "For both RTA and MCIR we need \n",
    "   * a motion surrogate which tells us which k-space point has been acquired in which motion state\n",
    "   * motion vector fields ($W_i$) which describe how each voxel moves between the different motion states\n",
    "    \n",
    "Lucky for use, _GRPE_ is a sampling scheme which allows to obtain both directly from the motion corrupted scan. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion surrogate\n",
    "For _GRPE_ data is acquired along radial lines in the phase encoding plane. This means that the k-space centre is sampled repeatedly. If there is no motion, then the value of the k-space centre should always be the same. Hence, if the value changes between different _RPE_ lines, the change must be related to motion. So we can use the k-space centre to as a so-called self-navigator to find out about the different motion states. We will\n",
    "   * Find out which k-space points have been acquired at $k_x$ = $k_y$ = $k_z$ = 0\n",
    "   * Select one coil which shows a good motion signal\n",
    "   * Use this motion surrogate to calculate which k-space point has been acquired in which motion state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information about the index of the phase encoding\n",
    "pe_ky = acq_data.get_ISMRMRD_info('kspace_encode_step_1')\n",
    "\n",
    "# Find the central value of each GRPE line (i.e. ky=0)\n",
    "ky_idx = np.where(pe_ky == (np.max(pe_ky)+1)//2)\n",
    "\n",
    "# Get the k-space as array\n",
    "acq_data_arr = acq_data.as_array()\n",
    "\n",
    "# Keep only points which have been acquired in the k-space centre (i.e. ky == 0 & kz == 0)\n",
    "acq_data_arr = acq_data_arr[ky_idx[0], :, :]\n",
    "\n",
    "# Select the last coil and the centre of each readout (i.e kx = 0) to get our final 1D motion signal\n",
    "self_nav = np.abs(np.squeeze(acq_data_arr[:,3,acq_data_arr.shape[2]//2]))\n",
    "\n",
    "# We said above that the value of the k-space centre should only vary because of motion. \n",
    "# This is not entirely true, because it can also vary because of other effects. One is that the spin system\n",
    "# is in a transient steady state at the beginning of the data acquisition. \n",
    "# Therefore, we will simply overwrite the first entry in our motion signal\n",
    "self_nav[0] = self_nav[1]\n",
    "\n",
    "# Do some filtering\n",
    "self_nav = sp_signal.medfilt(self_nav, 7)\n",
    "\n",
    "# Interpolate self navigator to all phase encoding points\n",
    "self_nav = np.interp(np.linspace(0, len(pe_ky)-1, len(pe_ky)), ky_idx[0], self_nav)\n",
    "\n",
    "# Sort navigator and obtain index\n",
    "nav_idx = np.argsort(self_nav)\n",
    "\n",
    "# Bin data into Nms motion states each with the same amount of data\n",
    "Nms = 4\n",
    "num_pe_per_ms = np.ceil(len(pe_ky) / Nms).astype(np.int64)\n",
    "acq_idx_ms = []\n",
    "\n",
    "for nnd in range(Nms):\n",
    "    if nnd < Nms - 1:\n",
    "        acq_idx_ms.append(nav_idx[nnd*num_pe_per_ms:(nnd+1)*num_pe_per_ms])\n",
    "    else:\n",
    "        acq_idx_ms.append(nav_idx[nnd*num_pe_per_ms:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the navigator signal and color in the different motion states in different colors\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(self_nav, '-k')\n",
    "plt.xlabel('Phase encoding index')\n",
    "plt.ylabel('Signal at kx=ky=kz=0 (a.u.)')\n",
    "for ind in range(Nms):\n",
    "    print('Number of phase encoding points in motion state {}: {}'.format(ind, len(acq_idx_ms[ind])))\n",
    "    plt.plot(acq_idx_ms[ind], self_nav[acq_idx_ms[ind]], 'o')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Resolved Images\n",
    "Now we know which phase encoding point has been acquired in which motion state. So we can now create (in our case) 4 different sets of k-space, one for each motion state. Then we can reconstruct these to get an image for each motion state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each motion states, create corresponding k-space and acquisition model\n",
    "acq_ms = [0] * Nms\n",
    "E_ms = [0] * Nms\n",
    "\n",
    "\n",
    "for ind in range(Nms):\n",
    "    \n",
    "    acq_ms[ind] = acq_data.get_subset(acq_idx_ms[ind])\n",
    "    acq_ms[ind].sort_by_time()\n",
    "        \n",
    "    # Create acquisition model\n",
    "    E_tmp = pMR.AcquisitionModel(acqs=acq_ms[ind], imgs=csm)\n",
    "    E_tmp.set_coil_sensitivity_maps(csm)\n",
    "    im_ms = E_tmp.inverse(acq_ms[ind])\n",
    "\n",
    "    E_ms[ind] = pMR.AcquisitionModel(acqs=acq_ms[ind], imgs=im_ms)\n",
    "    E_ms[ind].set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reconstruct each motion state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_fista_ms = [0] * Nms\n",
    "\n",
    "for ind in range(Nms):\n",
    "\n",
    "    # Starting image\n",
    "    x_init = im_ms.clone()\n",
    "    x_init.fill(0.0)\n",
    "\n",
    "    # Objective function\n",
    "    f = LeastSquares(E_ms[ind], acq_ms[ind], c=1)\n",
    "    G = ZeroFunction()\n",
    "\n",
    "    # Set up FISTA for least squares\n",
    "    fista = FISTA(initial=x_init, f=f, g=G)\n",
    "    fista.max_iteration = 100\n",
    "    fista.update_objective_interval = 5\n",
    "\n",
    "    # Run FISTA\n",
    "    fista.run(10, verbose=True)\n",
    "    \n",
    "    # Get result\n",
    "    im_fista_ms[ind] = fista.get_output()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "im_fista_ms_arr = []\n",
    "im_fista_diff_arr = []\n",
    "for ind in range(Nms):\n",
    "    im_fista_ms_arr.append(im_fista_ms[ind].as_array())\n",
    "    im_fista_diff_arr.append(np.abs(im_fista_ms[ind].as_array()) - np.abs(im_fista_ms_arr[0]))\n",
    "    \n",
    "# Visualise different motion states\n",
    "plot_rpe_3d(im_fista_ms_arr, [64, 50], ['MS 0', 'MS 1', 'MS 2', 'MS 3'])\n",
    "\n",
    "# Visualise difference to first motion state\n",
    "plot_rpe_3d(im_fista_diff_arr, [64, 50], ['MS 0 - MS 0', 'MS 1 - MS 0', 'MS 2 - MS 0', 'MS 3 - MS 0'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Motion Vector fields\n",
    "Now we have got different images showing the phantom at the different motion states. The image quality is of course not great, because for each image we are only taking $1/4$ of the data from the scan, which itself is already undersampled by approximately a factor of 2. Nevertheless, for motion estimation they should be fine.\n",
    "\n",
    "To estimate the motion vector fields (i.e. a vector for each voxel describing how it's position changes because of the motion) we will use NiftyReg (http://cmictig.cs.ucl.ac.uk/wiki/index.php/NiftyReg) which is provided via the `pReg` module of __SIRF__. \n",
    "\n",
    "Image registration can only deal with real-valued images and so we will take the absolute value of each complex valued image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_fista_ms_abs = []\n",
    "for ind in range(Nms):\n",
    "    im_fista_ms_abs.append(im_fista_ms[ind].abs())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the registration, we will use the first image as our reference image and then estimate the motion vector fields which transform all other images to the first reference image. In this example we will only estimate an affine transformation using _NiftyAladinSym()_. This is fine for this simple phantom. For in-vivo application especially of the thorax a non-rigid motion estimation is often required. This can be done by using _NiftyF3dSym()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion fransformation object\n",
    "mf_resampler = [0] * Nms\n",
    "\n",
    "# Forward transformation (i.e. reference image transformed to current motion state)\n",
    "im_forward = [0] * Nms\n",
    "\n",
    "# Backward transformation (i.e. current motion image transformed to reference motion state)\n",
    "im_backward = [0] * Nms\n",
    "\n",
    "\n",
    "for ind in range(Nms):\n",
    "    # Non-rigid image registration\n",
    "    #algo = pReg.NiftyF3dSym()\n",
    "    \n",
    "    # Affine image registration\n",
    "    algo = pReg.NiftyAladinSym()\n",
    "\n",
    "    # Set up images\n",
    "    algo.set_reference_image(pReg.NiftiImageData3D(im_fista_ms_abs[ind])) # remove NiftiImageData3D?????\n",
    "    algo.set_floating_image(pReg.NiftiImageData3D(im_fista_ms_abs[0]))\n",
    "\n",
    "    # Run registration    \n",
    "    algo.process()\n",
    "\n",
    "    # Get forward deformation \n",
    "    mf_forward = algo.get_deformation_field_forward()\n",
    "\n",
    "    # Create resampler\n",
    "    mf_resampler[ind] = pReg.NiftyResample()\n",
    "    mf_resampler[ind].set_reference_image(im_fista_ms[ind])\n",
    "    mf_resampler[ind].set_floating_image(im_fista_ms[ind])\n",
    "    mf_resampler[ind].add_transformation(mf_forward)\n",
    "    mf_resampler[ind].set_padding_value(0)\n",
    "    mf_resampler[ind].set_interpolation_type_to_linear()\n",
    "\n",
    "    im_forward[ind] = mf_resampler[ind].forward(im_fista_ms[0])\n",
    "    im_backward[ind] = mf_resampler[ind].backward(im_fista_ms[ind])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the backward transformed images (i.e. the images transformed to the reference motion state). If our registration worked, then the difference to the reference motion state should be very small now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "im_backward_arr = []\n",
    "im_backward_diff_arr = []\n",
    "for ind in range(Nms):\n",
    "    im_backward_arr.append(im_backward[ind].as_array())\n",
    "    im_backward_diff_arr.append(np.abs(im_backward[ind].as_array()) - np.abs(im_backward[0].as_array()))\n",
    "    \n",
    "# Visualise different motion states transformed to reference motion state\n",
    "plot_rpe_3d(im_backward_arr, [64, 50], ['MS 0', 'MS 1', 'MS 2', 'MS 3'])\n",
    "\n",
    "# Visualise difference to first motion state\n",
    "plot_rpe_3d(im_backward_diff_arr, [64, 50], ['MS 0 - MS 0', 'MS 1 - MS 0', 'MS 2 - MS 0', 'MS 3 - MS 0'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTA\n",
    "We have got the images of the different motion states and we have got transformation to transform everything to a single reference motion state. This is all we need for the RTA method, where we simply apply the backward transform to the image of each motion state and then sum over all motion states. Applying the backward transform we have already done above (`im_backward`) so we only need to sum now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTA\n",
    "im_rta = im_backward[0]\n",
    "for ind in range(1,Nms):\n",
    "    im_rta += im_backward[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the uncorrected and the RTA reconstruction\n",
    "plot_rpe_3d([im_fista_uncorr, im_rta.as_array()], [64, 50], ['FISTA uncorr', 'RTA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCIR\n",
    "This already looks much better than the uncorrected image. But we can still do better. Rather than reconstructing images of the different motion states with lots of undersampling artefacts, we want to be able to use all the acquired data in the reconstruction and still obtain an image without motion artefacts. \n",
    "\n",
    "For this we use MCIR via the `BlockOperator` approach from __CIL__. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine AcquisitionModel and motion transformation\n",
    "C = [CompositionOperator(am, res) for am, res in zip(*(E_ms, mf_resampler))]\n",
    "A = BlockOperator(*C)\n",
    "\n",
    "# Put together all the raw k-space data for each motion state in a BlockDataContainer\n",
    "acq_ms_block = BlockDataContainer(*acq_ms)\n",
    "\n",
    "# Starting image\n",
    "x_init = A.adjoint(acq_ms_block)\n",
    "x_init.fill(0.0)\n",
    "\n",
    "# Objective function\n",
    "f = LeastSquares(A, acq_ms_block, c=1)\n",
    "f.L = 8000.0\n",
    "G = ZeroFunction()\n",
    "\n",
    "# Set up FISTA for least squares\n",
    "fista = FISTA(initial=x_init, f=f, g=G)\n",
    "fista.max_iteration = 100\n",
    "fista.update_objective_interval = 10\n",
    "\n",
    "# Run FISTA\n",
    "fista.run(20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result\n",
    "im_fista_mcir = fista.get_output().as_array()\n",
    "\n",
    "# Compare to the uncorrected reconstruction and the RTA\n",
    "plot_rpe_3d([im_fista_uncorr, im_rta.as_array(), im_fista_mcir], [64, 64], ['Uncorr', 'RTA', 'MCIR'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some artefacts (probably our estimated motion transformations are not that great, because we used some quite low quality motion-resolved images) but we can see an improvement from _RTA_ to _MCIR_. The downside of course is, that the image reconstruction is computationally more demanding but luckily enough this is usually not too much of a problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "There are lots of things to try out now and play around with:\n",
    " * Compare the surrogate signal obtained from different coil elements\n",
    " * Try out different number of motion states\n",
    " * Different number of FISTA iterations for the motion-resolved images\n",
    " * Add additional regularisation to FISTA for the motion-resolved images\n",
    " * ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
