{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PET deconvolution data setup notebook\n",
    "\n",
    "In this notebook we'll simulate some 3D FDG brain PET acquisitions. \\\n",
    "1. Create FDG, uMap, T1 images of the brain as Numpy arrays - create SIRF `ImageData` objects using these\n",
    "2. Create a `BlurringOperator` using CIL and an `AcquisitionModel` using SIRF. Use CIL's `CompositionOperator` to create a blurred forward model\n",
    "3. Create simulated PET data using this blurred forward model\n",
    "4. Using SIRF's `OSMAPOSLReconstructor` class, produce an OSEM-reoncstructed image that will be both blurred and noisy when compared to the ground truth image\n",
    "5. Simulate a point source and use the blurred forward model to simulate the result of taking a point source measurement with our PET scanner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't going to be a huge amount of explanation in this notebook as we assume that you will all be somewhat familiar with the Synergistic Image Reconstruction Framework [(SIRF)](https://github.com/SyneRBI/SIRF). We use some of the functionality of the Core Imaging Library [(CIL)](https://github.com/TomographicImaging/CIL) but this should be relatively self-explanatory and limited. We'll use this a little more in the next notebook and will provide some extra information then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sirf.STIR as pet\n",
    "from sirf.Utilities import examples_data_path\n",
    "import brainweb\n",
    "\n",
    "from cil.utilities.display import show2D # cil.utilities is a module that contains some functions to display images\n",
    "from cil.optimisation.operators import  BlurringOperator, CompositionOperator # cil.optimisation operators to blur the image and to compose multiple operators\n",
    "\n",
    "msg = pet.MessageRedirector() # suppress STIR messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to ensure the same data is used for each run\n",
    "noise_seed = 5\n",
    "noise_level = 1\n",
    "bw_seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \"data\")\n",
    "data = brainweb.load_file(os.path.join(\"data\", fname))\n",
    "\n",
    "brainweb.seed(bw_seed)\n",
    "\n",
    "vol = brainweb.get_mmr_fromfile(os.path.join(\"data\", fname),\n",
    "        petNoise=1, t1Noise=0.75, t2Noise=0.75,\n",
    "        petSigma=1, t1Sigma=1, t2Sigma=1)\n",
    "\n",
    "# We'll extract the ground truth FDG PET, T1 MRI and T2 MRI images as well as the attenuation image (uMap)\n",
    "arr_dict = {'PET': vol['PET'], 'T1': vol['T1'], 'T2': vol['T2'], 'uMap': vol['uMap']}\n",
    "\n",
    "# desired shape of the images - note that changing these values requires updating the template sinogram header file (data/template_sinogram.hs)\n",
    "crop_dim = (8,102,102)\n",
    "\n",
    "def crop_array(arr, crop_dim):\n",
    "    \"\"\"\n",
    "    Crop the array to the desired dimensions\n",
    "    \"\"\"\n",
    "    y_start, x_start = (arr.shape[1] - crop_dim[1]) // 2, (arr.shape[2] - crop_dim[2]) // 2\n",
    "    y_end, x_end = y_start + crop_dim[1], x_start + crop_dim[2]\n",
    "    cropped_array = arr[:, y_start:y_end, x_start:x_end]\n",
    "\n",
    "    # Step 2: Select slices in a stride-based manner to downsample to [15, 128, 128]\n",
    "    num_slices = cropped_array.shape[0]\n",
    "    stride = num_slices // crop_dim[0]\n",
    "\n",
    "    # Ensure that we get exactly 15 slices\n",
    "    selected_indices = np.linspace(0, num_slices - stride, crop_dim[0], dtype=int)\n",
    "    downsampled_array = cropped_array[selected_indices, :, :]\n",
    "\n",
    "    return downsampled_array\n",
    "\n",
    "# Step 1: Crop the images to [15, 128, 128]\n",
    "for key, image in arr_dict.items():\n",
    "    arr_dict[key] = crop_array(image, crop_dim)\n",
    "\n",
    "show2D([arr_dict['PET'], arr_dict['uMap'], arr_dict['T1'], arr_dict['T2']], title = ['Ground Truth', 'uMap', 'T1 weighted MRI', 'T2 weighted MRI'], origin = 'upper', num_cols = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sirf.STIR import *\n",
    "from sirf.Utilities import examples_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the SIRF ImageData objects and then initialise them\n",
    "image_dict = {}\n",
    "vsize = (6.75, 2.2, 2.2) # voxel sizes in mm\n",
    "for key, image in arr_dict.items():\n",
    "    image_dict[key] = pet.ImageData()\n",
    "    image_dict[key].initialise(dim = crop_dim, vsize = vsize)\n",
    "    image_dict[key].fill(image)\n",
    "    image_dict[key].write(os.path.join('data',f'{key}_b{bw_seed}.hv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition Model\n",
    "\n",
    "Now let's set up our acquisition model. The acquisition process will both blur and add noise to the ground truth tracer distribution. Whilst there is functionality to do both of these in SIRF, we'll do the blurring using the CIL `BlurringOperator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a couple of functions to create a 3D Gaussian kernel\n",
    "\n",
    "def fwhm_to_sigma(fwhm):\n",
    "    return fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
    "\n",
    "def psf(n, fwhm, voxel_size=(1, 1, 1)):\n",
    "    \"\"\" Creates a 3D point spread function (PSF) with specified sizes `n`, FWHM values `fwhm`, and voxel sizes `voxel_size` \"\"\"\n",
    "\n",
    "    sigma_voxels = [fwhm_to_sigma(fwhm[i]) / voxel_size[i] for i in range(3)]\n",
    "    \n",
    "    # Create Gaussian distributions for each dimension\n",
    "    axes = [np.linspace(-(n - 1) / 2., (n - 1) / 2., n) for i in range(3)]\n",
    "    gauss = [np.exp(-0.5 * np.square(ax) / np.square(sigma_voxels[i])) for i, ax in enumerate(axes)]\n",
    "\n",
    "    # Create 3D Gaussian kernel\n",
    "    kernel_3d = np.outer(gauss[0], gauss[1]).reshape(n, n, 1) * gauss[2].reshape(1, 1, n)\n",
    "    \n",
    "    # Normalize the kernel to ensure its sum equals one\n",
    "    return kernel_3d / np.sum(kernel_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then let's create a factory to create SIRF acquisition model object\n",
    "\n",
    "def make_acquisition_model(template_sinogram, template_image, atten_image):\n",
    "\n",
    "    # We'll start by initialising the acquisition model object\n",
    "    acq_model = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "    acq_model.set_num_tangential_LORs(10) \n",
    "\n",
    "    # In order to create LOR attenuation coefficients, we need to project the attenuation image into sinogram space\n",
    "    acq_asm = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "    acq_asm.set_num_tangential_LORs(10)\n",
    "    acq_model.set_acquisition_sensitivity(pet.AcquisitionSensitivityModel(atten_image, acq_asm))\n",
    "\n",
    "    # And finally, we can set up the acquisition model\n",
    "    acq_model.set_up(template_sinogram,template_image)\n",
    "\n",
    "    return acq_model\n",
    "\n",
    "# and then add POISSON noise to simulated acquisition data\n",
    "\n",
    "def add_poission_noise(acquistion_data, noise_level=10, seed=10):\n",
    "    \"\"\"\n",
    "    Adds poisson noise to acquisition data\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    noisy_data = np.random.poisson(acquistion_data.as_array()/noise_level)*noise_level\n",
    "    return acquistion_data.clone().fill(noisy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cil.framework.framework as cil\n",
    "\n",
    "# Now we can create the acquisition model object\n",
    "acq_model = make_acquisition_model(pet.AcquisitionData(os.path.join(examples_data_path('PET'), 'brain', 'template_sinogram.hs')), image_dict['PET'], image_dict['uMap'])\n",
    "\n",
    "# We'll now create a noiseless sinogram for comparison\n",
    "sinogram = acq_model.direct(image_dict['PET'])\n",
    "\n",
    "# We'll create a 3D Gaussian kernel with a FWHM of 7 mm in each dimension\n",
    "PSF=psf(15, fwhm=(7,7,7), voxel_size=vsize)\n",
    "\n",
    "# And then we'll create a BlurringOperator to convolve the ground truth PET image with the PSF\n",
    "convolve=BlurringOperator(PSF, image_dict['PET'])\n",
    "\n",
    "# And now we'll model the forward operator including the point spread function\n",
    "blurred_acq_model=CompositionOperator(acq_model, convolve)\n",
    "\n",
    "# We'll now create a blurred sinogram and add Poisson noise\n",
    "blurred_sinogram=blurred_acq_model.direct(image_dict['PET'])\n",
    "blurred_noisy_sinogram=add_poission_noise(blurred_sinogram, noise_level=noise_level, seed=noise_seed)\n",
    "\n",
    "sinogram.write(f'data/bw_sinogram_b{bw_seed}.hs')\n",
    "blurred_noisy_sinogram.write(f'data/bw_blurred_noisy_sinogram_b_{bw_seed}_n{noise_seed}.hs')\n",
    "\n",
    "show2D([sinogram, blurred_noisy_sinogram], title = ['sinogram', 'Noisy blurred sinogram'], origin = 'upper', num_cols = 2, fix_range=[(0,1400), (0,1400)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some reconstructions. The aim of this exercise is not to worry about the PSF during reconstruction and then try and correct for it later so we'll just use the SIRF acquisiton model in our reconstruction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our objective function and reconstructor objects\n",
    "objective_function = pet.make_Poisson_loglikelihood(blurred_noisy_sinogram, acq_model=acq_model) # PLL because we have Poisson noise\n",
    "objective_function.set_num_subsets(8)\n",
    "reconstructor = pet.OSMAPOSLReconstructor() # Ordered Subset Maximum A Posteriori One Step Late (OSMAPOSL) algorithm - we don't need to worry about the OSL part becuse we have no prior\n",
    "reconstructor.set_num_subiterations(8)\n",
    "reconstructor.set_objective_function(objective_function)\n",
    "reconstructor.set_up(image_dict['PET'])\n",
    "\n",
    "# create a processor to truncate the image to the cylinder\n",
    "# This prevents edge effects from ruining the reconstruction\n",
    "cyl = pet.TruncateToCylinderProcessor()\n",
    "cyl.set_strictly_less_than_radius(True)\n",
    "\n",
    "current_estimate = image_dict['PET'].get_uniform_copy(1) # initial estimate is a uniform image\n",
    "cyl.apply(current_estimate) # truncated to a PET FOV\n",
    "objective_list = []\n",
    "full_iterations = 12 # number of full iterations to run\n",
    "\n",
    "for i in range(full_iterations):\n",
    "    reconstructor.reconstruct(current_estimate)\n",
    "    objective_list.append(-objective_function(current_estimate))\n",
    "    print(f\"Iteration: {i}, Objective: {objective_list[-1]}\", end = '\\r')\n",
    "    # remove any weird stuff from edge effects\n",
    "    cyl.apply(current_estimate)\n",
    "\n",
    "current_estimate.write(f'data/OSEM_b{bw_seed}_n{noise_seed}.hv')\n",
    "\n",
    "show2D([current_estimate, image_dict['PET']], \n",
    "       title = ['reconstructed image', 'ground truth'], \n",
    "       origin = 'upper', num_cols = 2, fix_range=[(0,160), (0,160)])\n",
    "\n",
    "plt.plot(objective_list)   \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective Function Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this a little more realistic, we'll simulate a point source mesurement, reconstructed with the same parameters as above. We can then use this reconstruction to estimate the PSF of our \"scanner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_source = image_dict['PET'].get_uniform_copy(0)\n",
    "point_source_arr = point_source.as_array()\n",
    "\n",
    "point_source_arr[point_source_arr.shape[0]//2, point_source_arr.shape[1]//2, point_source_arr.shape[2]//2] = 1000\n",
    "point_source.fill(point_source_arr)\n",
    "\n",
    "point_source_sinogram = blurred_acq_model.direct(point_source)\n",
    "point_source_sinogram = add_poission_noise(point_source_sinogram, noise_level=noise_level, seed=noise_seed)\n",
    "\n",
    "objective_function = pet.make_Poisson_loglikelihood(point_source_sinogram, acq_model=acq_model)\n",
    "objective_function.set_num_subsets(8)\n",
    "reconstructor = pet.OSMAPOSLReconstructor()\n",
    "reconstructor.set_num_subiterations(8)\n",
    "reconstructor.set_objective_function(objective_function)\n",
    "reconstructor.set_up(image_dict['PET'])\n",
    "\n",
    "current_ps_estimate = image_dict['PET'].get_uniform_copy(1)\n",
    "cyl.apply(current_ps_estimate)\n",
    "objective_list = []\n",
    "full_iterations = 12\n",
    "\n",
    "for i in range(full_iterations):\n",
    "    reconstructor.reconstruct(current_ps_estimate)\n",
    "    objective_list.append(-objective_function(current_ps_estimate))\n",
    "    print(f\"Iteration: {i}, Objective: {objective_list[-1]}\", end = '\\r')\n",
    "    # remove any weird stuff from edge effects\n",
    "    cyl.apply(current_ps_estimate)\n",
    "\n",
    "current_ps_estimate.write(os.path.join('data', f'OSEM_psf_n{noise_seed}.hv'))\n",
    "\n",
    "show2D(reconstructor.get_current_estimate(), \n",
    "       title = 'reconstructed point source', \n",
    "       origin = 'upper', num_cols = 1)\n",
    "\n",
    "plt.plot(objective_list)   \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective Function Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all files left over that should have been delted\n",
    "# these start with tmp_ and end with .hs or .s\n",
    "for file in os.listdir('.'):\n",
    "    if file.startswith('tmp_') and (file.endswith('.hs') or file.endswith('.s')):\n",
    "        os.remove(f'{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Brainweb\n",
    "C.A. Cocosco, V. Kollokian, R.K.-S. Kwan, A.C. Evans : \"BrainWeb: Online Interface to a 3D MRI Simulated Brain Database\"\n",
    "NeuroImage, vol.5, no.4, part 2/4, S425, 1997 -- Proceedings of 3-rd International Conference on Functional Mapping of the Human Brain, Copenhagen, May 1997.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
