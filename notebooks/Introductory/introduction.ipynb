{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of basic image manipulation with SIRF/CIL\n",
    "This demonstration shows how to create image data objects for MR, CT and PET and how to work with them.\n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Kris Thielemans, Richard Brown, Christoph Kolbitsch  \n",
    "First version: 8th of September 2016  \n",
    "Second Version: 17th of May 2018  \n",
    "Third Version: 23rd of October 2019  \n",
    "Fourth Version: 23rd of April 2021  \n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2015 - 2017 Rutherford Appleton Laboratory STFC.  \n",
    "Copyright 2015 - 2019, 2021 University College London.  \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Synergistic Reconstruction for Biomedical Imaging\n",
    "(http://www.ccpsynerbi.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "# Edit this to \"\"%matplotlib notebook\" when using the \"classic\" jupyter notebook interface\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have placed a file in this directory, notebook_setup.py, which will allow us to import the sirf_exercises library\n",
    "import notebook_setup\n",
    "\n",
    "# The sirf_exercises defines some handy tools for these notebooks\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "\n",
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sirf.Utilities import examples_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure that your installation knows where to read and write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later scripts will first have to download data. In addition, the SIRF exercises are set-up to write output in a separate \"working directory\" to avoid cluttering/overwriting your SIRF files. We need to tell Python where that will be. To do that, you have to run the `download_data.sh` script. You can do that from a terminal, or from this notebook.\n",
    "\n",
    "The following cell will run the script to simply print a usage message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ../../scripts/download_data.sh -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the script again. The line below will actually not download anything (see further notebooks) but configure the destination directory, which is also used for the \"working directory\" set-up.\n",
    "\n",
    "Note that you might want to use the `-d` option to write files somewhere else than the default location. (If you're running this as part of a training session, follow the advice given by your instructors of course!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash ../../scripts/download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move to a working directory for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_to_working_dir('Introductory', 'introduction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check where we are by using the ipython \"magic\" command to print the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define some handy function definitions to make subsequent code cleaner. You can ignore them when you first see this demo.\n",
    "They have (minimal) documentation using Python docstrings such that you can do for instance `help(plot_2d_image)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink=.6)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim = templ_im.as_array().shape\n",
    "    # Let's make sure everything is centered. \n",
    "    # Because offset is used to index an array it has to be of type integer, so we do an integer division using '//'\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.clone()\n",
    "    \n",
    "    # Fill image content\n",
    "    templ_im_out.fill(numpy.reshape(vol, idim))\n",
    "    return(templ_im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SIRF and CIL have their own `show*` functions which will be used on other demos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use Brainweb data, which is made more convenient by using the Python brainweb module. We will use a FDG image for PET. MR usually provides qualitative images with an image contrast proportional to difference in T1, T2 or T2* depending on the sequence parameters. Nevertheless, we will make our life easy, by directly using the T1 map provided by the brainweb for MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDG_arr  = vol['PET']\n",
    "T1_arr   = vol['T1']\n",
    "uMap_arr = vol['uMap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convention for the image dimensions in the brainweb images is [z, y, x]. If we want to\n",
    "display the central slice (i.e. z), we therefore have to use the 0th dimension of the array.\n",
    "We are using an integer division using '//' to ensure we can use the value to index the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "\n",
    "slice_show = FDG_arr.shape[0]//2\n",
    "\n",
    "# The images are very large, so we only want to visualise the central part of the image. In Python this can be \n",
    "# achieved by using e.g. 100:-100 as indices. This will \"crop\" the first 100 and last 100 voxels of the array. \n",
    "plot_2d_image([1,3,1], FDG_arr[slice_show, 100:-100, 100:-100], 'FDG', cmap=\"hot\")\n",
    "plot_2d_image([1,3,2], T1_arr[slice_show, 100:-100, 100:-100], 'T1', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,3,3], uMap_arr[slice_show, 100:-100, 100:-100], 'uMap', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than likely, this image came out a bit small for your set-up. You can check the default image size as follows (note: units are inches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then change them to a size more suitable for your situation, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[10,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the cell above that plots the images again to see if that helped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make this change permanent by changing your `matplotlibrc` file (this might be non-trivial when running on Docker or JupyterHub instance!). You will need to search for `figure.figsize` in that file. Its location can be found as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.matplotlib_fname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIRF/CIL ImageData based on Brainweb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create an __MR__, __PET__ or __CT__ `ImageData` object, we need some information about the modality, the hardware used for scanning and the to some extent also the acquisition and reconstruction process. Most of this information is contained in the raw data files which can be exported from the __MR__ and __PET__ scanners. For __CT__ the parameters can be defined manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will now go through each modality separately and show how a simple `ImageData` object can be created. In the last part of the notebook we will then show examples about how to display the image data with python or how to manipulate the image data (e.g. multiply it with a constant or calculate its norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make our life easier, we will assume that the voxel size and image orientation for __MR__, __PET__ and __CT__ are all the same and they are the same as the brainweb data. This is of course not true, real-life applications and/or synergistic image reconstruction we would need to resample the brainweb images before using them as input to the `ImageData` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the 'mr' prefix for all Gadgetron-based SIRF functions.\n",
    "This is done here to explicitly differentiate between SIRF mr functions and \n",
    "anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sirf.Gadgetron as mr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a template MR acquisition data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templ_mr = mr.AcquisitionData(os.path.join(examples_data_path('MR'), 'simulated_MR_2D_cartesian.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MR the dimensions of the image data depend of course on the data acquisition but they are also influenced by the reconstruction process. Therefore, we need to carry out an example reconstruction, in order to have all the information about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reconstruction\n",
    "preprocessed_data = mr.preprocess_acquisition_data(templ_mr)\n",
    "recon = mr.FullySampledReconstructor()\n",
    "recon.set_input(preprocessed_data)\n",
    "recon.process()\n",
    "im_mr = recon.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above failed with an error 'Server running Gadgetron not accessible', you probably still have to start a Gadgetron server. Check the [DocForParticipants](https://github.com/SyneRBI/SIRF-Exercises/blob/master/DocForParticipants.md#start-a-Gadgetron-server)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got an MR image object and can fill it with the brainweb data. The dimensions won't fit, but we will simply crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_mr = crop_and_fill(im_mr, T1_arr)\n",
    "\n",
    "# im_mr is an MR image object. In order to visualise it we need access to the underlying data array. This is\n",
    "# provided by the function as_array(). This yields a numpy array which can then be easily displayed. More \n",
    "# information on this is also provided at the end of the notebook.\n",
    "plt.figure();\n",
    "plot_2d_image([1,1,1], numpy.abs(im_mr.as_array())[im_mr.as_array().shape[0]//2, :, :], 'MR', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the 'ct' prefix for all CIL-based functions.\n",
    "This is done here to explicitly differentiate between CIL ct functions and \n",
    "anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cil.framework as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a template Cone Beam CT acquisition geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 120\n",
    "angles = numpy.linspace(0, 360, 50, True, dtype=numpy.float32)\n",
    "offset = 0.4\n",
    "channels = 1\n",
    "ag = ct.AcquisitionGeometry.create_Cone3D((offset,-100, 0), (offset,100,0))\n",
    "ag.set_panel((N,N-2))\n",
    "ag.set_channels(channels)\n",
    "ag.set_angles(angles, angle_unit=ct.AcquisitionGeometry.DEGREE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a template CT image object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = ag.get_ImageGeometry()\n",
    "im_ct = ig.allocate(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got an CT image object and can fill it with the brainweb data. The dimensions won't fit, but we will simply crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_ct = crop_and_fill(im_ct, uMap_arr)\n",
    "\n",
    "plt.figure();\n",
    "plot_2d_image([1,1,1], im_ct.as_array()[im_ct.as_array().shape[0]//2, :, :], 'CT', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the 'pet' prefix for all STIR-based SIRF functions.\n",
    "This is done here to explicitly differentiate between SIRF pet functions and \n",
    "anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sirf.STIR as pet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a template sinogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templ_sino = pet.AcquisitionData(os.path.join(examples_data_path('PET'), 'mMR','mMR_template_span11.hs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a template PET image object that would fit dimensions for that sinogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pet = pet.ImageData(templ_sino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got a PET image object and can fill it with the brainweb data. The dimensions won't fit, but we will simply crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pet = crop_and_fill(im_pet, FDG_arr)\n",
    "\n",
    "plt.figure();\n",
    "plot_2d_image([1,1,1], im_pet.as_array()[im_pet.as_array().shape[0]//2, :, :], 'PET', cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic image manipulations\n",
    "Images (like most other things in SIRF and CIL) are represented as *objects*, in this case of type `ImageData`.\n",
    "In practice, this means that you can only manipulate its data via *methods*.\n",
    "\n",
    "Image objects contain the actual voxel values, but also information on the number of voxels,\n",
    "voxel size, etc. There are methods to get this information.\n",
    "\n",
    "There are additional methods for other manipulations, such as basic image arithmetic (e.g.,\n",
    "you can add image objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we created an `ImageData` object for each modality we can now simply select which modality we want to look at. Because SIRF is implemented to make the transition from one modality to the next very easy, many of the *methods* and *attributes* are exactly the same between __MR__, __PET__ or __CT__ . There are of course *methods* and *attributes* which are modality-specific but the basic handling of the `ImageData` objects is very similar between __MR__, __PET__ or __CT__ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the image of a specific modality\n",
    "image_data_object = im_ct.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an ImageData?\n",
    "\n",
    "Images are represented by objects with several methods. The most important method \n",
    "is `as_array()` which we'll use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what all the methods are.\n",
    "help(pet.ImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use as_array to extract an array of voxel values\n",
    "# The resulting array as a `numpy` array, as standard in Python.\n",
    "image_array=image_data_object.as_array()\n",
    "\n",
    "# We can use the standard `numpy` methods on this array, such as getting its `shape` (i.e. dimensions).\n",
    "print(image_array.shape)\n",
    "\n",
    "# Whenever we want to do something with the image-values, we have to do it via this array.\n",
    "# Let's print a voxel-value roughly in the centre of the object. \n",
    "# We will not use the centre because the intensity here happens to be 0.\n",
    "centre = numpy.array(image_array.shape)//2\n",
    "print(image_array[centre[0], centre[1]+20, centre[2]+20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate the image data for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the data with a factor\n",
    "image_array *= 0.01\n",
    "\n",
    "# Stick this new data into the original image object.\n",
    "# (This will not modify the file content, only the variable in memory.)\n",
    "image_data_object.fill(image_array)\n",
    "print(image_array[centre[0], centre[1]+20, centre[2]+20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do basic math manipulations with ImageData objects \n",
    "So the above lines can be done directly on the `image` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_object *= 0.01\n",
    "\n",
    "# Let's check\n",
    "image_array=image_data_object.as_array()\n",
    "print(image_array[centre[0], centre[1]+20, centre[2]+20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the middle slice of the image (which is really a 3D volume)\n",
    "We will use our own `plot_2d_image` function (which was defined above) for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure\n",
    "plt.figure()\n",
    "\n",
    "# Display the slice (numpy.absolute is only necessary for MR but doesn't matter for PET or CT)\n",
    "plot_2d_image([1,1,1], numpy.absolute(image_array[centre[0], :, :]), 'image data', cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other things to do with ImageData objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_data_object.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_image=image_data_object*3+8.3\n",
    "and_another=another_image+image_data_object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
