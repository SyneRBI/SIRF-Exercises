{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Models for MR, PET and CT\n",
    "This demonstration shows how to set-up and use SIRF/CIL acquisition models for different modalities. You should have tried the `introduction` notebook first. The current notebook briefly repeats some items without explanation.\n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Christoph Kolbitsch, Edoardo Pasca, Kris Thielemans\n",
    "\n",
    "First version: 23rd of April 2021  \n",
    "\n",
    "CCP SyneRBI Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2015 - 2017 Rutherford Appleton Laboratory STFC.  \n",
    "Copyright 2015 - 2019, 2021 University College London.   \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Synergistic Reconstruction for Biomedical Imaging\n",
    "(http://www.ccpsynerbi.ac.uk/).\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib widget\n",
    "import notebook_setup\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "cd_to_working_dir('Introductory', 'acquisition_model_mr_pet_ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import MR, PET and CT functionality\n",
    "import sirf.Gadgetron as mr\n",
    "import sirf.STIR as pet\n",
    "import cil.framework as ct\n",
    "\n",
    "from sirf.Utilities import examples_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if your installation comes with the ASTRA plugin. If not, we won't be able to illustrate CT forward projections via CIL unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from cil.plugins.astra.operators import ProjectionOperator as ap\n",
    "    have_astra = True\n",
    "except:\n",
    "    have_astra = False\n",
    "    print(\"CIL ASTRA plugin is not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define some handy function definitions to make subsequent code cleaner. You can ignore them when you first see this demo.\n",
    "They have (minimal) documentation using Python docstrings such that you can do for instance `help(plot_2d_image)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink=.5)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    \n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb. We will use a FDG image for PET and the PET uMAP for CT. MR usually provides qualitative images with an image contrast proportional to difference in T1, T2 or T2* depending on the sequence parameters. Nevertheless, we will make our life easy, by directly using the T1 map provided by the brainweb for MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDG_arr  = vol['PET']\n",
    "T1_arr   = vol['T1']\n",
    "uMap_arr = vol['uMap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = FDG_arr.shape[0]//2\n",
    "plot_2d_image([1,3,1], FDG_arr[slice_show, 100:-100, 100:-100], 'FDG', cmap=\"hot\")\n",
    "plot_2d_image([1,3,2], T1_arr[slice_show, 100:-100, 100:-100], 'T1', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,3,3], uMap_arr[slice_show, 100:-100, 100:-100], 'uMap', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SIRF and CIL, an `AcquisitionModel` basically contains everything we need to know in order to describe what happens when we go from the imaged object to the acquired raw data (`AcquisitionData`) and then to the reconstructed image (`ImageData`). What we actually need to know depends strongly on the modality we are looking at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of modality specific information:  \n",
    "\n",
    "  * __PET__: scanner geometry, detector efficiency, attenuation, randoms/scatter background...  \n",
    "  * __CT__: scanner geometry  \n",
    "  * __MR__: k-space sampling pattern, coil sensitivity information,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then there is information which is independent of the modality such as field-of-view or image discretisation (e.g. voxel sizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For __PET__ and __MR__ a lot of this information is already in the raw data. Because it would be quite a lot of work to enter all the necessary information by hand and then checking it is consistent, we create `AcquisitionModel` objects from `AcquisitionData` objects. The `AcquisitionData` only serves as a template and both its actual image and raw data content can be (and in this exercise will be) replaced. For __CT__ we will create an acquisition model from scratch, i.e. we will define the scanner geometry, image dimensions and image voxels sizes and so by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's get started with __MR__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MR we basically need the following:\n",
    "\n",
    "  1. create an MR `AcquisitionData` object from a raw data file\n",
    "  2. calculate the coil sensitivity maps (csm, for more information on that please see the notebook `MR/c_coil_combination.ipynb`). \n",
    "  3. then we will carry out a simple image reconstruction to get a `ImageData` object which we can use as a template for our `AcquisitionModel`\n",
    "  4. then we will set up the MR `AcquisitionModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create MR AcquisitionData\n",
    "mr_acq = mr.AcquisitionData(os.path.join(examples_data_path('MR'),'grappa2_1rep.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. calculate CSM\n",
    "preprocessed_data = mr.preprocess_acquisition_data(mr_acq)\n",
    "\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 50\n",
    "csm.calculate(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. calculate image template\n",
    "recon = mr.FullySampledReconstructor()\n",
    "recon.set_input(preprocessed_data)\n",
    "recon.process()\n",
    "im_mr = recon.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create AcquisitionModel\n",
    "acq_mod_mr = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "\n",
    "# Supply csm to the acquisition model \n",
    "acq_mod_mr.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PET we need to:\n",
    "\n",
    "   1. create a PET `AcquisitionData` object from a raw data file\n",
    "   2. create a PET `ImageData` object from the PET `AcquisitionData`\n",
    "   3. then we will set up the PET `AcquisitionModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create PET AcquisitionData\n",
    "templ_sino = pet.AcquisitionData(os.path.join(examples_data_path('PET'),\"thorax_single_slice\",\"template_sinogram.hs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create a template PET ImageData\n",
    "im_pet = pet.ImageData(templ_sino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create AcquisitionModel\n",
    "\n",
    "# create PET acquisition model\n",
    "acq_mod_pet = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_mod_pet.set_up(templ_sino, im_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CT we need to:\n",
    "\n",
    "   1. create a CT `AcquisitionGeometry` object\n",
    "   2. obtain CT `ImageGeometry` from `AcquisitionGeometry`\n",
    "   3. create a CT `ImageData` object\n",
    "   4. then we will set up the CT `AcquisitionModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define AcquisitionGeometry\n",
    "angles = numpy.linspace(0, 360, 50, True, dtype=numpy.float32)\n",
    "ag2d = ct.AcquisitionGeometry.create_Cone2D((0,-1000), (0, 500))\\\n",
    "          .set_panel(128,pixel_size=3.104)\\\n",
    "          .set_angles(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get ImageGeometry\n",
    "ct_ig = ag2d.get_ImageGeometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create ImageData\n",
    "im_ct = ct_ig.allocate(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create AcquisitionModel if we have ASTRA\n",
    "if have_astra:\n",
    "    acq_mod_ct = ap(ct_ig, ag2d, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply acquisition models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to apply our acquisition models in order to create raw data, we first need some image data. Because this image data has to fit to the `ImageData` and `AcquisitionData` objects of the different modalities, we will use them to create our image data. For more information on that please have a look at the notebook _introductory/introduction.ipynb_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an `ImageData` object for each modality and display the slice in the centre of each data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR\n",
    "im_mr = crop_and_fill(im_mr, T1_arr)\n",
    "\n",
    "# PET\n",
    "im_pet = crop_and_fill(im_pet, FDG_arr)\n",
    "\n",
    "# CT\n",
    "im_ct = crop_and_fill(im_ct, uMap_arr)\n",
    "\n",
    "plt.figure();\n",
    "plot_2d_image([1,3,1], im_pet.as_array()[im_pet.dimensions()[0]//2, :, :], 'PET', cmap=\"hot\")\n",
    "plot_2d_image([1,3,2], numpy.abs(im_mr.as_array())[im_mr.dimensions()[0]//2, :, :], 'MR', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,3,3], numpy.abs(im_ct.as_array()), 'CT', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun fact\n",
    "The methods `forward` and `backward` for __MR__ and __PET__ describe to forward acquisition model (i.e. going from the object to the raw data) and backward acquisition model (i.e. going from the raw data to the object). For the __CT__ acquisition model we utilise functionality from __CIL__ . Here, these two operations are defined by `direct` (corresponding to `forward`) and `adjoint` (corresponding to `backward`). In order to make sure that the __MR__ and __PET__ acquisition models are fully compatible with all the __CIL__ functionality, both acquisition models also have the methods `direct` and `adjoint` which are simple aliases of `forward` and `backward`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to our three acquisition models and let's create some raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PET\n",
    "raw_pet = acq_mod_pet.forward(im_pet)\n",
    "\n",
    "# MR\n",
    "raw_mr = acq_mod_mr.forward(im_mr)\n",
    "\n",
    "# CT\n",
    "if have_astra:\n",
    "    raw_ct = acq_mod_ct.direct(im_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can apply the backward/adjoint operation to do a simply image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PET\n",
    "bwd_pet = acq_mod_pet.backward(raw_pet)\n",
    "\n",
    "# MR\n",
    "bwd_mr = acq_mod_mr.backward(raw_mr)\n",
    "\n",
    "# CT\n",
    "if have_astra:\n",
    "    bwd_ct = acq_mod_ct.adjoint(raw_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "# Raw data\n",
    "plot_2d_image([2,3,1], raw_pet.as_array()[0, raw_pet.dimensions()[1]//2, :, :], 'PET raw', cmap=\"viridis\")\n",
    "plot_2d_image([2,3,2], numpy.log(numpy.abs(raw_mr.as_array()[:, raw_mr.dimensions()[1]//2, :])), 'MR raw', cmap=\"viridis\")\n",
    "if have_astra:\n",
    "        plot_2d_image([2,3,3], raw_ct.as_array(), 'CT raw', cmap=\"viridis\")\n",
    "\n",
    "# Rec data\n",
    "plot_2d_image([2,3,4], bwd_pet.as_array()[bwd_pet.dimensions()[0]//2, :, :], 'PET', cmap=\"magma\")\n",
    "plot_2d_image([2,3,5], numpy.abs(bwd_mr.as_array()[bwd_mr.dimensions()[0]//2, :, :]), 'MR', cmap=\"Greys_r\")\n",
    "if have_astra:\n",
    "    plot_2d_image([2,3,6], bwd_ct.as_array(), 'CT', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images don't look too great. This is due to many things, e.g. for MR the raw data is missing a lot of k-space information and hence we get undersampling artefacts.In general, the adjoint operation is not equal to the inverse operation. Therefore SIRF offers a range of more sophisticated image reconstruction techniques which strongly improve the image quality, but this is another notebook..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
