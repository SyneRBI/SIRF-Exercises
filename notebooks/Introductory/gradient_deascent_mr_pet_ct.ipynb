{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent for MR, PET and CT\n",
    "This demonstration shows how to do image reconstruction using gradient descent for different modalities. \n",
    "\n",
    "It builds on the the notebook *acquisition_model_mr_pet_ct.ipynb*. The first part of the notebook which creates acquisition models and simulates data from the brainweb is the same code but with fewer comments. If anything is unclear, then please refer to the other notebook to get some further information.\n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christoph Kolbitsch, Edoardo Pasca\n",
    "First version: 23rd of April 2021  \n",
    "\n",
    "CCP PETMR Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2015 - 2017 Rutherford Appleton Laboratory STFC.  \n",
    "Copyright 2015 - 2019 University College London.   \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Positron Emission Tomography and Magnetic Resonance imaging\n",
    "(http://www.ccppetmr.ac.uk/).\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is installed that we need\n",
    "!pip install brainweb nibabel --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import MR, PET and CT functionality\n",
    "import sirf.Gadgetron as mr\n",
    "import sirf.STIR as pet\n",
    "import cil.framework as ct\n",
    "\n",
    "from sirf.Utilities import examples_data_path\n",
    "from cil.plugins.astra.operators import ProjectionOperator as ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    \n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb. We will use a FDG image for PET and the PET uMAP for CT. MR usually provides qualitative images with an image contrast proportional to difference in T1, T2 or T2* depending on the sequence parameters. Nevertheless, we will make our life easy, by directly using the T1 map provided by the brainweb for MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDG_arr  = vol['PET']\n",
    "T1_arr   = vol['T1']\n",
    "uMap_arr = vol['uMap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = FDG_arr.shape[0]//2\n",
    "plot_2d_image([1,3,1], FDG_arr[slice_show, 100:-100, 100:-100], 'FDG', cmap=\"hot\")\n",
    "plot_2d_image([1,3,2], T1_arr[slice_show, 100:-100, 100:-100], 'T1', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,3,3], uMap_arr[slice_show, 100:-100, 100:-100], 'uMap', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will set up the acquisition models for __MR__, __PET__ and __CT__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create MR AcquisitionData\n",
    "mr_acq = mr.AcquisitionData(examples_data_path('MR') + '/grappa2_1rep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. calculate CSM\n",
    "preprocessed_data = mr.preprocess_acquisition_data(mr_acq)\n",
    "\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 50\n",
    "csm.calculate(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. calculate image template\n",
    "recon = mr.FullySampledReconstructor()\n",
    "recon.set_input(preprocessed_data)\n",
    "recon.process()\n",
    "im_mr = recon.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create AcquisitionModel\n",
    "acq_mod_mr = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "\n",
    "# Supply csm to the acquisition model \n",
    "acq_mod_mr.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create PET AcquisitionData\n",
    "templ_sino = pet.AcquisitionData(examples_data_path('PET') + \"/thorax_single_slice/template_sinogram.hs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create a template PET ImageData\n",
    "im_pet = pet.ImageData(templ_sino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create AcquisitionModel\n",
    "\n",
    "# create PET acquisition model\n",
    "acq_mod_pet = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_mod_pet.set_up(templ_sino, im_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define AcquisitionGeometry\n",
    "angles = numpy.linspace(0, 360, 50, True, dtype=numpy.float32)\n",
    "ag2d = ct.AcquisitionGeometry.create_Cone2D((0,-1000), (0, 500))\\\n",
    "          .set_panel(128,pixel_size=3.104)\\\n",
    "          .set_angles(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get ImageGeometry\n",
    "ct_ig = ag2d.get_ImageGeometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create ImageData\n",
    "im_ct = ct_ig.allocate(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create AcquisitionModel\n",
    "acq_mod_ct = ap(ct_ig, ag2d, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply acquisition models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the acquisition models to create simulated raw data and then do a simple reconstruction to have some initial images (i.e. starting point) for our gradient descent algorithms. For each modality we will:\n",
    "\n",
    " * Fill an image template (`im_mr`, `im_pet`, `im_ct`)\n",
    " * Create raw data (`raw_mr`, `raw_pet`, `raw_ct`)\n",
    " * Reconstruct an initial guess of our image using `backward`/`adjoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR\n",
    "im_mr = crop_and_fill(im_mr, T1_arr)\n",
    "raw_mr = acq_mod_mr.forward(im_mr)\n",
    "bwd_mr = acq_mod_mr.backward(raw_mr)\n",
    "\n",
    "# PET\n",
    "im_pet = crop_and_fill(im_pet, FDG_arr)\n",
    "raw_pet = acq_mod_pet.forward(im_pet)\n",
    "bwd_pet = acq_mod_pet.backward(raw_pet)\n",
    "\n",
    "# CT\n",
    "im_ct = crop_and_fill(im_ct, uMap_arr)\n",
    "raw_ct = acq_mod_ct.direct(im_ct)\n",
    "bwd_ct = acq_mod_ct.adjoint(raw_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent or ascent depending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are basically two things we need to be able to run a gradient descent algorithm. First we need an objective function (`obj_func`) which calculates the difference between the acquired raw data and our current image estimate. Second, we need to know the gradient of the objective function (`obj_func_grad`), because we need to know how we have to update our current image estimate in order to decrease the value of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `obj_func` and `obj_func_grad` are modality specific and so here we will go through all modalities and define them. Let's start with __PET__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise in __PET__ follows a Poisson distribution and hence we can use a Poisson log-likelihood as our objective function. Luckily enough this is already part of __SIRF__ and hence we can simply create the objective function by providing the raw __PET__ data and __PET__ image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun_pet_sirf = pet.make_Poisson_loglikelihood(raw_pet)\n",
    "obj_fun_pet_sirf.set_up(bwd_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `obj_func_pet` cannot be called directly but `obj_func_pet.value()` has to be used, we write a quick wrapper around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_pet(curr_image_estimate):\n",
    "    return(obj_fun_pet_sirf.value(curr_image_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is also already implemented and can simply be calculated by calling `obj_fun_pet.gradient`. Nevertheless, to be more explicit and consistent with the other modalities we will define a new function to do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_grad_pet(curr_image_estimate):\n",
    "    # The 0 here means, only the gradient for subset 0 is returned. \n",
    "    # We will just accept this as is here, because subsets are too advanced for this demo here.\n",
    "    return(obj_fun_pet_sirf.gradient(curr_image_estimate, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __PET__ we need to make sure that all the image values are positive, so we will create a small function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positive(image_array):\n",
    "    \"\"\"truncate any negatives to zero\"\"\"\n",
    "    image_array[image_array<0] = 0\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, __PET__ is all done, now we will continue with __CT__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> CT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last but not least __MR__ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_mr(curr_image_estimate):\n",
    "    c =  acq_mod_mr.forward(curr_image_estimate) - raw_mr\n",
    "    return(0.5 * c.norm() ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun_grad_mr(curr_image_estimation):\n",
    "    # We are returning the negative gradient, because we are going to add it later to our image estimate\n",
    "    return(-acq_mod_mr.backward(acq_mod_mr.forward(curr_image_estimate) - raw_mr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our `obj_func` and `obj_func_grad` we will select one modality and then implement the gradient descent/ascent appproach. We also need an image `init_image` to start with. Here we will simply use the simple reconstruction which did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun = obj_fun_pet\n",
    "obj_fun_grad = obj_fun_grad_pet\n",
    "init_image = bwd_pet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define the number of iterations for the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "num_iters = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the probably most important paramter for gradient descent, the infamous __step-size__ . Unfortunately, the gradient only gives us the direction along which we need to update the image, but does not tell us by how much we have to go in this direction. Therefore, we need to define how far we step along the gradient direction in each iteration by hand. \n",
    "\n",
    "To make sure the step-size is adapted to each modality as much as possible, we won't define the step-size directly, but we will calculate the step-size as `tau` times the norm of the current image estimate. Nevertheless, this is still just a guess. \n",
    "\n",
    "If the step-size is too small, we have very slow convergence. If the step-size is too large, then we are overstepping our target image and the objective function starts to oscillate. The value below is reasonable for __PET__, but for the other modalities you will have to optimise it to ensure good convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative step-size\n",
    "tau = .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create an image as a starting point for the gradient descent algorithm and we will also create a numpy array, where we can store the image estimate at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial image\n",
    "curr_image_estimate = init_image.clone()\n",
    "\n",
    "# We will do the update of the image estimate in Python, so we also need the numpy array of our image data object\n",
    "curr_image_estimate_array = curr_image_estimate.as_array()\n",
    "\n",
    "# Array for images at each iteration\n",
    "image_iteration = numpy.zeros(shape=(num_iters + 1,) + curr_image_estimate_array.shape)\n",
    "\n",
    "# We use numpy.abs here to be compatible with MR complex data\n",
    "image_iteration[0, :, :, :] = numpy.abs(curr_image_estimate_array) \n",
    "\n",
    "# Variable to store the current value of the objective function\n",
    "obj_func_values = numpy.zeros(shape=(num_iters + 1,))\n",
    "obj_func_values[0] = obj_fun(curr_image_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally write down our gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, num_iters+1):  \n",
    "    # First we calculate the gradient to find out how we need to update our current image estimate\n",
    "    grad = obj_fun_grad(curr_image_estimate)\n",
    "    grad_array = grad.as_array()\n",
    "\n",
    "    # Compute step-size relative to current image-norm\n",
    "    step_size = tau * norm(curr_image_estimate_array) / norm(grad_array)\n",
    "\n",
    "    # Perform gradient ascent step\n",
    "    curr_image_estimate_array =  curr_image_estimate_array + step_size * grad_array\n",
    "\n",
    "    # In PET we have to ensure values are positive. \n",
    "    # We check if data is complex and therefore MR, or not and therefore PET or CT\n",
    "    if not curr_image_estimate_array.dtype in [numpy.complex, numpy.complex64]:\n",
    "        curr_image_estimate_array = make_positive(curr_image_estimate_array)\n",
    "    \n",
    "    # Update the image\n",
    "    curr_image_estimate.fill(curr_image_estimate_array)\n",
    "\n",
    "    # Compute objective function value for plotting, and write some diagnostics\n",
    "    obj_func_values[i] = obj_fun(curr_image_estimate)\n",
    "    \n",
    "    # We use numpy.abs here to be compatible with MR complex data\n",
    "    image_iteration[i,:,:,:] = numpy.abs(curr_image_estimate_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot objective function values\n",
    "plt.figure()\n",
    "plt.title('Objective function value')\n",
    "plt.plot(obj_func_values, 'o-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a slice for different iterations\n",
    "centre_slice = image_iteration.shape[1]//2\n",
    "plt.figure();\n",
    "for i in range(4):\n",
    "    curr_it = i*8\n",
    "    plot_2d_image([2,2,i+1], image_iteration[curr_it,centre_slice,:,:], 'It '+str(curr_it), cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
